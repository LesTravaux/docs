{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WEBSITE UNDER CONSTRUCTION TheHive Project # This is the official documentation website of TheHive Project. TheHive # TheHive is a scalable, open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. Sources: https://github.com/TheHive-Project/TheHive Documentation: https://docs.thehive-project.org/docs/thehive/ TheHive4py # TheHive4py is a Python API client for TheHive. Sources: https://github.com/TheHive-Project/TheHive4py Documentation: https://thehive-project.github.io/TheHive4py/ Cortex # Cortex is a powerful observable analysis and active response engine. Sources: https://github.com/TheHive-Project/Cortex Documentation: https://github.com/TheHive-Project/CortexDocs Cortex Neurons # Cortex neurons is the repository of the reviewed Analyzers and Responders, contributed by the community. Sources: https://github.com/TheHive-Project/Cortex-Analyzers/ Documentation: https://thehive-project.github.io/Cortex-Analyzers/ Cortex4py # Cortex4py is a Python API client for Cortex. Sources: https://github.com/TheHive-Project/Cortex4py Documentation: https://github.com/TheHive-Project/Cortex4py Cortexutils # Cortexutils is a Python library containing a set of classes that aims to make users write Cortex analyzers and responders easier. Sources: https://github.com/TheHive-Project/Cortexutils Documentation: https://github.com/TheHive-Project/Cortexutils Docker-templates # This repository is hosting docker configurations for TheHive, Cortex and 3rd party tools integrations. Sources: https://github.com/TheHive-Project/Docker-Templates Awesome # This repository aims at reference and centralise a curated list of awesome things related to TheHive & Cortex. Website: https://github.com/TheHive-Project/awesome TODO # How to update to TheHive 4.1.0 User Guides","title":"Home"},{"location":"#thehive-project","text":"This is the official documentation website of TheHive Project.","title":"TheHive Project"},{"location":"#thehive","text":"TheHive is a scalable, open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. Sources: https://github.com/TheHive-Project/TheHive Documentation: https://docs.thehive-project.org/docs/thehive/","title":"TheHive"},{"location":"#thehive4py","text":"TheHive4py is a Python API client for TheHive. Sources: https://github.com/TheHive-Project/TheHive4py Documentation: https://thehive-project.github.io/TheHive4py/","title":"TheHive4py"},{"location":"#cortex","text":"Cortex is a powerful observable analysis and active response engine. Sources: https://github.com/TheHive-Project/Cortex Documentation: https://github.com/TheHive-Project/CortexDocs","title":"Cortex"},{"location":"#cortex-neurons","text":"Cortex neurons is the repository of the reviewed Analyzers and Responders, contributed by the community. Sources: https://github.com/TheHive-Project/Cortex-Analyzers/ Documentation: https://thehive-project.github.io/Cortex-Analyzers/","title":"Cortex Neurons"},{"location":"#cortex4py","text":"Cortex4py is a Python API client for Cortex. Sources: https://github.com/TheHive-Project/Cortex4py Documentation: https://github.com/TheHive-Project/Cortex4py","title":"Cortex4py"},{"location":"#cortexutils","text":"Cortexutils is a Python library containing a set of classes that aims to make users write Cortex analyzers and responders easier. Sources: https://github.com/TheHive-Project/Cortexutils Documentation: https://github.com/TheHive-Project/Cortexutils","title":"Cortexutils"},{"location":"#docker-templates","text":"This repository is hosting docker configurations for TheHive, Cortex and 3rd party tools integrations. Sources: https://github.com/TheHive-Project/Docker-Templates","title":"Docker-templates"},{"location":"#awesome","text":"This repository aims at reference and centralise a curated list of awesome things related to TheHive & Cortex. Website: https://github.com/TheHive-Project/awesome","title":"Awesome"},{"location":"#todo","text":"How to update to TheHive 4.1.0 User Guides","title":"TODO"},{"location":"download/","text":"","title":"Index"},{"location":"resources/Keynotes/list/","text":"Additional Resources # The following page lists additional resources that should help you get more acquainted with TheHive, Cortex & other tools. Table of Contents # Presentations Workshops and Trainings Hack.lu 2019 Botconf 2018 User Contributions Presentations # We make several presentations throughout the year during conferences and various events. Please find below some of the latest presentation material we produced: Cruising Ocean Threat without Sinking Using TheHive, Cortex & MISP. BSidesLisbon 2018. November 29, 2018. ( PDF ) TheHive & Cortex UYBHYS 2018. November 17, 2018. ( PDF ) MISP, TheHive & Cortex: better, faster, happier. MISP Summit 04. October 16, 2018. ( PDF ) Workshops and Trainings # We frequently organize workshops and trainings, often with our friends from the MISP Project . We do not publish all the materials because we often leverage MISP instances containing training-specific events and Cortex servers configured with commercial analyzers that supporting partners such as DomainTools and Onyphe kindly give us access to for the duration of the workshops and trainings. If you'd like to attend a future workshop or training, please follow us on https://twitter.com/thehive_project or regularly visit our blog . However, if you'd like to do the training at your own pace, you can find below the materials used for some of the workshops and trainings we gave in the past. Please note that you might have some difficulties completing the case studies without access to the commercial analyzers highlighted above. Hack.lu 2019 # We gave a workshop during Hack.lu on Thu Oct 24, 2019. We prepared a MISP and Cortex instance on the cloud as well as a custom built training VM containing TheHive 3.4.0 which took advantage of those cloud instances.The VM was shared with the attendees during the workshop but will not be posted online. Indeed, the above-mentioned cloud instances were turned off after the workshop. That being said, you can still get a look at the slides we used to set the stage for the workshop. They contain some valuable information if you are considering installing TheHive, Cortex & MISP or just beginning with the trio. Botconf 2018 # We gave a workshop during Botconf on Tue Dec 4, 2018. If you'd like to give it a try on your own, you will need: - familiarity with TCP/IP, Linux (including editing configuration files), SSH & incident response - the joint MISP, TheHive & Cortex training VM ( SHA256 checksum ) - a powerful laptop with virtualization software (either VMware Workstation, VMware Fusion or VirtualBox) - the ability to give the training VM 6GB of RAM and 2 processor cores. If that's not possible, we consider 4GB and 1 processor core the bare minimum - the training instructions and cheatsheet - Case Study 1 - Case Study 2 Before undertaking the workshop, we highly recommend reading the following slides in the specified order: - Threat Intelligence and Information Sharing with MISP - Detect, Investigate & Respond with MISP, TheHive & Cortex Important Note : you won't be able to do case study 3 as it requires access to the instructors' MISP instance which is only available during the workshops and trainings. You must also skip the steps which ask you to synchronize your MISP instance with the instructors' (unless you have access to an instance pre-populated with events) or configure TheHive to leverage the instructors' Cortex instance. User Contributions # The resources below have been contributed by our user community. Please note that the fact that they are listed here does not mean that they have been checked, validated or endorsed in any way by TheHive Project. Use your own judgment if you decide to read them. TheHive Scripting: Task Imports , Matt B. Last accessed on March 26, 2019.","title":"Additional Resources"},{"location":"resources/Keynotes/list/#additional-resources","text":"The following page lists additional resources that should help you get more acquainted with TheHive, Cortex & other tools.","title":"Additional Resources"},{"location":"resources/Keynotes/list/#table-of-contents","text":"Presentations Workshops and Trainings Hack.lu 2019 Botconf 2018 User Contributions","title":"Table of Contents"},{"location":"resources/Keynotes/list/#presentations","text":"We make several presentations throughout the year during conferences and various events. Please find below some of the latest presentation material we produced: Cruising Ocean Threat without Sinking Using TheHive, Cortex & MISP. BSidesLisbon 2018. November 29, 2018. ( PDF ) TheHive & Cortex UYBHYS 2018. November 17, 2018. ( PDF ) MISP, TheHive & Cortex: better, faster, happier. MISP Summit 04. October 16, 2018. ( PDF )","title":"Presentations"},{"location":"resources/Keynotes/list/#workshops-and-trainings","text":"We frequently organize workshops and trainings, often with our friends from the MISP Project . We do not publish all the materials because we often leverage MISP instances containing training-specific events and Cortex servers configured with commercial analyzers that supporting partners such as DomainTools and Onyphe kindly give us access to for the duration of the workshops and trainings. If you'd like to attend a future workshop or training, please follow us on https://twitter.com/thehive_project or regularly visit our blog . However, if you'd like to do the training at your own pace, you can find below the materials used for some of the workshops and trainings we gave in the past. Please note that you might have some difficulties completing the case studies without access to the commercial analyzers highlighted above.","title":"Workshops and Trainings"},{"location":"resources/Keynotes/list/#hacklu-2019","text":"We gave a workshop during Hack.lu on Thu Oct 24, 2019. We prepared a MISP and Cortex instance on the cloud as well as a custom built training VM containing TheHive 3.4.0 which took advantage of those cloud instances.The VM was shared with the attendees during the workshop but will not be posted online. Indeed, the above-mentioned cloud instances were turned off after the workshop. That being said, you can still get a look at the slides we used to set the stage for the workshop. They contain some valuable information if you are considering installing TheHive, Cortex & MISP or just beginning with the trio.","title":"Hack.lu 2019"},{"location":"resources/Keynotes/list/#botconf-2018","text":"We gave a workshop during Botconf on Tue Dec 4, 2018. If you'd like to give it a try on your own, you will need: - familiarity with TCP/IP, Linux (including editing configuration files), SSH & incident response - the joint MISP, TheHive & Cortex training VM ( SHA256 checksum ) - a powerful laptop with virtualization software (either VMware Workstation, VMware Fusion or VirtualBox) - the ability to give the training VM 6GB of RAM and 2 processor cores. If that's not possible, we consider 4GB and 1 processor core the bare minimum - the training instructions and cheatsheet - Case Study 1 - Case Study 2 Before undertaking the workshop, we highly recommend reading the following slides in the specified order: - Threat Intelligence and Information Sharing with MISP - Detect, Investigate & Respond with MISP, TheHive & Cortex Important Note : you won't be able to do case study 3 as it requires access to the instructors' MISP instance which is only available during the workshops and trainings. You must also skip the steps which ask you to synchronize your MISP instance with the instructors' (unless you have access to an instance pre-populated with events) or configure TheHive to leverage the instructors' Cortex instance.","title":"Botconf 2018"},{"location":"resources/Keynotes/list/#user-contributions","text":"The resources below have been contributed by our user community. Please note that the fact that they are listed here does not mean that they have been checked, validated or endorsed in any way by TheHive Project. Use your own judgment if you decide to read them. TheHive Scripting: Task Imports , Matt B. Last accessed on March 26, 2019.","title":"User Contributions"},{"location":"resources/Virtual%20Machine/demo/","text":"Demo VM # A ready-to-use virtual machine can be downloaded at https://www.strangebee.com/tryit . This VM is prepared and updated by StrangeBee and is powered by the latest versions of: TheHive: Security Incident Response and Case management platform Cortex: Extendable Analysis, Enrichment and Response automation framework Warning The VM is built for testing purposes and is NOT RECOMMENDED for production .","title":"Demo VM"},{"location":"resources/Virtual%20Machine/demo/#demo-vm","text":"A ready-to-use virtual machine can be downloaded at https://www.strangebee.com/tryit . This VM is prepared and updated by StrangeBee and is powered by the latest versions of: TheHive: Security Incident Response and Case management platform Cortex: Extendable Analysis, Enrichment and Response automation framework Warning The VM is built for testing purposes and is NOT RECOMMENDED for production .","title":"Demo VM"},{"location":"thehive/","text":"TheHive : Installation, operation and user guides Source Code : https://github.com/thehive-project/TheHive/ Website : https://www.thehive-project.org TheHive # TheHive is a scalable, open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. TheHive supports different methods to store data, files, and indexes according to your needs. However, even for a standalone, production server, we strongly recommend using Apache Cassandra as a scalable and fault-tolerant database. Files and indexes storage can vary, depending on your target setup ; for standalone server, the local filesystem is suitable, while sereval options are possible in the case of a cluster configuration. Installation and configuration guides # This documentation contains step-by-step installation instructions for TheHive for different operating systems as well as corresponding binary archives. All aspects of the configuration are aslo detailled in a dedicated section. User guides # TheHive supports differents roles for users. Depending on if you are an administrator of the plateform, an administrator of an organisation or an analyst you can have access and run differents actions in the plateform. The user guides aims at describing all major howtos for users according to their roles and permissions. Operations # Discover how to migration from TheHive 3.x to TheHive 4.x with our migration guide . Several other operational guides are provided to the community. Setup HTTPS with nginx or haproxy Backup and restore : example on how to backup and restore data stored in Apache Cassandra Adding security in Apache Cassandra Using Fail2Ban and block unwanted connections to the plateform TheHive 3 # If you are still using TheHive 3.x, the associated documentation is available here End of Life TheHive 3 is coming End of Life. This version no longer benefits from new features. We recommend migrating as soon as possible to TheHive 4.x. License # TheHive is an open source and free software released under the AGPL (Affero General Public License). We, TheHive Project, are committed to ensure that TheHive will remain a free and open source project on the long-run. Updates and community discussions # Information, news and updates are regularly posted on several communication channels: TheHive Project Twitter account TheHive Project blog TheHive Project Discord Users forum on Google Groups . Request an access: using a Gmail address or without it . Contributing # We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests using issues . We do have a Code of conduct . Make sure to check it out before contributing. Community support # Please open an issue on GitHub if you'd like to report a bug or request a feature. We are also available on Gitter to help you out. If you need to contact the Project's team, send an email to support@thehive-project.org . Note If you have problems with TheHive4py , please open an issue on its dedicated repository . If you encounter an issue with Cortex or would like to request a Cortex-related feature, please open an issue on its dedicated GitHub repository . If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the analyzers' dedicated GitHub repository . Professional support # TheHive is fully developped and maintained by StrangeBee . Should you need specific assistance, be aware that StrangeBee also provides professional services and support.","title":"Home"},{"location":"thehive/#thehive","text":"TheHive is a scalable, open source and free Security Incident Response Platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. TheHive supports different methods to store data, files, and indexes according to your needs. However, even for a standalone, production server, we strongly recommend using Apache Cassandra as a scalable and fault-tolerant database. Files and indexes storage can vary, depending on your target setup ; for standalone server, the local filesystem is suitable, while sereval options are possible in the case of a cluster configuration.","title":"TheHive"},{"location":"thehive/#installation-and-configuration-guides","text":"This documentation contains step-by-step installation instructions for TheHive for different operating systems as well as corresponding binary archives. All aspects of the configuration are aslo detailled in a dedicated section.","title":"Installation and configuration guides"},{"location":"thehive/#user-guides","text":"TheHive supports differents roles for users. Depending on if you are an administrator of the plateform, an administrator of an organisation or an analyst you can have access and run differents actions in the plateform. The user guides aims at describing all major howtos for users according to their roles and permissions.","title":"User guides"},{"location":"thehive/#operations","text":"Discover how to migration from TheHive 3.x to TheHive 4.x with our migration guide . Several other operational guides are provided to the community. Setup HTTPS with nginx or haproxy Backup and restore : example on how to backup and restore data stored in Apache Cassandra Adding security in Apache Cassandra Using Fail2Ban and block unwanted connections to the plateform","title":"Operations"},{"location":"thehive/#thehive-3","text":"If you are still using TheHive 3.x, the associated documentation is available here End of Life TheHive 3 is coming End of Life. This version no longer benefits from new features. We recommend migrating as soon as possible to TheHive 4.x.","title":"TheHive 3"},{"location":"thehive/#license","text":"TheHive is an open source and free software released under the AGPL (Affero General Public License). We, TheHive Project, are committed to ensure that TheHive will remain a free and open source project on the long-run.","title":"License"},{"location":"thehive/#updates-and-community-discussions","text":"Information, news and updates are regularly posted on several communication channels: TheHive Project Twitter account TheHive Project blog TheHive Project Discord Users forum on Google Groups . Request an access: using a Gmail address or without it .","title":"Updates and community discussions"},{"location":"thehive/#contributing","text":"We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests using issues . We do have a Code of conduct . Make sure to check it out before contributing.","title":"Contributing"},{"location":"thehive/#community-support","text":"Please open an issue on GitHub if you'd like to report a bug or request a feature. We are also available on Gitter to help you out. If you need to contact the Project's team, send an email to support@thehive-project.org . Note If you have problems with TheHive4py , please open an issue on its dedicated repository . If you encounter an issue with Cortex or would like to request a Cortex-related feature, please open an issue on its dedicated GitHub repository . If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the analyzers' dedicated GitHub repository .","title":"Community support"},{"location":"thehive/#professional-support","text":"TheHive is fully developped and maintained by StrangeBee . Should you need specific assistance, be aware that StrangeBee also provides professional services and support.","title":"Professional support"},{"location":"thehive/code-of-conduct/","text":"Contributor Covenant Code of Conduct # Our Pledge # In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards # Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities # Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior, in compliance with the licensing terms applying to the Project developments. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. However, these actions shall respect the licensing terms of the Project Developments that will always supersede such Code of Conduct. Scope # This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement # Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at support@thehive-project.org. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution # This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4 This version includes a clarification to ensure that the code of conduct is in compliance with the free software licensing terms of the project.","title":"Contributor Covenant Code of Conduct"},{"location":"thehive/code-of-conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"thehive/code-of-conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"thehive/code-of-conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"thehive/code-of-conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior, in compliance with the licensing terms applying to the Project developments. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. However, these actions shall respect the licensing terms of the Project Developments that will always supersede such Code of Conduct.","title":"Our Responsibilities"},{"location":"thehive/code-of-conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"thehive/code-of-conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at support@thehive-project.org. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"thehive/code-of-conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at http://contributor-covenant.org/version/1/4 This version includes a clarification to ensure that the code of conduct is in compliance with the free software licensing terms of the project.","title":"Attribution"},{"location":"thehive/api/","text":"Introduction # APIs # Administration APIs # Manage Organisations Manage Users Manage Custom fields Organisation APIs # Manage Case Templates Case Management APIs # Alert APIs Case APIs Task APIs Observable APIs TTP APIs Library # StrangeBee provides an official library for integrating with the remote API of TheHive: TheHive4py","title":"Introduction"},{"location":"thehive/api/#introduction","text":"","title":"Introduction"},{"location":"thehive/api/#apis","text":"","title":"APIs"},{"location":"thehive/api/#administration-apis","text":"Manage Organisations Manage Users Manage Custom fields","title":"Administration APIs"},{"location":"thehive/api/#organisation-apis","text":"Manage Case Templates","title":"Organisation APIs"},{"location":"thehive/api/#case-management-apis","text":"Alert APIs Case APIs Task APIs Observable APIs TTP APIs","title":"Case Management APIs"},{"location":"thehive/api/#library","text":"StrangeBee provides an official library for integrating with the remote API of TheHive: TheHive4py","title":"Library"},{"location":"thehive/api/alert/","text":"Alert APIs # Alert operations # List alerts Create alert Delete alert Update alert Merge alert in case Promote alert into a case Mark alert as read Run responder on alert List responder jobs Get alerts' similar cases Alert observable operations # Add alert observable Update alert observable Delete alert observable List alert observables","title":"Overview"},{"location":"thehive/api/alert/#alert-apis","text":"","title":"Alert APIs"},{"location":"thehive/api/alert/#alert-operations","text":"List alerts Create alert Delete alert Update alert Merge alert in case Promote alert into a case Mark alert as read Run responder on alert List responder jobs Get alerts' similar cases","title":"Alert operations"},{"location":"thehive/api/alert/#alert-observable-operations","text":"Add alert observable Update alert observable Delete alert observable List alert observables","title":"Alert observable operations"},{"location":"thehive/api/alert/add-observable/","text":"Add observables # Add Observable to an Alert . Query # POST /api/alert/{id}/artifact With: id : Alert identifier Request Body Example # { \"dataType\" : \"ip\" , \"ioc\" : True , \"sighted\" : True , \"ignoreSimilarity\" : False , \"tlp\" : 2 , \"message\" : \"sample description\" , \"tags\" :[ \"test\" , \"Another Test Tag\" ], \"data\" :[ \"1.2.3.4\" ] } Response # Status codes # 201 : if Alert is created successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 201 [ { \"_id\" : \"~1564784\" , \"id\" : \"~1564784\" , \"createdBy\" : \"analyst@soc\" , \"createdAt\" : 1637091448338 , \"_type\" : \"case_artifact\" , \"dataType\" : \"ip\" , \"data\" : \"1.2.3.4\" , \"startDate\" : 1637091448338 , \"tlp\" : 2 , \"tags\" :[ \"test\" , \"Another Test Tag\" ], \"ioc\" : true , \"sighted\" : true , \"message\" : \"sample description\" , \"reports\" :{}, \"stats\" :{}, \"ignoreSimilarity\" : false } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Add observables"},{"location":"thehive/api/alert/add-observable/#add-observables","text":"Add Observable to an Alert .","title":"Add observables"},{"location":"thehive/api/alert/add-observable/#query","text":"POST /api/alert/{id}/artifact With: id : Alert identifier","title":"Query"},{"location":"thehive/api/alert/add-observable/#request-body-example","text":"{ \"dataType\" : \"ip\" , \"ioc\" : True , \"sighted\" : True , \"ignoreSimilarity\" : False , \"tlp\" : 2 , \"message\" : \"sample description\" , \"tags\" :[ \"test\" , \"Another Test Tag\" ], \"data\" :[ \"1.2.3.4\" ] }","title":"Request Body Example"},{"location":"thehive/api/alert/add-observable/#response","text":"","title":"Response"},{"location":"thehive/api/alert/add-observable/#status-codes","text":"201 : if Alert is created successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/alert/add-observable/#responsebody-example","text":"201 [ { \"_id\" : \"~1564784\" , \"id\" : \"~1564784\" , \"createdBy\" : \"analyst@soc\" , \"createdAt\" : 1637091448338 , \"_type\" : \"case_artifact\" , \"dataType\" : \"ip\" , \"data\" : \"1.2.3.4\" , \"startDate\" : 1637091448338 , \"tlp\" : 2 , \"tags\" :[ \"test\" , \"Another Test Tag\" ], \"ioc\" : true , \"sighted\" : true , \"message\" : \"sample description\" , \"reports\" :{}, \"stats\" :{}, \"ignoreSimilarity\" : false } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"ResponseBody Example"},{"location":"thehive/api/alert/create/","text":"Create # Create an Alert . Query # POST /api/alert Request Body Example # { \"artifacts\" : [], \"description\" : \"Imported from MISP Event #1311.\" , \"severity\" : 0 , \"source\" : \"misp server\" , \"sourceRef\" : \"1311\" , \"tags\" : [ \"tlp:white\" , \"type:OSINT\" ], \"title\" : \"CISA.gov - AA21-062A Mitigate Microsoft Exchange Server Vulnerabilities\" , \"tlp\" : 0 , \"type\" : \"MISP Event\" } The following fields are required: title : (String) source : (String) sourceRef : (String) type : (String) Response # Status codes # 201 : if Alert is created successfully 401 : Authentication error ResponseBody Example # { \"_id\" : \"~987889880\" , \"id\" : \"~987889880\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1630323713949 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"misp event\" , \"source\" : \"misp server\" , \"sourceRef\" : \"1311-2\" , \"externalLink\" : null , \"case\" : null , \"title\" : \"CISA.gov - AA21-062A Mitigate Microsoft Exchange Server Vulnerabilities\" , \"description\" : \"Imported from MISP Event #1311.\" , \"severity\" : 0 , \"date\" : 1630323713937 , \"tags\" : [ \"tlp:pwhite\" , \"type:OSINT\" , ], \"tlp\" : 0 , \"pap\" : 2 , \"status\" : \"New\" , \"follow\" : true , \"customFields\" : {}, \"caseTemplate\" : null , \"artifacts\" : [], \"similarCases\" : [] }","title":"Create"},{"location":"thehive/api/alert/create/#create","text":"Create an Alert .","title":"Create"},{"location":"thehive/api/alert/create/#query","text":"POST /api/alert","title":"Query"},{"location":"thehive/api/alert/create/#request-body-example","text":"{ \"artifacts\" : [], \"description\" : \"Imported from MISP Event #1311.\" , \"severity\" : 0 , \"source\" : \"misp server\" , \"sourceRef\" : \"1311\" , \"tags\" : [ \"tlp:white\" , \"type:OSINT\" ], \"title\" : \"CISA.gov - AA21-062A Mitigate Microsoft Exchange Server Vulnerabilities\" , \"tlp\" : 0 , \"type\" : \"MISP Event\" } The following fields are required: title : (String) source : (String) sourceRef : (String) type : (String)","title":"Request Body Example"},{"location":"thehive/api/alert/create/#response","text":"","title":"Response"},{"location":"thehive/api/alert/create/#status-codes","text":"201 : if Alert is created successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/create/#responsebody-example","text":"{ \"_id\" : \"~987889880\" , \"id\" : \"~987889880\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1630323713949 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"misp event\" , \"source\" : \"misp server\" , \"sourceRef\" : \"1311-2\" , \"externalLink\" : null , \"case\" : null , \"title\" : \"CISA.gov - AA21-062A Mitigate Microsoft Exchange Server Vulnerabilities\" , \"description\" : \"Imported from MISP Event #1311.\" , \"severity\" : 0 , \"date\" : 1630323713937 , \"tags\" : [ \"tlp:pwhite\" , \"type:OSINT\" , ], \"tlp\" : 0 , \"pap\" : 2 , \"status\" : \"New\" , \"follow\" : true , \"customFields\" : {}, \"caseTemplate\" : null , \"artifacts\" : [], \"similarCases\" : [] }","title":"ResponseBody Example"},{"location":"thehive/api/alert/delete-observable/","text":"Add observables # Delete an Observable from an Alert . Query # DELETE /api/alert/artifact/{id} With: id : Observable identifier Response # Status codes # 204 : if Observable is deleted successfully 401 : Authentication error","title":"Add observables"},{"location":"thehive/api/alert/delete-observable/#add-observables","text":"Delete an Observable from an Alert .","title":"Add observables"},{"location":"thehive/api/alert/delete-observable/#query","text":"DELETE /api/alert/artifact/{id} With: id : Observable identifier","title":"Query"},{"location":"thehive/api/alert/delete-observable/#response","text":"","title":"Response"},{"location":"thehive/api/alert/delete-observable/#status-codes","text":"204 : if Observable is deleted successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/delete/","text":"Delete # Delete an Alert . Query # DELETE /api/alert/{id}?force=1 Response # Status codes # 204 : if Alert is deleted successfully 401 : Authentication error","title":"Delete"},{"location":"thehive/api/alert/delete/#delete","text":"Delete an Alert .","title":"Delete"},{"location":"thehive/api/alert/delete/#query","text":"DELETE /api/alert/{id}?force=1","title":"Query"},{"location":"thehive/api/alert/delete/#response","text":"","title":"Response"},{"location":"thehive/api/alert/delete/#status-codes","text":"204 : if Alert is deleted successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/list-observables/","text":"List Observables # List observables of an Alerts . Query # POST /api/v0/query?name Request Body Example # List last 15 added observables: { \"query\" : [ { \"_name\" : \"getAlert\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"observables\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 , \"extraData\" : [ \"seen\" ] } ] } With: id : id of the Alert Response # Status codes # 200 : if query is run successfully 401 : Authentication error ResponseBody Example # [ ... { \"_id\" : \"~11111462234\" , \"id\" : \"~11111462234\" , \"_type\" : \"Observable\" , \"_createdBy\" : \"system@thehive.local\" , \"_createdAt\" : 1629309258431 , \"dataType\" : \"other\" , \"data\" : \"1.2.3.4\" , \"startDate\" : 1629309258431 , \"tlp\" : 0 , \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} } ... ]","title":"List Observables"},{"location":"thehive/api/alert/list-observables/#list-observables","text":"List observables of an Alerts .","title":"List Observables"},{"location":"thehive/api/alert/list-observables/#query","text":"POST /api/v0/query?name","title":"Query"},{"location":"thehive/api/alert/list-observables/#request-body-example","text":"List last 15 added observables: { \"query\" : [ { \"_name\" : \"getAlert\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"observables\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 , \"extraData\" : [ \"seen\" ] } ] } With: id : id of the Alert","title":"Request Body Example"},{"location":"thehive/api/alert/list-observables/#response","text":"","title":"Response"},{"location":"thehive/api/alert/list-observables/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/list-observables/#responsebody-example","text":"[ ... { \"_id\" : \"~11111462234\" , \"id\" : \"~11111462234\" , \"_type\" : \"Observable\" , \"_createdBy\" : \"system@thehive.local\" , \"_createdAt\" : 1629309258431 , \"dataType\" : \"other\" , \"data\" : \"1.2.3.4\" , \"startDate\" : 1629309258431 , \"tlp\" : 0 , \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} } ... ]","title":"ResponseBody Example"},{"location":"thehive/api/alert/list-responder-jobs/","text":"List responder actions # List actions run on an Alert . Query # GET /api/connector/cortex/action/responder/alert/{id} With: id : Alert identifier Response # Status codes # 200 : if query is run successfully 401 : Authentication error Response Body Example # 200 [ { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Alert\" , \"objectId\" : \"~25313328\" , \"status\" : \"Success\" , \"startDate\" : 1630917246993 , \"endDate\" : 1630917254406 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":true,\\\"artifacts\\\":[],\\\"operations\\\":[],\\\\\\\"message\\\\\\\":\\\\\\\"Ok\\\\\\\",\\\\\\\"parameters\\\\\\\":{\\\\\\\"organisation\\\\\\\":\\\\\\\"StrangeBee\\\\\\\",\\\\\\\"user\\\\\\\":\\\\\\\"user@thehive.local\\\\\\\"},\\\\\\\"config\\\\\\\":{\\\\\\\"proxy_https\\\\\\\":null,\\\\\\\"cacerts\\\\\\\":null,\\\\\\\"check_tlp\\\\\\\":false,\\\\\\\"max_tlp\\\\\\\":2,\\\\\\\"check_pap\\\\\\\":false,\\\\\\\"max_pap\\\\\\\":2,\\\\\\\"jobTimeout\\\\\\\":30,\\\\\\\"proxy_http\\\\\\\":null}}\\\"}\" } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } List available Responders # Request # To get the list of Responders available for an Alert , based on its TLP and PAP, you can call the following API: GET /api/connector/cortex/responder/alert/{id} With: id : Alert identifier Response # 200 [ { \"id\" : \"e33d63082066c739c07d2bbc199bfe7e\" , \"name\" : \"MALSPAM_Reply_to_user_1_0\" , \"version\" : \"1.0\" , \"description\" : \"Reply to user with an email. Applies on tasks\" , \"dataTypeList\" : [ \"thehive:Alert\" ], \"cortexIds\" : [ \"Demo\" ] } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"List responder actions"},{"location":"thehive/api/alert/list-responder-jobs/#list-responder-actions","text":"List actions run on an Alert .","title":"List responder actions"},{"location":"thehive/api/alert/list-responder-jobs/#query","text":"GET /api/connector/cortex/action/responder/alert/{id} With: id : Alert identifier","title":"Query"},{"location":"thehive/api/alert/list-responder-jobs/#response","text":"","title":"Response"},{"location":"thehive/api/alert/list-responder-jobs/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/list-responder-jobs/#response-body-example","text":"200 [ { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Alert\" , \"objectId\" : \"~25313328\" , \"status\" : \"Success\" , \"startDate\" : 1630917246993 , \"endDate\" : 1630917254406 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":true,\\\"artifacts\\\":[],\\\"operations\\\":[],\\\\\\\"message\\\\\\\":\\\\\\\"Ok\\\\\\\",\\\\\\\"parameters\\\\\\\":{\\\\\\\"organisation\\\\\\\":\\\\\\\"StrangeBee\\\\\\\",\\\\\\\"user\\\\\\\":\\\\\\\"user@thehive.local\\\\\\\"},\\\\\\\"config\\\\\\\":{\\\\\\\"proxy_https\\\\\\\":null,\\\\\\\"cacerts\\\\\\\":null,\\\\\\\"check_tlp\\\\\\\":false,\\\\\\\"max_tlp\\\\\\\":2,\\\\\\\"check_pap\\\\\\\":false,\\\\\\\"max_pap\\\\\\\":2,\\\\\\\"jobTimeout\\\\\\\":30,\\\\\\\"proxy_http\\\\\\\":null}}\\\"}\" } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"Response Body Example"},{"location":"thehive/api/alert/list-responder-jobs/#list-available-responders","text":"","title":"List available Responders"},{"location":"thehive/api/alert/list-responder-jobs/#request","text":"To get the list of Responders available for an Alert , based on its TLP and PAP, you can call the following API: GET /api/connector/cortex/responder/alert/{id} With: id : Alert identifier","title":"Request"},{"location":"thehive/api/alert/list-responder-jobs/#response_1","text":"200 [ { \"id\" : \"e33d63082066c739c07d2bbc199bfe7e\" , \"name\" : \"MALSPAM_Reply_to_user_1_0\" , \"version\" : \"1.0\" , \"description\" : \"Reply to user with an email. Applies on tasks\" , \"dataTypeList\" : [ \"thehive:Alert\" ], \"cortexIds\" : [ \"Demo\" ] } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"Response"},{"location":"thehive/api/alert/list/","text":"List / Search # List Alerts . Query # POST /api/v1/query?name=alerts Request Body Example # List last 15 alerts: { \"query\" : [ { \"_name\" : \"listAlert\" }, { \"_name\" : \"filter\" , \"_field\" : \"imported\" , \"_value\" : false }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"date\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 , \"extraData\" : [ \"importDate\" , \"caseNumber\" ] } ] } Response # Status codes # 200 : if query is run successfully 401 : Authentication error ResponseBody Example # ```json [ ... { \"_id\": \"~789196976\", \"_type\": \"Alert\", \"_createdBy\": \"florian@strangebee.com\", \"_createdAt\": 1620393156944, \"status\": \"New\", \"type\": \"external\", \"source\": \"MISP server\", \"sourceRef\": \"event_1576\", \"externalLink\": null, \"title\": \"Phishing list update 7.5.2021\", \"description\": \"A curated list of phishing IOCs\", \"severity\": 2, \"date\": 1620393156000, \"tags\": [ \"source:MISP\", \"origin:CIRCL_LU\" ], \"tlp\": 3, \"pap\": 2, \"read\": false, \"follow\": true, \"customFields\": [], \"caseTemplate\": null, \"artifacts\": [], \"similarCases\": [] } ... ] ```","title":"List / Search"},{"location":"thehive/api/alert/list/#list-search","text":"List Alerts .","title":"List / Search"},{"location":"thehive/api/alert/list/#query","text":"POST /api/v1/query?name=alerts","title":"Query"},{"location":"thehive/api/alert/list/#request-body-example","text":"List last 15 alerts: { \"query\" : [ { \"_name\" : \"listAlert\" }, { \"_name\" : \"filter\" , \"_field\" : \"imported\" , \"_value\" : false }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"date\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 , \"extraData\" : [ \"importDate\" , \"caseNumber\" ] } ] }","title":"Request Body Example"},{"location":"thehive/api/alert/list/#response","text":"","title":"Response"},{"location":"thehive/api/alert/list/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/list/#responsebody-example","text":"```json [ ... { \"_id\": \"~789196976\", \"_type\": \"Alert\", \"_createdBy\": \"florian@strangebee.com\", \"_createdAt\": 1620393156944, \"status\": \"New\", \"type\": \"external\", \"source\": \"MISP server\", \"sourceRef\": \"event_1576\", \"externalLink\": null, \"title\": \"Phishing list update 7.5.2021\", \"description\": \"A curated list of phishing IOCs\", \"severity\": 2, \"date\": 1620393156000, \"tags\": [ \"source:MISP\", \"origin:CIRCL_LU\" ], \"tlp\": 3, \"pap\": 2, \"read\": false, \"follow\": true, \"customFields\": [], \"caseTemplate\": null, \"artifacts\": [], \"similarCases\": [] } ... ] ```","title":"ResponseBody Example"},{"location":"thehive/api/alert/merge/","text":"Merge # Merge an Alert into an existing Case . Query # POST /api/alert/{id1}/merge/{id2} With: id1 : id of the Alert to merge id2 : id of the destination Case Response # Status codes # 200 : if Alert is successfully merged 401 : Authentication error ResponseBody Example # { \"_id\" : \"~6658533455\" , \"id\" : \"~6658533455\" , \"createdBy\" : \"florian@strangebee.com\" , \"updatedBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620397519028 , \"updatedAt\" : 1624373852175 , \"_type\" : \"case\" , \"caseId\" : 114 , \"title\" : \"User connected to known malicious IP over Telnet / Malicious payload detected\" , \"description\" : \"EDR automated alert: the user robb@training.org has connected to known malicious IP over Telnet\\n\\nEDR automated alert: malicious payload detected on computer PC-Robb\\n \\n#### Merged with alert #90e044 User posted information on known phishing URL\\n\\nSIEM automated alert: the user robb@training.org has posted information on a known phishing url\" , \"severity\" : 2 , \"startDate\" : 1620396059728 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" : [ \"log-source:proxy\" , \"source:edr\" , \"log-source:endpoint-protection\" , \"source:siem\" , \"protocol: telnet\" , \"ex2\" ], \"flag\" : false , \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"florian@strangebee.com\" , \"customFields\" : { \"businessUnit\" : { \"string\" : \"Finance\" , \"order\" : 0 }, \"location\" : { \"string\" : \"Sydney\" , \"order\" : 1 } }, \"stats\" : {}, \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] }","title":"Merge"},{"location":"thehive/api/alert/merge/#merge","text":"Merge an Alert into an existing Case .","title":"Merge"},{"location":"thehive/api/alert/merge/#query","text":"POST /api/alert/{id1}/merge/{id2} With: id1 : id of the Alert to merge id2 : id of the destination Case","title":"Query"},{"location":"thehive/api/alert/merge/#response","text":"","title":"Response"},{"location":"thehive/api/alert/merge/#status-codes","text":"200 : if Alert is successfully merged 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/merge/#responsebody-example","text":"{ \"_id\" : \"~6658533455\" , \"id\" : \"~6658533455\" , \"createdBy\" : \"florian@strangebee.com\" , \"updatedBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620397519028 , \"updatedAt\" : 1624373852175 , \"_type\" : \"case\" , \"caseId\" : 114 , \"title\" : \"User connected to known malicious IP over Telnet / Malicious payload detected\" , \"description\" : \"EDR automated alert: the user robb@training.org has connected to known malicious IP over Telnet\\n\\nEDR automated alert: malicious payload detected on computer PC-Robb\\n \\n#### Merged with alert #90e044 User posted information on known phishing URL\\n\\nSIEM automated alert: the user robb@training.org has posted information on a known phishing url\" , \"severity\" : 2 , \"startDate\" : 1620396059728 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" : [ \"log-source:proxy\" , \"source:edr\" , \"log-source:endpoint-protection\" , \"source:siem\" , \"protocol: telnet\" , \"ex2\" ], \"flag\" : false , \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"florian@strangebee.com\" , \"customFields\" : { \"businessUnit\" : { \"string\" : \"Finance\" , \"order\" : 0 }, \"location\" : { \"string\" : \"Sydney\" , \"order\" : 1 } }, \"stats\" : {}, \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] }","title":"ResponseBody Example"},{"location":"thehive/api/alert/promote-as-case/","text":"Promote # Promote an Alert as a new Case . Query # POST /api/alert/{id}/createCase With: id : id of the Alert to promote Request Body example # Specify a Case template applied with Case creation: { \"caseTemplate\" : \"SIEM_Alert\" } The following fields are optional: caseTemplate : (String) Response # Status codes # 201 : if Case is successfully created 401 : Authentication error ResponseBody Example # { \"_id\" : \"~907709843\" , \"id\" : \"~907709843\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1630416621805 , \"updatedAt\" : null , \"_type\" : \"case\" , \"caseId\" : 126 , \"title\" : \"User posted information on known phishing URL\" , \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url. \" , \"severity\" : 2 , \"startDate\" : 1630416621797 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" : [ \"source:siem\" , \"log-source:proxy\" ], \"flag\" : false , \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"jerome@strangebee.com\" , \"customFields\" : { \"businessUnit\" : { \"string\" : \"Finance\" , \"order\" : 0 }, \"location\" : { \"string\" : \"Sydney\" , \"order\" : 1 } }, \"stats\" : {}, \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] }","title":"Promote"},{"location":"thehive/api/alert/promote-as-case/#promote","text":"Promote an Alert as a new Case .","title":"Promote"},{"location":"thehive/api/alert/promote-as-case/#query","text":"POST /api/alert/{id}/createCase With: id : id of the Alert to promote","title":"Query"},{"location":"thehive/api/alert/promote-as-case/#request-body-example","text":"Specify a Case template applied with Case creation: { \"caseTemplate\" : \"SIEM_Alert\" } The following fields are optional: caseTemplate : (String)","title":"Request Body example"},{"location":"thehive/api/alert/promote-as-case/#response","text":"","title":"Response"},{"location":"thehive/api/alert/promote-as-case/#status-codes","text":"201 : if Case is successfully created 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/promote-as-case/#responsebody-example","text":"{ \"_id\" : \"~907709843\" , \"id\" : \"~907709843\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1630416621805 , \"updatedAt\" : null , \"_type\" : \"case\" , \"caseId\" : 126 , \"title\" : \"User posted information on known phishing URL\" , \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url. \" , \"severity\" : 2 , \"startDate\" : 1630416621797 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" : [ \"source:siem\" , \"log-source:proxy\" ], \"flag\" : false , \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"jerome@strangebee.com\" , \"customFields\" : { \"businessUnit\" : { \"string\" : \"Finance\" , \"order\" : 0 }, \"location\" : { \"string\" : \"Sydney\" , \"order\" : 1 } }, \"stats\" : {}, \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] }","title":"ResponseBody Example"},{"location":"thehive/api/alert/read/","text":"Mark as Read/Unread # Mark an Alert as read Query # Mark as read # POST /api/alert/{id}/markAsRead with: id : id of the Alert Mark as unread # POST /api/alert/{id}/markAsUnead with: id : id of the Alert Response # Status codes # 200 : if Alert is updated successfully 401 : Authentication error ResponseBody Example # { \"_id\" : \"~911601872\" , \"id\" : \"~911601872\" , \"createdBy\" : \"florian@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1620333017135 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"external\" , \"source\" : \"SIEM\" , \"sourceRef\" : \"8257b4\" , \"externalLink\" : null , \"case\" : null , \"title\" : \"User posted information on known phishing URL\" , \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url\" , \"severity\" : 2 , \"date\" : 1620333017000 , \"tags\" : [ \"source:siem\" , \"log-source:proxy\" ], \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Ignored\" , \"follow\" : true , \"customFields\" : { \"businessUnit\" : { \"string\" : \"Finance\" }, \"location\" : { \"string\" : \"Sydney\" } }, \"caseTemplate\" : null , \"artifacts\" : [ { \"_id\" : \"~624226312\" , \"id\" : \"~624226312\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620333017175 , \"_type\" : \"case_artifact\" , \"dataType\" : \"mail\" , \"data\" : \"robb@training.org\" , \"startDate\" : 1620333017175 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~788742360\" , \"id\" : \"~788742360\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620333017168 , \"_type\" : \"case_artifact\" , \"dataType\" : \"url\" , \"data\" : \"https://moneyfornothing.pl-getbuys.icu/\" , \"startDate\" : 1620333017168 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"message\" : \"http method: POST\" , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~870416536\" , \"id\" : \"~870416536\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620333017157 , \"_type\" : \"case_artifact\" , \"dataType\" : \"ip\" , \"data\" : \"94.154.129.50\" , \"startDate\" : 1620333017157 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} } ], \"similarCases\" : [] }","title":"Mark as Read/Unread"},{"location":"thehive/api/alert/read/#mark-as-readunread","text":"Mark an Alert as read","title":"Mark as Read/Unread"},{"location":"thehive/api/alert/read/#query","text":"","title":"Query"},{"location":"thehive/api/alert/read/#mark-as-read","text":"POST /api/alert/{id}/markAsRead with: id : id of the Alert","title":"Mark as read"},{"location":"thehive/api/alert/read/#mark-as-unread","text":"POST /api/alert/{id}/markAsUnead with: id : id of the Alert","title":"Mark as unread"},{"location":"thehive/api/alert/read/#response","text":"","title":"Response"},{"location":"thehive/api/alert/read/#status-codes","text":"200 : if Alert is updated successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/read/#responsebody-example","text":"{ \"_id\" : \"~911601872\" , \"id\" : \"~911601872\" , \"createdBy\" : \"florian@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1620333017135 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"external\" , \"source\" : \"SIEM\" , \"sourceRef\" : \"8257b4\" , \"externalLink\" : null , \"case\" : null , \"title\" : \"User posted information on known phishing URL\" , \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url\" , \"severity\" : 2 , \"date\" : 1620333017000 , \"tags\" : [ \"source:siem\" , \"log-source:proxy\" ], \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Ignored\" , \"follow\" : true , \"customFields\" : { \"businessUnit\" : { \"string\" : \"Finance\" }, \"location\" : { \"string\" : \"Sydney\" } }, \"caseTemplate\" : null , \"artifacts\" : [ { \"_id\" : \"~624226312\" , \"id\" : \"~624226312\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620333017175 , \"_type\" : \"case_artifact\" , \"dataType\" : \"mail\" , \"data\" : \"robb@training.org\" , \"startDate\" : 1620333017175 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~788742360\" , \"id\" : \"~788742360\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620333017168 , \"_type\" : \"case_artifact\" , \"dataType\" : \"url\" , \"data\" : \"https://moneyfornothing.pl-getbuys.icu/\" , \"startDate\" : 1620333017168 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"message\" : \"http method: POST\" , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~870416536\" , \"id\" : \"~870416536\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620333017157 , \"_type\" : \"case_artifact\" , \"dataType\" : \"ip\" , \"data\" : \"94.154.129.50\" , \"startDate\" : 1620333017157 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} } ], \"similarCases\" : [] }","title":"ResponseBody Example"},{"location":"thehive/api/alert/run-responder/","text":"Run Responder # Run a Responder on an Alert . Query # POST /api/connector/cortex/action Request Body Example # { \"responderId\" : \"05521ec727f75d69e828604dc5ae4c03\" , \"objectType\" : \"alert\" , \"objectId\" : \"~947478656\" } The following fields are required: responderId : (String) objectType : \"alert\" objectId : (String) Response # Status codes # 200 : if Responder is run successfully 401 : Authentication error ResponseBody Example # { \"responderId\" : \"05521ec727f75d69e828604dc5ae4bed\" , \"responderName\" : \"JIRA_Create_Ticket_1_0\" , \"responderDefinition\" : \"JIRA_Create_Ticket_1_0\" , \"cortexId\" : \"CORTEX_INTERNAL\" , \"cortexJobId\" : \"_v2EnHsB8Pn57ilsukA3\" , \"objectType\" : \"Alert\" , \"objectId\" : \"~947478656\" , \"status\" : \"Waiting\" , \"startDate\" : 1630418550145 , \"operations\" : \"[]\" , \"report\" : \"{}\" }","title":"Run Responder"},{"location":"thehive/api/alert/run-responder/#run-responder","text":"Run a Responder on an Alert .","title":"Run Responder"},{"location":"thehive/api/alert/run-responder/#query","text":"POST /api/connector/cortex/action","title":"Query"},{"location":"thehive/api/alert/run-responder/#request-body-example","text":"{ \"responderId\" : \"05521ec727f75d69e828604dc5ae4c03\" , \"objectType\" : \"alert\" , \"objectId\" : \"~947478656\" } The following fields are required: responderId : (String) objectType : \"alert\" objectId : (String)","title":"Request Body Example"},{"location":"thehive/api/alert/run-responder/#response","text":"","title":"Response"},{"location":"thehive/api/alert/run-responder/#status-codes","text":"200 : if Responder is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/run-responder/#responsebody-example","text":"{ \"responderId\" : \"05521ec727f75d69e828604dc5ae4bed\" , \"responderName\" : \"JIRA_Create_Ticket_1_0\" , \"responderDefinition\" : \"JIRA_Create_Ticket_1_0\" , \"cortexId\" : \"CORTEX_INTERNAL\" , \"cortexJobId\" : \"_v2EnHsB8Pn57ilsukA3\" , \"objectType\" : \"Alert\" , \"objectId\" : \"~947478656\" , \"status\" : \"Waiting\" , \"startDate\" : 1630418550145 , \"operations\" : \"[]\" , \"report\" : \"{}\" }","title":"ResponseBody Example"},{"location":"thehive/api/alert/similar-cases/","text":"List similar Cases # List similar Cases . Query # POST /api/v1/query?name=alert-similar-cases Request Body Example # { \"query\" : [ { \"_name\" : \"getAlert\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"similarCases\" , \"caseFilter\" : { \"_field\" : \"status\" , \"_value\" : \"Open\" } } ] } with: id : id of the Alert. Response # Status codes # 200 : if query is successful 401 : Authentication error ResponseBody Example # [ { \"case\" : { \"_id\" : \"~665851112\" , \"_type\" : \"Case\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_updatedBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620397519028 , \"_updatedAt\" : 1624373852175 , \"number\" : 114 , \"title\" : \"User connected to known malicious IP over Telnet / Malicious payload detected\" , \"description\" : \"EDR automated alert: the user robb@training.org has connected to known malicious IP over Telnet\\n\\nEDR automated alert: malicious payload detected on computer PC-Robb\" , \"severity\" : 2 , \"startDate\" : 1620396059728 , \"tags\" : [ \"source:edr\" , \"protocol: telnet\" , \"log-source:endpoint-protection\" ], \"flag\" : false , \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Open\" , \"assignee\" : \"florian@strangebee.com\" , \"customFields\" : [], \"extraData\" : {} }, \"similarObservableCount\" : 1 , \"observableCount\" : 6 , \"similarIocCount\" : 0 , \"iocCount\" : 0 , \"observableTypes\" : { \"username\" : 1 } }, { \"case\" : { \"_id\" : \"~789202345\" , \"_type\" : \"Case\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620393185339 , \"number\" : 111 , \"title\" : \"Phishing -User posted information on known phishing URL\" , \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url\" , \"severity\" : 2 , \"startDate\" : 1620393185257 , \"tags\" : [ \"source:siem\" , \"log-source:proxy\" , \"category:Phishing\" ], \"flag\" : false , \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Open\" , \"assignee\" : \"florian@strangebee.com\" , \"customFields\" : [], \"extraData\" : {} }, \"similarObservableCount\" : 2 , \"observableCount\" : 4 , \"similarIocCount\" : 0 , \"iocCount\" : 1 , \"observableTypes\" : { \"username\" : 1 , \"mail\" : 1 } } ]","title":"List similar Cases"},{"location":"thehive/api/alert/similar-cases/#list-similar-cases","text":"List similar Cases .","title":"List similar Cases"},{"location":"thehive/api/alert/similar-cases/#query","text":"POST /api/v1/query?name=alert-similar-cases","title":"Query"},{"location":"thehive/api/alert/similar-cases/#request-body-example","text":"{ \"query\" : [ { \"_name\" : \"getAlert\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"similarCases\" , \"caseFilter\" : { \"_field\" : \"status\" , \"_value\" : \"Open\" } } ] } with: id : id of the Alert.","title":"Request Body Example"},{"location":"thehive/api/alert/similar-cases/#response","text":"","title":"Response"},{"location":"thehive/api/alert/similar-cases/#status-codes","text":"200 : if query is successful 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/similar-cases/#responsebody-example","text":"[ { \"case\" : { \"_id\" : \"~665851112\" , \"_type\" : \"Case\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_updatedBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620397519028 , \"_updatedAt\" : 1624373852175 , \"number\" : 114 , \"title\" : \"User connected to known malicious IP over Telnet / Malicious payload detected\" , \"description\" : \"EDR automated alert: the user robb@training.org has connected to known malicious IP over Telnet\\n\\nEDR automated alert: malicious payload detected on computer PC-Robb\" , \"severity\" : 2 , \"startDate\" : 1620396059728 , \"tags\" : [ \"source:edr\" , \"protocol: telnet\" , \"log-source:endpoint-protection\" ], \"flag\" : false , \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Open\" , \"assignee\" : \"florian@strangebee.com\" , \"customFields\" : [], \"extraData\" : {} }, \"similarObservableCount\" : 1 , \"observableCount\" : 6 , \"similarIocCount\" : 0 , \"iocCount\" : 0 , \"observableTypes\" : { \"username\" : 1 } }, { \"case\" : { \"_id\" : \"~789202345\" , \"_type\" : \"Case\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620393185339 , \"number\" : 111 , \"title\" : \"Phishing -User posted information on known phishing URL\" , \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url\" , \"severity\" : 2 , \"startDate\" : 1620393185257 , \"tags\" : [ \"source:siem\" , \"log-source:proxy\" , \"category:Phishing\" ], \"flag\" : false , \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Open\" , \"assignee\" : \"florian@strangebee.com\" , \"customFields\" : [], \"extraData\" : {} }, \"similarObservableCount\" : 2 , \"observableCount\" : 4 , \"similarIocCount\" : 0 , \"iocCount\" : 1 , \"observableTypes\" : { \"username\" : 1 , \"mail\" : 1 } } ]","title":"ResponseBody Example"},{"location":"thehive/api/alert/update-observable/","text":"Update observable # update an Alert Observable . Query # PATCH /api/alert/artifact/{id} With: id : Alert identifier Updatable fields are: tlp , ioc , sighted , tags , message , ignoreSimilarity Request Body Example # { \"ioc\" : True , \"tags\" :[ \"malicious\" ] } Response # Status codes # 200 : if Alert observable is updated successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 200 { \"_id\" : \"~1564784\" , \"id\" : \"~1564784\" , \"createdBy\" : \"analyst@soc\" , \"updatedBy\" : \"analyst@soc\" , \"createdAt\" : 1637091448338 , \"updatedAt\" : 1637092980667 , \"_type\" : \"case_artifact\" , \"dataType\" : \"ip\" , \"data\" : \"1.2.3.4\" , \"startDate\" : 1637091448338 , \"tlp\" : 2 , \"tags\" :[ \"malicious\" ], \"ioc\" : true , \"sighted\" : true , \"message\" : \"sample description\" , \"reports\" :{}, \"stats\" :{}, \"ignoreSimilarity\" : false } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Update observable"},{"location":"thehive/api/alert/update-observable/#update-observable","text":"update an Alert Observable .","title":"Update observable"},{"location":"thehive/api/alert/update-observable/#query","text":"PATCH /api/alert/artifact/{id} With: id : Alert identifier Updatable fields are: tlp , ioc , sighted , tags , message , ignoreSimilarity","title":"Query"},{"location":"thehive/api/alert/update-observable/#request-body-example","text":"{ \"ioc\" : True , \"tags\" :[ \"malicious\" ] }","title":"Request Body Example"},{"location":"thehive/api/alert/update-observable/#response","text":"","title":"Response"},{"location":"thehive/api/alert/update-observable/#status-codes","text":"200 : if Alert observable is updated successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/alert/update-observable/#responsebody-example","text":"200 { \"_id\" : \"~1564784\" , \"id\" : \"~1564784\" , \"createdBy\" : \"analyst@soc\" , \"updatedBy\" : \"analyst@soc\" , \"createdAt\" : 1637091448338 , \"updatedAt\" : 1637092980667 , \"_type\" : \"case_artifact\" , \"dataType\" : \"ip\" , \"data\" : \"1.2.3.4\" , \"startDate\" : 1637091448338 , \"tlp\" : 2 , \"tags\" :[ \"malicious\" ], \"ioc\" : true , \"sighted\" : true , \"message\" : \"sample description\" , \"reports\" :{}, \"stats\" :{}, \"ignoreSimilarity\" : false } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"ResponseBody Example"},{"location":"thehive/api/alert/update/","text":"Update # Update an Alert . Query # PATCH /api/alert/{id} with: id : id of the Alert Request Body Example # { \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url. \" } Response # Status codes # 200 : if Alert is updated successfully 401 : Authentication error ResponseBody Example # { \"_id\" : \"~624443400\" , \"id\" : \"~624443400\" , \"createdBy\" : \"florian@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1620373264377 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"external\" , \"source\" : \"SIEM\" , \"sourceRef\" : \"47e379\" , \"externalLink\" : null , \"case\" : null , \"title\" : \"User posted information on known phishing URL\" , \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url. \" , \"severity\" : 2 , \"date\" : 1620373264000 , \"tags\" : [ \"source:siem\" , \"log-source:proxy\" ], \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Ignored\" , \"follow\" : true , \"customFields\" : { \"businessUnit\" : { \"string\" : \"Finance\" }, \"location\" : { \"string\" : \"Sydney\" } }, \"caseTemplate\" : null , \"artifacts\" : [ { \"_id\" : \"~665772152\" , \"id\" : \"~665772152\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620373264410 , \"_type\" : \"case_artifact\" , \"dataType\" : \"username\" , \"data\" : \"robb@training.org\" , \"startDate\" : 1620373264410 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~677015568\" , \"id\" : \"~677015568\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620373264398 , \"_type\" : \"case_artifact\" , \"dataType\" : \"domain\" , \"data\" : \"pl-getbuys.icu\" , \"startDate\" : 1620373264398 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~677019664\" , \"id\" : \"~677019664\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620373264405 , \"_type\" : \"case_artifact\" , \"dataType\" : \"mail\" , \"data\" : \"robb@training.org\" , \"startDate\" : 1620373264405 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~706650224\" , \"id\" : \"~706650224\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620373264391 , \"_type\" : \"case_artifact\" , \"dataType\" : \"url\" , \"data\" : \"https://poczta.pl-getbuys.icu/\" , \"startDate\" : 1620373264391 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"message\" : \"http method: POST\" , \"reports\" : {}, \"stats\" : {} } ], \"similarCases\" : [] }","title":"Update"},{"location":"thehive/api/alert/update/#update","text":"Update an Alert .","title":"Update"},{"location":"thehive/api/alert/update/#query","text":"PATCH /api/alert/{id} with: id : id of the Alert","title":"Query"},{"location":"thehive/api/alert/update/#request-body-example","text":"{ \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url. \" }","title":"Request Body Example"},{"location":"thehive/api/alert/update/#response","text":"","title":"Response"},{"location":"thehive/api/alert/update/#status-codes","text":"200 : if Alert is updated successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/alert/update/#responsebody-example","text":"{ \"_id\" : \"~624443400\" , \"id\" : \"~624443400\" , \"createdBy\" : \"florian@strangebee.com\" , \"updatedBy\" : null , \"createdAt\" : 1620373264377 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"external\" , \"source\" : \"SIEM\" , \"sourceRef\" : \"47e379\" , \"externalLink\" : null , \"case\" : null , \"title\" : \"User posted information on known phishing URL\" , \"description\" : \"SIEM automated alert: the user robb@training.org has posted information on a known phishing url. \" , \"severity\" : 2 , \"date\" : 1620373264000 , \"tags\" : [ \"source:siem\" , \"log-source:proxy\" ], \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Ignored\" , \"follow\" : true , \"customFields\" : { \"businessUnit\" : { \"string\" : \"Finance\" }, \"location\" : { \"string\" : \"Sydney\" } }, \"caseTemplate\" : null , \"artifacts\" : [ { \"_id\" : \"~665772152\" , \"id\" : \"~665772152\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620373264410 , \"_type\" : \"case_artifact\" , \"dataType\" : \"username\" , \"data\" : \"robb@training.org\" , \"startDate\" : 1620373264410 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~677015568\" , \"id\" : \"~677015568\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620373264398 , \"_type\" : \"case_artifact\" , \"dataType\" : \"domain\" , \"data\" : \"pl-getbuys.icu\" , \"startDate\" : 1620373264398 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~677019664\" , \"id\" : \"~677019664\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620373264405 , \"_type\" : \"case_artifact\" , \"dataType\" : \"mail\" , \"data\" : \"robb@training.org\" , \"startDate\" : 1620373264405 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"reports\" : {}, \"stats\" : {} }, { \"_id\" : \"~706650224\" , \"id\" : \"~706650224\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1620373264391 , \"_type\" : \"case_artifact\" , \"dataType\" : \"url\" , \"data\" : \"https://poczta.pl-getbuys.icu/\" , \"startDate\" : 1620373264391 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : false , \"sighted\" : false , \"message\" : \"http method: POST\" , \"reports\" : {}, \"stats\" : {} } ], \"similarCases\" : [] }","title":"ResponseBody Example"},{"location":"thehive/api/case/","text":"Case APIs # Create case Update case Delete case Merge cases Export case to MISP List related case List related alerts List attachments Run responder List responder jobs","title":"Overview"},{"location":"thehive/api/case/#case-apis","text":"Create case Update case Delete case Merge cases Export case to MISP List related case List related alerts List attachments Run responder List responder jobs","title":"Case APIs"},{"location":"thehive/api/case/attachments/","text":"List related Alerts # List attachments added to task logs of a Case . Query # POST /api/v0/query Request Body Example # List attachments added to task logs or a Case identified by {id} : { \"query\" : [ { \"_name\" : \"getCase\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"tasks\" }, { \"_name\" : \"filter\" , \"_ne\" : { \"_field\" : \"status\" , \"_value\" : \"Cancel\" } }, { \"_name\" : \"logs\" }, { \"_contains\" : \"attachment.id\" , \"_name\" : \"filter\" }, { \"_name\" : \"page\" , \"extraData\" : [ \"taskId\" ], \"from\" : 0 , \"to\" : 100 } ] } With: id : id of the Case Response # Status codes # 200 : if query is run successfully 401 : Authentication error 404 : if the Case is not found Response Body Example # [ ... { \"_id\" : \"~122892472\" , \"id\" : \"~122892472\" , \"createdBy\" : \"user@thehive.local\" , \"createdAt\" : 1632124353194 , \"_type\" : \"case_task_log\" , \"message\" : \"message\" , \"startDate\" : 1632124353194 , \"attachment\" : { \"name\" : \"filename.png\" , \"hashes\" : [ \"0b62003cb73578d9e738a70aa7a81e89d3683282ac393856a96ef364cd1038cb\" , \"caa75ff1e33ee8bfba764c9a6139fb72e7f4e20a\" , \"a3e41c32ff817fc759bafeb1a106a433\" ], \"size\" : 42213 , \"contentType\" : \"image/png\" , \"id\" : \"0b62003cb73578d9e738a70aa7a81e89d3683282ac393856a96ef364cd1038cb\" }, \"status\" : \"Ok\" , \"owner\" : \"user@thehive.local\" } ... ]","title":"List related Alerts"},{"location":"thehive/api/case/attachments/#list-related-alerts","text":"List attachments added to task logs of a Case .","title":"List related Alerts"},{"location":"thehive/api/case/attachments/#query","text":"POST /api/v0/query","title":"Query"},{"location":"thehive/api/case/attachments/#request-body-example","text":"List attachments added to task logs or a Case identified by {id} : { \"query\" : [ { \"_name\" : \"getCase\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"tasks\" }, { \"_name\" : \"filter\" , \"_ne\" : { \"_field\" : \"status\" , \"_value\" : \"Cancel\" } }, { \"_name\" : \"logs\" }, { \"_contains\" : \"attachment.id\" , \"_name\" : \"filter\" }, { \"_name\" : \"page\" , \"extraData\" : [ \"taskId\" ], \"from\" : 0 , \"to\" : 100 } ] } With: id : id of the Case","title":"Request Body Example"},{"location":"thehive/api/case/attachments/#response","text":"","title":"Response"},{"location":"thehive/api/case/attachments/#status-codes","text":"200 : if query is run successfully 401 : Authentication error 404 : if the Case is not found","title":"Status codes"},{"location":"thehive/api/case/attachments/#response-body-example","text":"[ ... { \"_id\" : \"~122892472\" , \"id\" : \"~122892472\" , \"createdBy\" : \"user@thehive.local\" , \"createdAt\" : 1632124353194 , \"_type\" : \"case_task_log\" , \"message\" : \"message\" , \"startDate\" : 1632124353194 , \"attachment\" : { \"name\" : \"filename.png\" , \"hashes\" : [ \"0b62003cb73578d9e738a70aa7a81e89d3683282ac393856a96ef364cd1038cb\" , \"caa75ff1e33ee8bfba764c9a6139fb72e7f4e20a\" , \"a3e41c32ff817fc759bafeb1a106a433\" ], \"size\" : 42213 , \"contentType\" : \"image/png\" , \"id\" : \"0b62003cb73578d9e738a70aa7a81e89d3683282ac393856a96ef364cd1038cb\" }, \"status\" : \"Ok\" , \"owner\" : \"user@thehive.local\" } ... ]","title":"Response Body Example"},{"location":"thehive/api/case/create/","text":"Create # Create a Case Query # POST /api/case With mandatory fields: title : (String) title of the Case description : (String) description of the Case Request Body Example # Basic request # { \"title\" : \"my first case\" , \"description\" : \"my first case description\" } Request with more details, customFields and tasks # { \"title\" : \"my first case\" , \"description\" : \"my first case description\" , \"Severity\" : 3 , \"tlp\" : 2 , \"pap\" : 2 , \"startDate\" : 1635876967233 , \"tags\" : [ \"Test Tag\" , \"Another Test Tag\" ], \"flag\" : false , \"owner\" : \"username@org\" , \"tasks\" : [{ \"title\" : \"mytask\" , \"description\" : \"description of my task\" }], \"customFields\" :{ \"cvss\" : { \"integer\" : 9 }, \"businessUnit\" : { \"string\" : \"Sales\" } } } Response # Status code # 201 : if Case is created successfully 401 : Authentication error 403 : Authorization error Response Body Example # 201 { \"_id\" : \"~41644112\" , \"id\" : \"~41644112\" , \"createdBy\" : \"user@org\" , \"updatedBy\" : null , \"createdAt\" : 1635876967235 , \"updatedAt\" : null , \"_type\" : \"case\" , \"caseId\" : 4 , \"title\" : \"my first case\" , \"description\" : \"my first case description\" , \"severity\" : 2 , \"startDate\" : 1635876967233 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" :[], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"user@org\" , \"customFields\" :{}, \"stats\" :{}, \"permissions\" :[ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Create"},{"location":"thehive/api/case/create/#create","text":"Create a Case","title":"Create"},{"location":"thehive/api/case/create/#query","text":"POST /api/case With mandatory fields: title : (String) title of the Case description : (String) description of the Case","title":"Query"},{"location":"thehive/api/case/create/#request-body-example","text":"","title":"Request Body Example"},{"location":"thehive/api/case/create/#basic-request","text":"{ \"title\" : \"my first case\" , \"description\" : \"my first case description\" }","title":"Basic request"},{"location":"thehive/api/case/create/#request-with-more-details-customfields-and-tasks","text":"{ \"title\" : \"my first case\" , \"description\" : \"my first case description\" , \"Severity\" : 3 , \"tlp\" : 2 , \"pap\" : 2 , \"startDate\" : 1635876967233 , \"tags\" : [ \"Test Tag\" , \"Another Test Tag\" ], \"flag\" : false , \"owner\" : \"username@org\" , \"tasks\" : [{ \"title\" : \"mytask\" , \"description\" : \"description of my task\" }], \"customFields\" :{ \"cvss\" : { \"integer\" : 9 }, \"businessUnit\" : { \"string\" : \"Sales\" } } }","title":"Request with more details, customFields and tasks"},{"location":"thehive/api/case/create/#response","text":"","title":"Response"},{"location":"thehive/api/case/create/#status-code","text":"201 : if Case is created successfully 401 : Authentication error 403 : Authorization error","title":"Status code"},{"location":"thehive/api/case/create/#response-body-example","text":"201 { \"_id\" : \"~41644112\" , \"id\" : \"~41644112\" , \"createdBy\" : \"user@org\" , \"updatedBy\" : null , \"createdAt\" : 1635876967235 , \"updatedAt\" : null , \"_type\" : \"case\" , \"caseId\" : 4 , \"title\" : \"my first case\" , \"description\" : \"my first case description\" , \"severity\" : 2 , \"startDate\" : 1635876967233 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" :[], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"user@org\" , \"customFields\" :{}, \"stats\" :{}, \"permissions\" :[ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Response Body Example"},{"location":"thehive/api/case/delete/","text":"Delete # Permanently delete a Case . Query # DELETE /api/case/{id}?force=1 With: id : id of the Case Response # Status codes # 204 : if Case is deleted successfully 401 : Authentication error 404 : if Case is not found","title":"Delete"},{"location":"thehive/api/case/delete/#delete","text":"Permanently delete a Case .","title":"Delete"},{"location":"thehive/api/case/delete/#query","text":"DELETE /api/case/{id}?force=1 With: id : id of the Case","title":"Query"},{"location":"thehive/api/case/delete/#response","text":"","title":"Response"},{"location":"thehive/api/case/delete/#status-codes","text":"204 : if Case is deleted successfully 401 : Authentication error 404 : if Case is not found","title":"Status codes"},{"location":"thehive/api/case/export/","text":"Export Case to MISP # Export Case to a MISP server to create an event including the Case observables marked as IOC. Query # POST /api/connector/misp/export/{id}/{misp-server} With: id : id of the Case misp-server : name of the MISP server as defined in the configuration Note Only MISP servers with purpose equals to ExportOnly or ImportAndExport can recieve Case exports Response # Status codes # 204 : if Case is successfully exported 401 : Authentication error 404 : if Case or MISP server is not found.","title":"Export Case to MISP"},{"location":"thehive/api/case/export/#export-case-to-misp","text":"Export Case to a MISP server to create an event including the Case observables marked as IOC.","title":"Export Case to MISP"},{"location":"thehive/api/case/export/#query","text":"POST /api/connector/misp/export/{id}/{misp-server} With: id : id of the Case misp-server : name of the MISP server as defined in the configuration Note Only MISP servers with purpose equals to ExportOnly or ImportAndExport can recieve Case exports","title":"Query"},{"location":"thehive/api/case/export/#response","text":"","title":"Response"},{"location":"thehive/api/case/export/#status-codes","text":"204 : if Case is successfully exported 401 : Authentication error 404 : if Case or MISP server is not found.","title":"Status codes"},{"location":"thehive/api/case/merge/","text":"Merge # Merge two Cases in a single Case . This APIs permanently removes the source Cases and creates a Case by merging all the data from the sources. Query # POST /api/v0/case/{id1}/_merge/{id2} with: id1 : id of the first Case id2 : id of the second Case Response # Status codes # 204 : if the Cases are merged successfully 401 : Authentication error 404 : if at least one of the Cases is not found Response Body Example # { \"_id\" : \"~81928240\" , \"id\" : \"~81928240\" , \"createdBy\" : \"user@thehive.local\" , \"updatedBy\" : null , \"createdAt\" : 1632132365250 , \"updatedAt\" : null , \"_type\" : \"case\" , \"caseId\" : 87 , \"title\" : \"Case 1 / Case 2\" , \"description\" : \"test\\n\\ntest\" , \"severity\" : 2 , \"startDate\" : 1632124020000 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" : [], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"user@thehive.local\" , \"customFields\" : {}, \"stats\" : {}, \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] }","title":"Merge"},{"location":"thehive/api/case/merge/#merge","text":"Merge two Cases in a single Case . This APIs permanently removes the source Cases and creates a Case by merging all the data from the sources.","title":"Merge"},{"location":"thehive/api/case/merge/#query","text":"POST /api/v0/case/{id1}/_merge/{id2} with: id1 : id of the first Case id2 : id of the second Case","title":"Query"},{"location":"thehive/api/case/merge/#response","text":"","title":"Response"},{"location":"thehive/api/case/merge/#status-codes","text":"204 : if the Cases are merged successfully 401 : Authentication error 404 : if at least one of the Cases is not found","title":"Status codes"},{"location":"thehive/api/case/merge/#response-body-example","text":"{ \"_id\" : \"~81928240\" , \"id\" : \"~81928240\" , \"createdBy\" : \"user@thehive.local\" , \"updatedBy\" : null , \"createdAt\" : 1632132365250 , \"updatedAt\" : null , \"_type\" : \"case\" , \"caseId\" : 87 , \"title\" : \"Case 1 / Case 2\" , \"description\" : \"test\\n\\ntest\" , \"severity\" : 2 , \"startDate\" : 1632124020000 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" : [], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"user@thehive.local\" , \"customFields\" : {}, \"stats\" : {}, \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] }","title":"Response Body Example"},{"location":"thehive/api/case/related-alerts/","text":"List related Alerts # List alerts merged in a Case . Query # POST /api/v0/query Request Body Example # List last 5 merged alerts in a Case identified by {id} : { \"query\" : [ { \"_name\" : \"getCase\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"alerts\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 5 } ] } With: id : id of the Case Response # Status codes # 200 : if query is run successfully 401 : Authentication error ResponseBody Example # [ ... [ { \"_id\" : \"~43618512\" , \"id\" : \"~43618512\" , \"createdBy\" : \"demo@thehive.local\" , \"updatedBy\" : null , \"createdAt\" : 1618344277475 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"testing\" , \"source\" : \"create-alert.py\" , \"sourceRef\" : \"85a766ec\" , \"externalLink\" : null , \"case\" : \"~122884120\" , \"title\" : \"Alert 85a766ec-060a-49a0-bc82-c672b6e51e6c\" , \"description\" : \"N/A\" , \"severity\" : 1 , \"date\" : 1618344277000 , \"tags\" : [ \"sample\" ], \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Imported\" , \"follow\" : true , \"customFields\" : { \"company\" : { \"string\" : \"Customer 1\" } }, \"caseTemplate\" : null , \"artifacts\" : [], \"similarCases\" : [] } ] ... ]","title":"List related Alerts"},{"location":"thehive/api/case/related-alerts/#list-related-alerts","text":"List alerts merged in a Case .","title":"List related Alerts"},{"location":"thehive/api/case/related-alerts/#query","text":"POST /api/v0/query","title":"Query"},{"location":"thehive/api/case/related-alerts/#request-body-example","text":"List last 5 merged alerts in a Case identified by {id} : { \"query\" : [ { \"_name\" : \"getCase\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"alerts\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 5 } ] } With: id : id of the Case","title":"Request Body Example"},{"location":"thehive/api/case/related-alerts/#response","text":"","title":"Response"},{"location":"thehive/api/case/related-alerts/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/case/related-alerts/#responsebody-example","text":"[ ... [ { \"_id\" : \"~43618512\" , \"id\" : \"~43618512\" , \"createdBy\" : \"demo@thehive.local\" , \"updatedBy\" : null , \"createdAt\" : 1618344277475 , \"updatedAt\" : null , \"_type\" : \"alert\" , \"type\" : \"testing\" , \"source\" : \"create-alert.py\" , \"sourceRef\" : \"85a766ec\" , \"externalLink\" : null , \"case\" : \"~122884120\" , \"title\" : \"Alert 85a766ec-060a-49a0-bc82-c672b6e51e6c\" , \"description\" : \"N/A\" , \"severity\" : 1 , \"date\" : 1618344277000 , \"tags\" : [ \"sample\" ], \"tlp\" : 3 , \"pap\" : 2 , \"status\" : \"Imported\" , \"follow\" : true , \"customFields\" : { \"company\" : { \"string\" : \"Customer 1\" } }, \"caseTemplate\" : null , \"artifacts\" : [], \"similarCases\" : [] } ] ... ]","title":"ResponseBody Example"},{"location":"thehive/api/case/related-cases/","text":"List related Cases # List similar Cases of a given Case . This API uses observable based similarity to find related Cases Query # GET /api/case/{id}/links With: id : id of the Case Response # Status codes # 200 : if query is run successfully 401 : Authentication error 404 : if the case doesn't exist ResponseBody Example # [ { \"_id\" : \"~48144448\" , \"_type\" : \"case\" , \"caseId\" : 66 , \"createdAt\" : 1618344529302 , \"createdBy\" : \"user@thehive.local\" , \"customFields\" : {}, \"description\" : \"N/A\" , \"endDate\" : null , \"flag\" : false , \"id\" : \"~48144448\" , \"impactStatus\" : null , \"linkedWith\" : [ { \"_id\" : \"~122888216\" , \"_type\" : \"case_artifact\" , \"createdAt\" : 1632114988895 , \"createdBy\" : \"user@strangebee.com\" , \"data\" : \"google.com\" , \"dataType\" : \"domain\" , \"id\" : \"~122888216\" , \"ignoreSimilarity\" : false , \"ioc\" : false , \"message\" : \"test\" , \"reports\" : {}, \"sighted\" : false , \"startDate\" : 1632114988895 , \"stats\" : {}, \"tags\" : [], \"tlp\" : 2 } ], \"linksCount\" : 1 , \"owner\" : \"nabil@thehive.local\" , \"pap\" : 1 , \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ], \"resolutionStatus\" : null , \"severity\" : 4 , \"startDate\" : 1618344529000 , \"stats\" : {}, \"status\" : \"Open\" , \"summary\" : null , \"tags\" : [ \"sample\" ], \"title\" : \"Case a31acfad-8368-4395-bf1d-6d5c1675c0ba\" , \"tlp\" : 1 , \"updatedAt\" : null , \"updatedBy\" : null } ]","title":"List related Cases"},{"location":"thehive/api/case/related-cases/#list-related-cases","text":"List similar Cases of a given Case . This API uses observable based similarity to find related Cases","title":"List related Cases"},{"location":"thehive/api/case/related-cases/#query","text":"GET /api/case/{id}/links With: id : id of the Case","title":"Query"},{"location":"thehive/api/case/related-cases/#response","text":"","title":"Response"},{"location":"thehive/api/case/related-cases/#status-codes","text":"200 : if query is run successfully 401 : Authentication error 404 : if the case doesn't exist","title":"Status codes"},{"location":"thehive/api/case/related-cases/#responsebody-example","text":"[ { \"_id\" : \"~48144448\" , \"_type\" : \"case\" , \"caseId\" : 66 , \"createdAt\" : 1618344529302 , \"createdBy\" : \"user@thehive.local\" , \"customFields\" : {}, \"description\" : \"N/A\" , \"endDate\" : null , \"flag\" : false , \"id\" : \"~48144448\" , \"impactStatus\" : null , \"linkedWith\" : [ { \"_id\" : \"~122888216\" , \"_type\" : \"case_artifact\" , \"createdAt\" : 1632114988895 , \"createdBy\" : \"user@strangebee.com\" , \"data\" : \"google.com\" , \"dataType\" : \"domain\" , \"id\" : \"~122888216\" , \"ignoreSimilarity\" : false , \"ioc\" : false , \"message\" : \"test\" , \"reports\" : {}, \"sighted\" : false , \"startDate\" : 1632114988895 , \"stats\" : {}, \"tags\" : [], \"tlp\" : 2 } ], \"linksCount\" : 1 , \"owner\" : \"nabil@thehive.local\" , \"pap\" : 1 , \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ], \"resolutionStatus\" : null , \"severity\" : 4 , \"startDate\" : 1618344529000 , \"stats\" : {}, \"status\" : \"Open\" , \"summary\" : null , \"tags\" : [ \"sample\" ], \"title\" : \"Case a31acfad-8368-4395-bf1d-6d5c1675c0ba\" , \"tlp\" : 1 , \"updatedAt\" : null , \"updatedBy\" : null } ]","title":"ResponseBody Example"},{"location":"thehive/api/case/responder-jobs/","text":"List responder actions # List actions run on a Case . Query # GET /api/connector/cortex/action/case/{id} With: id : Case identifier Response # Status codes # 200 : if query is run successfully 401 : Authentication error Response Body Example # 200 [ { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Case\" , \"objectId\" : \"~25313328\" , \"status\" : \"Success\" , \"startDate\" : 1630917246993 , \"endDate\" : 1630917254406 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":true,\\\"artifacts\\\":[],\\\"operations\\\":[],\\\\\\\"message\\\\\\\":\\\\\\\"Ok\\\\\\\",\\\\\\\"parameters\\\\\\\":{\\\\\\\"organisation\\\\\\\":\\\\\\\"StrangeBee\\\\\\\",\\\\\\\"user\\\\\\\":\\\\\\\"user@thehive.local\\\\\\\"},\\\\\\\"config\\\\\\\":{\\\\\\\"proxy_https\\\\\\\":null,\\\\\\\"cacerts\\\\\\\":null,\\\\\\\"check_tlp\\\\\\\":false,\\\\\\\"max_tlp\\\\\\\":2,\\\\\\\"check_pap\\\\\\\":false,\\\\\\\"max_pap\\\\\\\":2,\\\\\\\"jobTimeout\\\\\\\":30,\\\\\\\"proxy_http\\\\\\\":null}}\\\"}\" } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } List available Responders # Request # To get the list of Responders available for a Case , based on its TLP and PAP, you can call the following API: GET /api/connector/cortex/responder/case/{id} With: id : Case identifier Response # 200 [ { \"id\" : \"e33d63082066c739c07d2bbc199bfe7e\" , \"name\" : \"MALSPAM_Reply_to_user_1_0\" , \"version\" : \"1.0\" , \"description\" : \"Reply to user with an email. Applies on tasks\" , \"dataTypeList\" : [ \"thehive:case\" , \"thehive:case_task\" , \"thehive:case_task_log\" ], \"cortexIds\" : [ \"Demo\" ] } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"List responder actions"},{"location":"thehive/api/case/responder-jobs/#list-responder-actions","text":"List actions run on a Case .","title":"List responder actions"},{"location":"thehive/api/case/responder-jobs/#query","text":"GET /api/connector/cortex/action/case/{id} With: id : Case identifier","title":"Query"},{"location":"thehive/api/case/responder-jobs/#response","text":"","title":"Response"},{"location":"thehive/api/case/responder-jobs/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/case/responder-jobs/#response-body-example","text":"200 [ { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Case\" , \"objectId\" : \"~25313328\" , \"status\" : \"Success\" , \"startDate\" : 1630917246993 , \"endDate\" : 1630917254406 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":true,\\\"artifacts\\\":[],\\\"operations\\\":[],\\\\\\\"message\\\\\\\":\\\\\\\"Ok\\\\\\\",\\\\\\\"parameters\\\\\\\":{\\\\\\\"organisation\\\\\\\":\\\\\\\"StrangeBee\\\\\\\",\\\\\\\"user\\\\\\\":\\\\\\\"user@thehive.local\\\\\\\"},\\\\\\\"config\\\\\\\":{\\\\\\\"proxy_https\\\\\\\":null,\\\\\\\"cacerts\\\\\\\":null,\\\\\\\"check_tlp\\\\\\\":false,\\\\\\\"max_tlp\\\\\\\":2,\\\\\\\"check_pap\\\\\\\":false,\\\\\\\"max_pap\\\\\\\":2,\\\\\\\"jobTimeout\\\\\\\":30,\\\\\\\"proxy_http\\\\\\\":null}}\\\"}\" } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"Response Body Example"},{"location":"thehive/api/case/responder-jobs/#list-available-responders","text":"","title":"List available Responders"},{"location":"thehive/api/case/responder-jobs/#request","text":"To get the list of Responders available for a Case , based on its TLP and PAP, you can call the following API: GET /api/connector/cortex/responder/case/{id} With: id : Case identifier","title":"Request"},{"location":"thehive/api/case/responder-jobs/#response_1","text":"200 [ { \"id\" : \"e33d63082066c739c07d2bbc199bfe7e\" , \"name\" : \"MALSPAM_Reply_to_user_1_0\" , \"version\" : \"1.0\" , \"description\" : \"Reply to user with an email. Applies on tasks\" , \"dataTypeList\" : [ \"thehive:case\" , \"thehive:case_task\" , \"thehive:case_task_log\" ], \"cortexIds\" : [ \"Demo\" ] } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"Response"},{"location":"thehive/api/case/run-responder/","text":"Run responder # Run a responder on a Case (requires manageAction permission). Query # POST /api/connector/cortex/action Request Body Example # { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"cortexId\" : \"local-cortex\" , \"objectType\" : \"case\" , \"objectId\" : \"{id}\" } With: id : Case identifier The required fields are responderId , objectType and objectId . Response # Status codes # 201 : if responder is started successfully 401 : Authentication error 403 : Authorization error 404 : Case is not found Response Body Example # 201 { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Case\" , \"objectId\" : \"~25313328\" , \"status\" : \"Waiting\" , \"startDate\" : 1630917246993 , \"operations\" : \"[]\" , \"report\" : \"{}\" } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 404 { \"type\" : \"AuthenticationError\" , \"message\" : \"Task not found\" }","title":"Run responder"},{"location":"thehive/api/case/run-responder/#run-responder","text":"Run a responder on a Case (requires manageAction permission).","title":"Run responder"},{"location":"thehive/api/case/run-responder/#query","text":"POST /api/connector/cortex/action","title":"Query"},{"location":"thehive/api/case/run-responder/#request-body-example","text":"{ \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"cortexId\" : \"local-cortex\" , \"objectType\" : \"case\" , \"objectId\" : \"{id}\" } With: id : Case identifier The required fields are responderId , objectType and objectId .","title":"Request Body Example"},{"location":"thehive/api/case/run-responder/#response","text":"","title":"Response"},{"location":"thehive/api/case/run-responder/#status-codes","text":"201 : if responder is started successfully 401 : Authentication error 403 : Authorization error 404 : Case is not found","title":"Status codes"},{"location":"thehive/api/case/run-responder/#response-body-example","text":"201 { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Case\" , \"objectId\" : \"~25313328\" , \"status\" : \"Waiting\" , \"startDate\" : 1630917246993 , \"operations\" : \"[]\" , \"report\" : \"{}\" } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 404 { \"type\" : \"AuthenticationError\" , \"message\" : \"Task not found\" }","title":"Response Body Example"},{"location":"thehive/api/case/update/","text":"Update # Update a Case Query # PATCH /api/case/{id} Request Body Example # Case details update # { \"severity\" : 3 , \"tags\" : [ \"updated\" ] } Case customFields update # To update specific customFields : { \"customFields.business-unit.string\" : \"VIP\" , \"customFields.cvss.integer\" : 3 } To patch Case customFields : \"customFields\" : { \"business-unit\" : { \"string\" : \"VIP\" } } Danger Case customFields not mentionned in this request will be erased. Response # Status codes # 200 : Case has been updated successfully 401 : Authentication error 403 : Authorization error Response Body Example # 201 { \"_id\" : \"~311352\" , \"id\" : \"~311352\" , \"createdBy\" : \"analyst@soc\" , \"updatedBy\" : \"analyst@soc\" , \"createdAt\" : 1635879111239 , \"updatedAt\" : 1637083041511 , \"_type\" : \"case\" , \"caseId\" : 6 , \"title\" : \"my first case\" , \"description\" : \"my first case description\" , \"severity\" : 3 , \"startDate\" : 1635876967233 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" :[ \"updated\" ], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"analyst@soc\" , \"customFields\" :{ \"business-unit\" :{ \"string\" : \"Sales\" , \"order\" : 1 }, \"cvss\" :{ \"integer\" : 9 , \"order\" : 0 } }, \"stats\" :{}, \"permissions\" :[ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Update"},{"location":"thehive/api/case/update/#update","text":"Update a Case","title":"Update"},{"location":"thehive/api/case/update/#query","text":"PATCH /api/case/{id}","title":"Query"},{"location":"thehive/api/case/update/#request-body-example","text":"","title":"Request Body Example"},{"location":"thehive/api/case/update/#case-details-update","text":"{ \"severity\" : 3 , \"tags\" : [ \"updated\" ] }","title":"Case details update"},{"location":"thehive/api/case/update/#case-customfields-update","text":"To update specific customFields : { \"customFields.business-unit.string\" : \"VIP\" , \"customFields.cvss.integer\" : 3 } To patch Case customFields : \"customFields\" : { \"business-unit\" : { \"string\" : \"VIP\" } } Danger Case customFields not mentionned in this request will be erased.","title":"Case customFields update"},{"location":"thehive/api/case/update/#response","text":"","title":"Response"},{"location":"thehive/api/case/update/#status-codes","text":"200 : Case has been updated successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/case/update/#response-body-example","text":"201 { \"_id\" : \"~311352\" , \"id\" : \"~311352\" , \"createdBy\" : \"analyst@soc\" , \"updatedBy\" : \"analyst@soc\" , \"createdAt\" : 1635879111239 , \"updatedAt\" : 1637083041511 , \"_type\" : \"case\" , \"caseId\" : 6 , \"title\" : \"my first case\" , \"description\" : \"my first case description\" , \"severity\" : 3 , \"startDate\" : 1635876967233 , \"endDate\" : null , \"impactStatus\" : null , \"resolutionStatus\" : null , \"tags\" :[ \"updated\" ], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"status\" : \"Open\" , \"summary\" : null , \"owner\" : \"analyst@soc\" , \"customFields\" :{ \"business-unit\" :{ \"string\" : \"Sales\" , \"order\" : 1 }, \"cvss\" :{ \"integer\" : 9 , \"order\" : 0 } }, \"stats\" :{}, \"permissions\" :[ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCaseTemplate\" , \"manageCase\" , \"manageUser\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageTag\" , \"manageConfig\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ] } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Response Body Example"},{"location":"thehive/api/case-template/","text":"Case template APIs # List case templates Create case template Delete case template Update case template","title":"Overview"},{"location":"thehive/api/case-template/#case-template-apis","text":"List case templates Create case template Delete case template Update case template","title":"Case template APIs"},{"location":"thehive/api/case-template/create/","text":"Create # Create a Case Templates . Query # POST /api/case/template Request Body Example # { \"name\" : \"MISPEvent\" , \"titlePrefix\" : \"\" , \"severity\" : 2 , \"tlp\" : 2 , \"pap\" : 2 , \"tags\" : [ \"hunting\" ], \"tasks\" : [ { \"order\" : 0 , \"title\" : \"Search for IOCs on Mail gateway logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in Mail gateway logs and look for IOcs of type IP, email addresses, hostnames, free text. \" }, { \"order\" : 1 , \"title\" : \"Search for IOCs on Firewall logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in firewall logs and look for IOcs of type IP, port\" }, { \"order\" : 2 , \"title\" : \"Search for IOCs on Web proxy logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in web proxy logs and look for IOcs of type IP, domain, hostname, user-agent\" } ], \"customFields\" : { \"hits\" : { \"integer\" : null , \"order\" : 1 } }, \"description\" : \"Check if IOCs shared by the community have been seen on the network\" , \"displayName\" : \"MISP\" } name should be unique. Otherwise an error 400 Bad Request is returned Response # Status codes # 201 : if template was created successfully 400 : in case of error in input 401 : Authentication error 403 : Authorization error ResponseBody Example # 201 { \"_id\" : \"~910319824\" , \"id\" : \"~910319824\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267739 , \"_type\" : \"caseTemplate\" , \"name\" : \"MISPEvent\" , \"displayName\" : \"MISP\" , \"titlePrefix\" : \"[MISP]\" , \"description\" : \"Check if IOCs shared by the community have been seen on the network\" , \"severity\" : 2 , \"tags\" : [ \"hunting\" ], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"tasks\" : [ { \"id\" : \"~122896536\" , \"_id\" : \"~122896536\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267741 , \"_type\" : \"case_task\" , \"title\" : \"Search for IOCs on Mail gateway logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in Mail gateway logs and look for IOcs of type IP, email addresses, hostnames, free text. \" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 }, { \"id\" : \"~81932320\" , \"_id\" : \"~81932320\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267743 , \"_type\" : \"case_task\" , \"title\" : \"Search for IOCs on Firewall logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in firewall logs and look for IOcs of type IP, port\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 1 }, { \"id\" : \"~81928376\" , \"_id\" : \"~81928376\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267750 , \"_type\" : \"case_task\" , \"title\" : \"Search for IOCs on Web proxy logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in web proxy logs and look for IOcs of type IP, domain, hostname, user-agent\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 2 } ], \"status\" : \"Ok\" , \"customFields\" : { \"hits\" : { \"integer\" : null , \"order\" : 1 , \"_id\" : \"~122900632\" } }, \"metrics\" : {} } 400 { \"type\" : \"CreateError\" , \"message\" : \"The case template \\\"MISPEvent\\\" already exists\" } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"Create"},{"location":"thehive/api/case-template/create/#create","text":"Create a Case Templates .","title":"Create"},{"location":"thehive/api/case-template/create/#query","text":"POST /api/case/template","title":"Query"},{"location":"thehive/api/case-template/create/#request-body-example","text":"{ \"name\" : \"MISPEvent\" , \"titlePrefix\" : \"\" , \"severity\" : 2 , \"tlp\" : 2 , \"pap\" : 2 , \"tags\" : [ \"hunting\" ], \"tasks\" : [ { \"order\" : 0 , \"title\" : \"Search for IOCs on Mail gateway logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in Mail gateway logs and look for IOcs of type IP, email addresses, hostnames, free text. \" }, { \"order\" : 1 , \"title\" : \"Search for IOCs on Firewall logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in firewall logs and look for IOcs of type IP, port\" }, { \"order\" : 2 , \"title\" : \"Search for IOCs on Web proxy logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in web proxy logs and look for IOcs of type IP, domain, hostname, user-agent\" } ], \"customFields\" : { \"hits\" : { \"integer\" : null , \"order\" : 1 } }, \"description\" : \"Check if IOCs shared by the community have been seen on the network\" , \"displayName\" : \"MISP\" } name should be unique. Otherwise an error 400 Bad Request is returned","title":"Request Body Example"},{"location":"thehive/api/case-template/create/#response","text":"","title":"Response"},{"location":"thehive/api/case-template/create/#status-codes","text":"201 : if template was created successfully 400 : in case of error in input 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/case-template/create/#responsebody-example","text":"201 { \"_id\" : \"~910319824\" , \"id\" : \"~910319824\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267739 , \"_type\" : \"caseTemplate\" , \"name\" : \"MISPEvent\" , \"displayName\" : \"MISP\" , \"titlePrefix\" : \"[MISP]\" , \"description\" : \"Check if IOCs shared by the community have been seen on the network\" , \"severity\" : 2 , \"tags\" : [ \"hunting\" ], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"tasks\" : [ { \"id\" : \"~122896536\" , \"_id\" : \"~122896536\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267741 , \"_type\" : \"case_task\" , \"title\" : \"Search for IOCs on Mail gateway logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in Mail gateway logs and look for IOcs of type IP, email addresses, hostnames, free text. \" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 }, { \"id\" : \"~81932320\" , \"_id\" : \"~81932320\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267743 , \"_type\" : \"case_task\" , \"title\" : \"Search for IOCs on Firewall logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in firewall logs and look for IOcs of type IP, port\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 1 }, { \"id\" : \"~81928376\" , \"_id\" : \"~81928376\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267750 , \"_type\" : \"case_task\" , \"title\" : \"Search for IOCs on Web proxy logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in web proxy logs and look for IOcs of type IP, domain, hostname, user-agent\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 2 } ], \"status\" : \"Ok\" , \"customFields\" : { \"hits\" : { \"integer\" : null , \"order\" : 1 , \"_id\" : \"~122900632\" } }, \"metrics\" : {} } 400 { \"type\" : \"CreateError\" , \"message\" : \"The case template \\\"MISPEvent\\\" already exists\" } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"ResponseBody Example"},{"location":"thehive/api/case-template/delete/","text":"Delete # Delete a Case Template by its id. Query # DELETE /api/case/template/{id} With: id : Case template identifier Response # Status codes # 200 : if Case Template is deleted successfully 401 : Authentication error 403 : Authorization error 404 : Case template does not exists (or was already deleted)","title":"Delete"},{"location":"thehive/api/case-template/delete/#delete","text":"Delete a Case Template by its id.","title":"Delete"},{"location":"thehive/api/case-template/delete/#query","text":"DELETE /api/case/template/{id} With: id : Case template identifier","title":"Query"},{"location":"thehive/api/case-template/delete/#response","text":"","title":"Response"},{"location":"thehive/api/case-template/delete/#status-codes","text":"200 : if Case Template is deleted successfully 401 : Authentication error 403 : Authorization error 404 : Case template does not exists (or was already deleted)","title":"Status codes"},{"location":"thehive/api/case-template/list/","text":"Get / List # List Case Templates of a given organisation. Query # POST /api/v1/query Request Body Example # { \"query\" : [ { \"_name\" : \"getOrganisation\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"caseTemplates\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"displayName\" : \"asc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] } With: id : Organisation identifier of Name Response # Status codes # 200 : if query is run successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 200 [ ... { \"_id\" : \"~910319824\" , \"_type\" : \"CaseTemplate\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_updatedBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620297081745 , \"_updatedAt\" : 1620389292177 , \"name\" : \"Phishing\" , \"displayName\" : \"Phishing\" , \"titlePrefix\" : \"Phishing -\" , \"description\" : \"Phishing attempt has succeed.\" , \"severity\" : 2 , \"tags\" : [ \"category:Phishing\" ], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"customFields\" : [], \"tasks\" : [ { \"_id\" : \"~677056528\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292172 , \"title\" : \"Initial alert\" , \"group\" : \"default\" , \"description\" : \"-What happened?\\n-When does it happened?\\n-How did it happened?\\n-How did we detected the anomaly/alert/incident?\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 , \"extraData\" : {} }, { \"_id\" : \"~677060624\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292173 , \"title\" : \"Remediation\" , \"group\" : \"default\" , \"description\" : \"Explain here all the actions performed to contain and remediate the threat.\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 3 , \"extraData\" : {} }, { \"_id\" : \"~677064720\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292174 , \"title\" : \"Lessons learnt\" , \"group\" : \"default\" , \"description\" : \"Write here the lessons learnt for the case.\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 4 , \"extraData\" : {} }, { \"_id\" : \"~706662512\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292171 , \"title\" : \"Notification / Communication\" , \"group\" : \"default\" , \"description\" : \"Write here all the communications related to this case\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 2 , \"extraData\" : {} }, { \"_id\" : \"~789033176\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292174 , \"title\" : \"Analysis\" , \"group\" : \"default\" , \"description\" : \"-Technical analysis of the incident\\n-Current impact\\n-Potential damages due to the incident\\n-...\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 1 , \"extraData\" : {} } ] } ... ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Get / List"},{"location":"thehive/api/case-template/list/#get-list","text":"List Case Templates of a given organisation.","title":"Get / List"},{"location":"thehive/api/case-template/list/#query","text":"POST /api/v1/query","title":"Query"},{"location":"thehive/api/case-template/list/#request-body-example","text":"{ \"query\" : [ { \"_name\" : \"getOrganisation\" , \"idOrName\" : \"{id}\" }, { \"_name\" : \"caseTemplates\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"displayName\" : \"asc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] } With: id : Organisation identifier of Name","title":"Request Body Example"},{"location":"thehive/api/case-template/list/#response","text":"","title":"Response"},{"location":"thehive/api/case-template/list/#status-codes","text":"200 : if query is run successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/case-template/list/#responsebody-example","text":"200 [ ... { \"_id\" : \"~910319824\" , \"_type\" : \"CaseTemplate\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_updatedBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620297081745 , \"_updatedAt\" : 1620389292177 , \"name\" : \"Phishing\" , \"displayName\" : \"Phishing\" , \"titlePrefix\" : \"Phishing -\" , \"description\" : \"Phishing attempt has succeed.\" , \"severity\" : 2 , \"tags\" : [ \"category:Phishing\" ], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"customFields\" : [], \"tasks\" : [ { \"_id\" : \"~677056528\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292172 , \"title\" : \"Initial alert\" , \"group\" : \"default\" , \"description\" : \"-What happened?\\n-When does it happened?\\n-How did it happened?\\n-How did we detected the anomaly/alert/incident?\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 , \"extraData\" : {} }, { \"_id\" : \"~677060624\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292173 , \"title\" : \"Remediation\" , \"group\" : \"default\" , \"description\" : \"Explain here all the actions performed to contain and remediate the threat.\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 3 , \"extraData\" : {} }, { \"_id\" : \"~677064720\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292174 , \"title\" : \"Lessons learnt\" , \"group\" : \"default\" , \"description\" : \"Write here the lessons learnt for the case.\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 4 , \"extraData\" : {} }, { \"_id\" : \"~706662512\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292171 , \"title\" : \"Notification / Communication\" , \"group\" : \"default\" , \"description\" : \"Write here all the communications related to this case\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 2 , \"extraData\" : {} }, { \"_id\" : \"~789033176\" , \"_type\" : \"Task\" , \"_createdBy\" : \"florian@strangebee.com\" , \"_createdAt\" : 1620389292174 , \"title\" : \"Analysis\" , \"group\" : \"default\" , \"description\" : \"-Technical analysis of the incident\\n-Current impact\\n-Potential damages due to the incident\\n-...\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 1 , \"extraData\" : {} } ] } ... ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"ResponseBody Example"},{"location":"thehive/api/case-template/update/","text":"Update # Update a Case Template by its id. Query # PATCH /api/case/template/{id} Request Body Example # Example { \"displayName\" : \"New Display name\" , \"tlp\" : 4 , \"tasks\" : [ { \"order\" : 0 , \"title\" : \"Search for IOCs on Mail gateway logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in Mail gateway logs and look for IOcs of type IP, email addresses, hostnames, free text. \" } ] } Fields that can be updated: name displayName titlePrefix description severity tags flag tlp pap summary customFields tasks ResponseBody Example # Example { \"_id\" : \"~910319824\" , \"id\" : \"~910319824\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267739 , \"_type\" : \"caseTemplate\" , \"name\" : \"MISPEvent\" , \"displayName\" : \"New Display name\" , \"titlePrefix\" : \"[MISP]\" , \"description\" : \"Check if IOCs shared by the community have been seen on the network\" , \"severity\" : 2 , \"tags\" : [ \"hunting\" ], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"tasks\" : [ { \"id\" : \"~122896536\" , \"_id\" : \"~122896536\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267741 , \"_type\" : \"case_task\" , \"title\" : \"Search for IOCs on Mail gateway logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in Mail gateway logs and look for IOcs of type IP, email addresses, hostnames, free text. \" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 } ], \"status\" : \"Ok\" , \"customFields\" : { \"hits\" : { \"integer\" : null , \"order\" : 1 , \"_id\" : \"~122900632\" } }, \"metrics\" : {} }","title":"Update"},{"location":"thehive/api/case-template/update/#update","text":"Update a Case Template by its id.","title":"Update"},{"location":"thehive/api/case-template/update/#query","text":"PATCH /api/case/template/{id}","title":"Query"},{"location":"thehive/api/case-template/update/#request-body-example","text":"Example { \"displayName\" : \"New Display name\" , \"tlp\" : 4 , \"tasks\" : [ { \"order\" : 0 , \"title\" : \"Search for IOCs on Mail gateway logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in Mail gateway logs and look for IOcs of type IP, email addresses, hostnames, free text. \" } ] } Fields that can be updated: name displayName titlePrefix description severity tags flag tlp pap summary customFields tasks","title":"Request Body Example"},{"location":"thehive/api/case-template/update/#responsebody-example","text":"Example { \"_id\" : \"~910319824\" , \"id\" : \"~910319824\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267739 , \"_type\" : \"caseTemplate\" , \"name\" : \"MISPEvent\" , \"displayName\" : \"New Display name\" , \"titlePrefix\" : \"[MISP]\" , \"description\" : \"Check if IOCs shared by the community have been seen on the network\" , \"severity\" : 2 , \"tags\" : [ \"hunting\" ], \"flag\" : false , \"tlp\" : 2 , \"pap\" : 2 , \"tasks\" : [ { \"id\" : \"~122896536\" , \"_id\" : \"~122896536\" , \"createdBy\" : \"florian@strangebee.com\" , \"createdAt\" : 1630675267741 , \"_type\" : \"case_task\" , \"title\" : \"Search for IOCs on Mail gateway logs\" , \"group\" : \"default\" , \"description\" : \"Run queries in Mail gateway logs and look for IOcs of type IP, email addresses, hostnames, free text. \" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 } ], \"status\" : \"Ok\" , \"customFields\" : { \"hits\" : { \"integer\" : null , \"order\" : 1 , \"_id\" : \"~122900632\" } }, \"metrics\" : {} }","title":"ResponseBody Example"},{"location":"thehive/api/custom-field/","text":"Custom Field APIs # List custom fields Create a custom field Update custom field Delete a custom field Get a custom field Get custom field useage","title":"Overview"},{"location":"thehive/api/custom-field/#custom-field-apis","text":"List custom fields Create a custom field Update custom field Delete a custom field Get a custom field Get custom field useage","title":"Custom Field APIs"},{"location":"thehive/api/custom-field/create/","text":"Create # Create a Custom Field (requires manageCustomField permission). Query # POST /api/customField Request Body Example # { \"name\" : \"BusinesUnit\" , \"reference\" : \"businessunit\" , \"description\" : \"Targeted business unit\" , \"type\" : \"string\" , \"mandatory\" : false , \"options\" : [ \"VIP\" , \"HR\" , \"Security\" , \"Sys Administrators\" , \"Developers\" , \"Sales\" , \"Marketing\" , \"Procurement\" , \"Legal\" ] } The following fields are required: name : (String) reference : (String) description : (String) type : [string|integer|boolean|date|float] Response # Status codes # 201 : if Custom Fields is created successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 201 { \"id\" : \"~32912\" , \"name\" : \"Business Unit\" , \"reference\" : \"businessUnit\" , \"description\" : \"Targetted business unit\" , \"type\" : \"string\" , \"options\" : [ \"Sales\" , \"Marketing\" , \"VIP\" , \"Security\" , \"Sys admins\" , \"HR\" , \"Procurement\" , \"Legal\" ], \"mandatory\" : false } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Create"},{"location":"thehive/api/custom-field/create/#create","text":"Create a Custom Field (requires manageCustomField permission).","title":"Create"},{"location":"thehive/api/custom-field/create/#query","text":"POST /api/customField","title":"Query"},{"location":"thehive/api/custom-field/create/#request-body-example","text":"{ \"name\" : \"BusinesUnit\" , \"reference\" : \"businessunit\" , \"description\" : \"Targeted business unit\" , \"type\" : \"string\" , \"mandatory\" : false , \"options\" : [ \"VIP\" , \"HR\" , \"Security\" , \"Sys Administrators\" , \"Developers\" , \"Sales\" , \"Marketing\" , \"Procurement\" , \"Legal\" ] } The following fields are required: name : (String) reference : (String) description : (String) type : [string|integer|boolean|date|float]","title":"Request Body Example"},{"location":"thehive/api/custom-field/create/#response","text":"","title":"Response"},{"location":"thehive/api/custom-field/create/#status-codes","text":"201 : if Custom Fields is created successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/custom-field/create/#responsebody-example","text":"201 { \"id\" : \"~32912\" , \"name\" : \"Business Unit\" , \"reference\" : \"businessUnit\" , \"description\" : \"Targetted business unit\" , \"type\" : \"string\" , \"options\" : [ \"Sales\" , \"Marketing\" , \"VIP\" , \"Security\" , \"Sys admins\" , \"HR\" , \"Procurement\" , \"Legal\" ], \"mandatory\" : false } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"ResponseBody Example"},{"location":"thehive/api/custom-field/delete/","text":"Delete # Delete a Custom Field (requires manageCustomField permission). Query # DELETE /api/customField/{id} with: id : id or name of the Custom Field. Response # Status codes # 204 : if Custom Fields is successfully deleted 401 : Authentication error 403 : Authorization error","title":"Delete"},{"location":"thehive/api/custom-field/delete/#delete","text":"Delete a Custom Field (requires manageCustomField permission).","title":"Delete"},{"location":"thehive/api/custom-field/delete/#query","text":"DELETE /api/customField/{id} with: id : id or name of the Custom Field.","title":"Query"},{"location":"thehive/api/custom-field/delete/#response","text":"","title":"Response"},{"location":"thehive/api/custom-field/delete/#status-codes","text":"204 : if Custom Fields is successfully deleted 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/custom-field/get/","text":"Get # Get Custom Field by id; Query # GET /api/customField/{id} with: id : id or name of the custom field. Response # Status codes # 200 : if query is run successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 200 { \"id\" : \"~28672\" , \"name\" : \"Number of Accounts\" , \"reference\" : \"Number of Accounts\" , \"description\" : \"Number of accounts leaked\" , \"type\" : \"integer\" , \"options\" : [], \"mandatory\" : true } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Get"},{"location":"thehive/api/custom-field/get/#get","text":"Get Custom Field by id;","title":"Get"},{"location":"thehive/api/custom-field/get/#query","text":"GET /api/customField/{id} with: id : id or name of the custom field.","title":"Query"},{"location":"thehive/api/custom-field/get/#response","text":"","title":"Response"},{"location":"thehive/api/custom-field/get/#status-codes","text":"200 : if query is run successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/custom-field/get/#responsebody-example","text":"200 { \"id\" : \"~28672\" , \"name\" : \"Number of Accounts\" , \"reference\" : \"Number of Accounts\" , \"description\" : \"Number of accounts leaked\" , \"type\" : \"integer\" , \"options\" : [], \"mandatory\" : true } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"ResponseBody Example"},{"location":"thehive/api/custom-field/getUse/","text":"Use count # Get Custom Field use count by id. Query # GET /api/customField/{id}/use with: id : id or name of the custom field. Response # Status codes # 200 : if query is run successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 200 { \"case\" : 12 , \"alert\" : 1 , \"case_artifact\" : 9 , \"total\" : 22 } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"Use count"},{"location":"thehive/api/custom-field/getUse/#use-count","text":"Get Custom Field use count by id.","title":"Use count"},{"location":"thehive/api/custom-field/getUse/#query","text":"GET /api/customField/{id}/use with: id : id or name of the custom field.","title":"Query"},{"location":"thehive/api/custom-field/getUse/#response","text":"","title":"Response"},{"location":"thehive/api/custom-field/getUse/#status-codes","text":"200 : if query is run successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/custom-field/getUse/#responsebody-example","text":"200 { \"case\" : 12 , \"alert\" : 1 , \"case_artifact\" : 9 , \"total\" : 22 } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"ResponseBody Example"},{"location":"thehive/api/custom-field/list/","text":"List # List Custom Fields . Query # GET /api/customField Response # Status codes # 200 : if query is run successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 200 [ { \"id\" : \"~28672\" , \"name\" : \"Number of Accounts\" , \"reference\" : \"Number of Accounts\" , \"description\" : \"Number of accounts leaked\" , \"type\" : \"integer\" , \"options\" : [], \"mandatory\" : true }, { \"id\" : \"~53440\" , \"name\" : \"Nb of emails delivered\" , \"reference\" : \"Nb of emails delivered\" , \"description\" : \"Nb of emails delivered\" , \"type\" : \"integer\" , \"options\" : [], \"mandatory\" : true } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"List"},{"location":"thehive/api/custom-field/list/#list","text":"List Custom Fields .","title":"List"},{"location":"thehive/api/custom-field/list/#query","text":"GET /api/customField","title":"Query"},{"location":"thehive/api/custom-field/list/#response","text":"","title":"Response"},{"location":"thehive/api/custom-field/list/#status-codes","text":"200 : if query is run successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/custom-field/list/#responsebody-example","text":"200 [ { \"id\" : \"~28672\" , \"name\" : \"Number of Accounts\" , \"reference\" : \"Number of Accounts\" , \"description\" : \"Number of accounts leaked\" , \"type\" : \"integer\" , \"options\" : [], \"mandatory\" : true }, { \"id\" : \"~53440\" , \"name\" : \"Nb of emails delivered\" , \"reference\" : \"Nb of emails delivered\" , \"description\" : \"Nb of emails delivered\" , \"type\" : \"integer\" , \"options\" : [], \"mandatory\" : true } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create custom field, you haven't the permission manageCustomField\" }","title":"ResponseBody Example"},{"location":"thehive/api/custom-field/update/","text":"Update # Update a Custom Field (requires manageCustomField permission). Query # PATCH /api/customField/{id} with: id : id or name of the custom field. Request Body Example # { \"name\" : \"Business Unit\" , \"reference\" : \"businessUnit\" , \"description\" : \"Targetted business unit\" , \"type\" : \"string\" , \"options\" : [ \"Sales\" , \"Marketing\" , \"VIP\" , \"Security\" , \"Sys admins\" , \"HR\" , \"Procurement\" , \"Legal\" ], \"mandatory\" : false } No fields are required. Response # Status codes # 200 : if Custom Fields is updated successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 201 { \"id\" : \"~32912\" , \"name\" : \"Business Unit\" , \"reference\" : \"businessUnit\" , \"description\" : \"Targetted business unit\" , \"type\" : \"string\" , \"options\" : [ \"HR\" , \"Legal\" , \"Marketing\" , \"Procurement\" , \"Sales\" , \"Security\" , \"Sys admins\" , \"VIP\" ], \"mandatory\" : false } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to update custom field, you haven't the permission manageCustomField\" }","title":"Update"},{"location":"thehive/api/custom-field/update/#update","text":"Update a Custom Field (requires manageCustomField permission).","title":"Update"},{"location":"thehive/api/custom-field/update/#query","text":"PATCH /api/customField/{id} with: id : id or name of the custom field.","title":"Query"},{"location":"thehive/api/custom-field/update/#request-body-example","text":"{ \"name\" : \"Business Unit\" , \"reference\" : \"businessUnit\" , \"description\" : \"Targetted business unit\" , \"type\" : \"string\" , \"options\" : [ \"Sales\" , \"Marketing\" , \"VIP\" , \"Security\" , \"Sys admins\" , \"HR\" , \"Procurement\" , \"Legal\" ], \"mandatory\" : false } No fields are required.","title":"Request Body Example"},{"location":"thehive/api/custom-field/update/#response","text":"","title":"Response"},{"location":"thehive/api/custom-field/update/#status-codes","text":"200 : if Custom Fields is updated successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/custom-field/update/#responsebody-example","text":"201 { \"id\" : \"~32912\" , \"name\" : \"Business Unit\" , \"reference\" : \"businessUnit\" , \"description\" : \"Targetted business unit\" , \"type\" : \"string\" , \"options\" : [ \"HR\" , \"Legal\" , \"Marketing\" , \"Procurement\" , \"Sales\" , \"Security\" , \"Sys admins\" , \"VIP\" ], \"mandatory\" : false } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to update custom field, you haven't the permission manageCustomField\" }","title":"ResponseBody Example"},{"location":"thehive/api/dashboard/create/","text":"","title":"Create"},{"location":"thehive/api/dashboard/update/","text":"Update # Query # Request Body Example # ResponseBody Example #","title":"Update"},{"location":"thehive/api/dashboard/update/#update","text":"","title":"Update"},{"location":"thehive/api/dashboard/update/#query","text":"","title":"Query"},{"location":"thehive/api/dashboard/update/#request-body-example","text":"","title":"Request Body Example"},{"location":"thehive/api/dashboard/update/#responsebody-example","text":"","title":"ResponseBody Example"},{"location":"thehive/api/observable/","text":"Observable APIs # List observables Create observable Update observable Delete observable Run analyzer in observable Run responder in observable","title":"Overview"},{"location":"thehive/api/observable/#observable-apis","text":"List observables Create observable Update observable Delete observable Run analyzer in observable Run responder in observable","title":"Observable APIs"},{"location":"thehive/api/observable/analyzer/","text":"Analyzer # You need to connect TheHive to a cortex server in order to enable analyzers. Attention Analyzer can only be run on an observable of a case. Run an analyzer on an observable # POST /api/connector/cortex/job Request example # { \"cortexId\" : \"Stable\" , \"artifactId\" : \"~816984288\" , \"analyzerId\" : \"Abuse_Finder_3_0\" } The following fields are required: - cortexId : name of the cortex server from the configuration - artifactId : id of the observable to analyze - analyzerId : id of the cortex analyzer to use Responseexample # { \"_type\" : \"case_artifact_job\" , \"analyzerId\" : \"bface6faa22029dcf81d5d817f27eb98\" , \"analyzerName\" : \"Abuse_Finder_3_0\" , \"analyzerDefinition\" : \"Abuse_Finder_3_0\" , \"status\" : \"Waiting\" , \"startDate\" : 1630660394582 , \"endDate\" : 1630660394582 , \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"BP3uqnsB8Pn57ils_kFX\" , \"id\" : \"~1433604184\" } Get analyzer report # GET /api/connector/job/{jobId} jobId should be the id returned from the creation request Responseexample # Example { \"_type\" : \"case_artifact_job\" , \"analyzerId\" : \"bface6faa22029dcf81d5d817f27eb98\" , \"analyzerName\" : \"Abuse_Finder_3_0\" , \"analyzerDefinition\" : \"Abuse_Finder_3_0\" , \"status\" : \"Success\" , \"startDate\" : 1630660394582 , \"endDate\" : 1630660427845 , \"report\" : { \"success\" : true , \"full\" : { \"abuse_finder\" : { \"value\" : \"1.2.3.4\" , \"names\" : [ \"APNIC Debogon Project\" ], \"abuse\" : [ \"helpdesk@apnic.net\" ], \"raw\" : \"% [whois.apnic.net]\\n% Whois data copyright terms http://www.apnic.net/db/dbcopyright.html\\n\\n% Inf...\" } }, \"artifacts\" : [] }, \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"BP3uqnsB8Pn57ils_kFX\" , \"id\" : \"~1433604184\" } status can be one of: Waiting Success InProgress Failure Deleted List reports for an observable # POST /api/v1/query Query body example # Replace the value of idOrName by the id of your observable Example { \"query\" : [ { \"_name\" : \"getObservable\" , \"idOrName\" : \"~816984288\" }, { \"_name\" : \"jobs\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 200 } ] } Responseexample # Example [ { \"_type\" : \"case_artifact_job\" , \"analyzerId\" : \"bface6faa22029dcf81d5d817f27eb98\" , \"analyzerName\" : \"Abuse_Finder_3_0\" , \"analyzerDefinition\" : \"Abuse_Finder_3_0\" , \"status\" : \"Waiting\" , \"startDate\" : 1630660394582 , \"endDate\" : 1630660394582 , \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"BP3uqnsB8Pn57ils_kFX\" , \"id\" : \"~1433604184\" } ]","title":"Analyzer"},{"location":"thehive/api/observable/analyzer/#analyzer","text":"You need to connect TheHive to a cortex server in order to enable analyzers. Attention Analyzer can only be run on an observable of a case.","title":"Analyzer"},{"location":"thehive/api/observable/analyzer/#run-an-analyzer-on-an-observable","text":"POST /api/connector/cortex/job","title":"Run an analyzer on an observable"},{"location":"thehive/api/observable/analyzer/#request-example","text":"{ \"cortexId\" : \"Stable\" , \"artifactId\" : \"~816984288\" , \"analyzerId\" : \"Abuse_Finder_3_0\" } The following fields are required: - cortexId : name of the cortex server from the configuration - artifactId : id of the observable to analyze - analyzerId : id of the cortex analyzer to use","title":"Request example"},{"location":"thehive/api/observable/analyzer/#responseexample","text":"{ \"_type\" : \"case_artifact_job\" , \"analyzerId\" : \"bface6faa22029dcf81d5d817f27eb98\" , \"analyzerName\" : \"Abuse_Finder_3_0\" , \"analyzerDefinition\" : \"Abuse_Finder_3_0\" , \"status\" : \"Waiting\" , \"startDate\" : 1630660394582 , \"endDate\" : 1630660394582 , \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"BP3uqnsB8Pn57ils_kFX\" , \"id\" : \"~1433604184\" }","title":"Responseexample"},{"location":"thehive/api/observable/analyzer/#get-analyzer-report","text":"GET /api/connector/job/{jobId} jobId should be the id returned from the creation request","title":"Get analyzer report"},{"location":"thehive/api/observable/analyzer/#responseexample_1","text":"Example { \"_type\" : \"case_artifact_job\" , \"analyzerId\" : \"bface6faa22029dcf81d5d817f27eb98\" , \"analyzerName\" : \"Abuse_Finder_3_0\" , \"analyzerDefinition\" : \"Abuse_Finder_3_0\" , \"status\" : \"Success\" , \"startDate\" : 1630660394582 , \"endDate\" : 1630660427845 , \"report\" : { \"success\" : true , \"full\" : { \"abuse_finder\" : { \"value\" : \"1.2.3.4\" , \"names\" : [ \"APNIC Debogon Project\" ], \"abuse\" : [ \"helpdesk@apnic.net\" ], \"raw\" : \"% [whois.apnic.net]\\n% Whois data copyright terms http://www.apnic.net/db/dbcopyright.html\\n\\n% Inf...\" } }, \"artifacts\" : [] }, \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"BP3uqnsB8Pn57ils_kFX\" , \"id\" : \"~1433604184\" } status can be one of: Waiting Success InProgress Failure Deleted","title":"Responseexample"},{"location":"thehive/api/observable/analyzer/#list-reports-for-an-observable","text":"POST /api/v1/query","title":"List reports for an observable"},{"location":"thehive/api/observable/analyzer/#query-body-example","text":"Replace the value of idOrName by the id of your observable Example { \"query\" : [ { \"_name\" : \"getObservable\" , \"idOrName\" : \"~816984288\" }, { \"_name\" : \"jobs\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 200 } ] }","title":"Query body example"},{"location":"thehive/api/observable/analyzer/#responseexample_2","text":"Example [ { \"_type\" : \"case_artifact_job\" , \"analyzerId\" : \"bface6faa22029dcf81d5d817f27eb98\" , \"analyzerName\" : \"Abuse_Finder_3_0\" , \"analyzerDefinition\" : \"Abuse_Finder_3_0\" , \"status\" : \"Waiting\" , \"startDate\" : 1630660394582 , \"endDate\" : 1630660394582 , \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"BP3uqnsB8Pn57ils_kFX\" , \"id\" : \"~1433604184\" } ]","title":"Responseexample"},{"location":"thehive/api/observable/create/","text":"Create # Creates an observable which can be linked to a case or an alert. Note (The name artifcat comes from TheHive v3) Query # Create an observable for a case POST /api/v0/case/{caseId}/artifact Create an observable for an alert POST /api/v0/alert/{alertId}/artifact Request Example # Observable with attachment If you want to upload an observable with a dataType of kind attachment, you need to use a multipart request curl -XPOST http://THEHIVE/api/v0/case/{caseId}/artifact -F attachment=@myFile -F _json=' { \"dataType\": \"file\", \"tlp\": 2, \"ioc\": true, \"sighted\": false, \"tags\": [], \"message\": \"foo\", \"data\": [], \"isZip\": false, \"zipPassword\": \"\" } ' Observables without atttachment To add an observable with no attachment, you can post a json body { \"dataType\" : \"hostname\" , \"tlp\" : 2 , \"ioc\" : true , \"sighted\" : true , \"tags\" : [], \"data\" : [ \"server.local\" ] } The following fields are required: dataType : (enum String, should be one registered observable type) One of data (Array of String) or attachment (File) Other optional fields: message : (String) description of the observable in the context of the case startDate : (Date) date of the observable creation default=now tlp : (Int) TLP ( 0 : white ; 1 : green ; 2 : amber ; 3 : red ) default=2 tags : (Array of string) a list of tags default=[] ioc : (Boolean) indicates if the observable is an IOC default=false sighted : (Boolean) indicates if the observable was sighted default=false ignoreSimilarity : (Boolean) indicates if the observable should be used or not to calculate the similarity stats default=false ResponseBody Example # Observables with atttachment [ { \"_id\" : \"~4104\" , \"id\" : \"~4104\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630508511351 , \"_type\" : \"case_artifact\" , \"dataType\" : \"file\" , \"startDate\" : 1630508511351 , \"attachment\" : { \"name\" : \"server.log\" , \"hashes\" : [ \"ccbda6ed6aac6cde57ebac1f011bdf1f58bf61c40c759dc4f7fccb729de10147\" , \"a09531845b3b26d5707cdf50a8bb11aa507dd88c\" , \"1f08c024363568d6eb4e18ee97618acc\" ], \"size\" : 37165 , \"contentType\" : \"application/octet-stream\" , \"id\" : \"ccbda6ed6aac6cde57ebac1f011bdf1f58bf61c40c759dc4f7fccb729de10147\" }, \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : false , \"message\" : \"my message\" , \"reports\" : {}, \"stats\" : {} } ] Observables without atttachment [ { \"_id\" : \"~4104\" , \"id\" : \"~4104\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630508511351 , \"_type\" : \"case_artifact\" , \"ip\" : \"file\" , \"data\" : \"1.2.3.4\" , \"startDate\" : 1630508511351 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : false , \"message\" : \"my message\" , \"reports\" : {}, \"stats\" : {} } ]","title":"Create"},{"location":"thehive/api/observable/create/#create","text":"Creates an observable which can be linked to a case or an alert. Note (The name artifcat comes from TheHive v3)","title":"Create"},{"location":"thehive/api/observable/create/#query","text":"Create an observable for a case POST /api/v0/case/{caseId}/artifact Create an observable for an alert POST /api/v0/alert/{alertId}/artifact","title":"Query"},{"location":"thehive/api/observable/create/#request-example","text":"Observable with attachment If you want to upload an observable with a dataType of kind attachment, you need to use a multipart request curl -XPOST http://THEHIVE/api/v0/case/{caseId}/artifact -F attachment=@myFile -F _json=' { \"dataType\": \"file\", \"tlp\": 2, \"ioc\": true, \"sighted\": false, \"tags\": [], \"message\": \"foo\", \"data\": [], \"isZip\": false, \"zipPassword\": \"\" } ' Observables without atttachment To add an observable with no attachment, you can post a json body { \"dataType\" : \"hostname\" , \"tlp\" : 2 , \"ioc\" : true , \"sighted\" : true , \"tags\" : [], \"data\" : [ \"server.local\" ] } The following fields are required: dataType : (enum String, should be one registered observable type) One of data (Array of String) or attachment (File) Other optional fields: message : (String) description of the observable in the context of the case startDate : (Date) date of the observable creation default=now tlp : (Int) TLP ( 0 : white ; 1 : green ; 2 : amber ; 3 : red ) default=2 tags : (Array of string) a list of tags default=[] ioc : (Boolean) indicates if the observable is an IOC default=false sighted : (Boolean) indicates if the observable was sighted default=false ignoreSimilarity : (Boolean) indicates if the observable should be used or not to calculate the similarity stats default=false","title":"Request Example"},{"location":"thehive/api/observable/create/#responsebody-example","text":"Observables with atttachment [ { \"_id\" : \"~4104\" , \"id\" : \"~4104\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630508511351 , \"_type\" : \"case_artifact\" , \"dataType\" : \"file\" , \"startDate\" : 1630508511351 , \"attachment\" : { \"name\" : \"server.log\" , \"hashes\" : [ \"ccbda6ed6aac6cde57ebac1f011bdf1f58bf61c40c759dc4f7fccb729de10147\" , \"a09531845b3b26d5707cdf50a8bb11aa507dd88c\" , \"1f08c024363568d6eb4e18ee97618acc\" ], \"size\" : 37165 , \"contentType\" : \"application/octet-stream\" , \"id\" : \"ccbda6ed6aac6cde57ebac1f011bdf1f58bf61c40c759dc4f7fccb729de10147\" }, \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : false , \"message\" : \"my message\" , \"reports\" : {}, \"stats\" : {} } ] Observables without atttachment [ { \"_id\" : \"~4104\" , \"id\" : \"~4104\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630508511351 , \"_type\" : \"case_artifact\" , \"ip\" : \"file\" , \"data\" : \"1.2.3.4\" , \"startDate\" : 1630508511351 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : false , \"message\" : \"my message\" , \"reports\" : {}, \"stats\" : {} } ]","title":"ResponseBody Example"},{"location":"thehive/api/observable/delete/","text":"Delete # Delete a case or alert Observable by its id Query # DELETE /api/v0/case/artifact/{observableId} DELETE /api/v0/alert/artifact/{observableId} Response # 204 No Content","title":"Delete"},{"location":"thehive/api/observable/delete/#delete","text":"Delete a case or alert Observable by its id","title":"Delete"},{"location":"thehive/api/observable/delete/#query","text":"DELETE /api/v0/case/artifact/{observableId} DELETE /api/v0/alert/artifact/{observableId}","title":"Query"},{"location":"thehive/api/observable/delete/#response","text":"204 No Content","title":"Response"},{"location":"thehive/api/observable/list/","text":"List / Search # Query # POST /api/v1/query Request Body Example # List last 30 observables for a case: { \"query\" : [ { \"_name\" : \"getCase\" , \"idOrName\" : \"{caseId}\" }, { \"_name\" : \"observables\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 30 } ] } ResponseExample # [ { \"_id\" : \"~122884120\" , \"_type\" : \"Observable\" , \"_createdBy\" : \"foo@local.io\" , \"_updatedBy\" : \"foo@local.io\" , \"_createdAt\" : 1630509659446 , \"_updatedAt\" : 1630511666911 , \"dataType\" : \"hostname\" , \"data\" : \"server.local\" , \"startDate\" : 1630509659446 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : false , \"reports\" : {}, \"message\" : \"myMessage\" , \"extraData\" : {} }, { \"_id\" : \"~4104\" , \"_type\" : \"Observable\" , \"_createdBy\" : \"foo@local.io\" , \"_createdAt\" : 1630508511351 , \"dataType\" : \"file\" , \"startDate\" : 1630508511351 , \"attachment\" : { \"_id\" : \"~40964280\" , \"_type\" : \"Attachment\" , \"_createdBy\" : \"foo@local.io\" , \"_createdAt\" : 1630508511313 , \"name\" : \"server.log\" , \"hashes\" : [ \"ccbda6ed6aac6cde57ebac1f011bdf1f58bf61c40c759dc4f7fccb729de10147\" , \"a09531845b3b26d5707cdf50a8bb11aa507dd88c\" , \"1f08c024363568d6eb4e18ee97618acc\" ], \"size\" : 37165 , \"contentType\" : \"application/octet-stream\" , \"id\" : \"ccbda6ed6aac6cde57ebac1f011bdf1f58bf61c40c759dc4f7fccb729de10147\" }, \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : false , \"reports\" : {}, \"message\" : \"foo\" , \"extraData\" : {} } ]","title":"List / Search"},{"location":"thehive/api/observable/list/#list-search","text":"","title":"List / Search"},{"location":"thehive/api/observable/list/#query","text":"POST /api/v1/query","title":"Query"},{"location":"thehive/api/observable/list/#request-body-example","text":"List last 30 observables for a case: { \"query\" : [ { \"_name\" : \"getCase\" , \"idOrName\" : \"{caseId}\" }, { \"_name\" : \"observables\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 30 } ] }","title":"Request Body Example"},{"location":"thehive/api/observable/list/#responseexample","text":"[ { \"_id\" : \"~122884120\" , \"_type\" : \"Observable\" , \"_createdBy\" : \"foo@local.io\" , \"_updatedBy\" : \"foo@local.io\" , \"_createdAt\" : 1630509659446 , \"_updatedAt\" : 1630511666911 , \"dataType\" : \"hostname\" , \"data\" : \"server.local\" , \"startDate\" : 1630509659446 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : false , \"reports\" : {}, \"message\" : \"myMessage\" , \"extraData\" : {} }, { \"_id\" : \"~4104\" , \"_type\" : \"Observable\" , \"_createdBy\" : \"foo@local.io\" , \"_createdAt\" : 1630508511351 , \"dataType\" : \"file\" , \"startDate\" : 1630508511351 , \"attachment\" : { \"_id\" : \"~40964280\" , \"_type\" : \"Attachment\" , \"_createdBy\" : \"foo@local.io\" , \"_createdAt\" : 1630508511313 , \"name\" : \"server.log\" , \"hashes\" : [ \"ccbda6ed6aac6cde57ebac1f011bdf1f58bf61c40c759dc4f7fccb729de10147\" , \"a09531845b3b26d5707cdf50a8bb11aa507dd88c\" , \"1f08c024363568d6eb4e18ee97618acc\" ], \"size\" : 37165 , \"contentType\" : \"application/octet-stream\" , \"id\" : \"ccbda6ed6aac6cde57ebac1f011bdf1f58bf61c40c759dc4f7fccb729de10147\" }, \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : false , \"reports\" : {}, \"message\" : \"foo\" , \"extraData\" : {} } ]","title":"ResponseExample"},{"location":"thehive/api/observable/responder/","text":"Responder # You need to connect TheHive to a cortex server in order to enable responders. Attention Responder can only be run on an observable of a case. Run a responder on an observable # POST /api/connector/cortex/action Request example # { \"cortexId\" : \"Stable\" , \"responderId\" : \"e4c500d589d14503883c02d02313cf57\" , \"objectType\" : \"case_artifact\" , \"objectId\" : \"~816984288\" } The following fields are required: cortexId : name of the cortex server from the configuration objectType : should be case_artifact here objectId : id of the observable to analyze responderId : id of the cortex responder to use Responseexample # { \"responderId\" : \"e4c500d589d14503883c02d02313cf57\" , \"responderName\" : \"ADD_TO_WEBPROXY_BL_1_0\" , \"responderDefinition\" : \"ADD_TO_WEBPROXY_BL_1_0\" , \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"Bv0cq3sB8Pn57ilsUkFM\" , \"objectType\" : \"Observable\" , \"objectId\" : \"~816984288\" , \"status\" : \"Waiting\" , \"startDate\" : 1630663366136 , \"operations\" : \"[]\" , \"report\" : \"{}\" } List responder actions # GET /api/connector/cortex/action/case_artifact/{observableId} Responseexample # Example [ { \"responderId\" : \"e4c500d589d14503883c02d02313cf57\" , \"responderName\" : \"ADD_TO_WEBPROXY_BL_1_0\" , \"responderDefinition\" : \"ADD_TO_WEBPROXY_BL_1_0\" , \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"Bv0cq3sB8Pn57ilsUkFM\" , \"objectType\" : \"Observable\" , \"objectId\" : \"~816984288\" , \"status\" : \"Failure\" , \"startDate\" : 1630663366136 , \"endDate\" : 1630663372393 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":false,\\\"artifacts\\\":[],\\\"operations\\\":[],...}\" } ] status can be one of: Waiting Success InProgress Failure Deleted report is a string that contains the output of the responder","title":"Responder"},{"location":"thehive/api/observable/responder/#responder","text":"You need to connect TheHive to a cortex server in order to enable responders. Attention Responder can only be run on an observable of a case.","title":"Responder"},{"location":"thehive/api/observable/responder/#run-a-responder-on-an-observable","text":"POST /api/connector/cortex/action","title":"Run a responder on an observable"},{"location":"thehive/api/observable/responder/#request-example","text":"{ \"cortexId\" : \"Stable\" , \"responderId\" : \"e4c500d589d14503883c02d02313cf57\" , \"objectType\" : \"case_artifact\" , \"objectId\" : \"~816984288\" } The following fields are required: cortexId : name of the cortex server from the configuration objectType : should be case_artifact here objectId : id of the observable to analyze responderId : id of the cortex responder to use","title":"Request example"},{"location":"thehive/api/observable/responder/#responseexample","text":"{ \"responderId\" : \"e4c500d589d14503883c02d02313cf57\" , \"responderName\" : \"ADD_TO_WEBPROXY_BL_1_0\" , \"responderDefinition\" : \"ADD_TO_WEBPROXY_BL_1_0\" , \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"Bv0cq3sB8Pn57ilsUkFM\" , \"objectType\" : \"Observable\" , \"objectId\" : \"~816984288\" , \"status\" : \"Waiting\" , \"startDate\" : 1630663366136 , \"operations\" : \"[]\" , \"report\" : \"{}\" }","title":"Responseexample"},{"location":"thehive/api/observable/responder/#list-responder-actions","text":"GET /api/connector/cortex/action/case_artifact/{observableId}","title":"List responder actions"},{"location":"thehive/api/observable/responder/#responseexample_1","text":"Example [ { \"responderId\" : \"e4c500d589d14503883c02d02313cf57\" , \"responderName\" : \"ADD_TO_WEBPROXY_BL_1_0\" , \"responderDefinition\" : \"ADD_TO_WEBPROXY_BL_1_0\" , \"cortexId\" : \"Stable\" , \"cortexJobId\" : \"Bv0cq3sB8Pn57ilsUkFM\" , \"objectType\" : \"Observable\" , \"objectId\" : \"~816984288\" , \"status\" : \"Failure\" , \"startDate\" : 1630663366136 , \"endDate\" : 1630663372393 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":false,\\\"artifacts\\\":[],\\\"operations\\\":[],...}\" } ] status can be one of: Waiting Success InProgress Failure Deleted report is a string that contains the output of the responder","title":"Responseexample"},{"location":"thehive/api/observable/update/","text":"Update # Update a case or alert Observable by its id Query # PATCH /api/v0/case/artifact/{observableId} PATCH /api/v0/alert/artifact/{observableId} Request Body Example # { \"sighted\" : true , \"ioc\" : true , \"message\" : \"This observable was sighted\" } Fields that can be updated: ioc sighted ignoreSimilarity tags message tlp Once an observable is created, it is not possible to change its type or data ResponseBody Example # { \"_id\" : \"~122884120\" , \"id\" : \"~122884120\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : \"lydia@strangebee.com\" , \"createdAt\" : 1630509659446 , \"updatedAt\" : 1630511666911 , \"_type\" : \"case_artifact\" , \"dataType\" : \"hostname\" , \"data\" : \"server.local\" , \"startDate\" : 1630509659446 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : true , \"message\" : \"This observable was sighted\" , \"reports\" : {}, \"stats\" : {} }","title":"Update"},{"location":"thehive/api/observable/update/#update","text":"Update a case or alert Observable by its id","title":"Update"},{"location":"thehive/api/observable/update/#query","text":"PATCH /api/v0/case/artifact/{observableId} PATCH /api/v0/alert/artifact/{observableId}","title":"Query"},{"location":"thehive/api/observable/update/#request-body-example","text":"{ \"sighted\" : true , \"ioc\" : true , \"message\" : \"This observable was sighted\" } Fields that can be updated: ioc sighted ignoreSimilarity tags message tlp Once an observable is created, it is not possible to change its type or data","title":"Request Body Example"},{"location":"thehive/api/observable/update/#responsebody-example","text":"{ \"_id\" : \"~122884120\" , \"id\" : \"~122884120\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : \"lydia@strangebee.com\" , \"createdAt\" : 1630509659446 , \"updatedAt\" : 1630511666911 , \"_type\" : \"case_artifact\" , \"dataType\" : \"hostname\" , \"data\" : \"server.local\" , \"startDate\" : 1630509659446 , \"tlp\" : 2 , \"tags\" : [], \"ioc\" : true , \"sighted\" : true , \"message\" : \"This observable was sighted\" , \"reports\" : {}, \"stats\" : {} }","title":"ResponseBody Example"},{"location":"thehive/api/organisation/","text":"Organisation APIs # List organisations Create organisation Update organisation List organisation links Set organisation links","title":"Overview"},{"location":"thehive/api/organisation/#organisation-apis","text":"List organisations Create organisation Update organisation List organisation links Set organisation links","title":"Organisation APIs"},{"location":"thehive/api/organisation/create/","text":"Create # API to create a new TheHive organisation. Query # POST /api/v0/organisation Authorization # This API requires a super admin user with manageOrganisation permission Request # Request Body Example # { \"description\" : \"SOC team\" , \"name\" : \"soc\" } Fields # The following fields are required: name : (String) description : (String) Response # Status codes # 201 : if organisation creation completed successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 200 { \"_id\" : \"~204804296\" , \"_type\" : \"organisation\" , \"createdAt\" : 1630385478884 , \"createdBy\" : \"admin@thehive.local\" , \"description\" : \"SOC team\" , \"id\" : \"~204804296\" , \"links\" : [], \"name\" : \"soc\" } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Unauthorized action\" }","title":"Create"},{"location":"thehive/api/organisation/create/#create","text":"API to create a new TheHive organisation.","title":"Create"},{"location":"thehive/api/organisation/create/#query","text":"POST /api/v0/organisation","title":"Query"},{"location":"thehive/api/organisation/create/#authorization","text":"This API requires a super admin user with manageOrganisation permission","title":"Authorization"},{"location":"thehive/api/organisation/create/#request","text":"","title":"Request"},{"location":"thehive/api/organisation/create/#request-body-example","text":"{ \"description\" : \"SOC team\" , \"name\" : \"soc\" }","title":"Request Body Example"},{"location":"thehive/api/organisation/create/#fields","text":"The following fields are required: name : (String) description : (String)","title":"Fields"},{"location":"thehive/api/organisation/create/#response","text":"","title":"Response"},{"location":"thehive/api/organisation/create/#status-codes","text":"201 : if organisation creation completed successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/organisation/create/#responsebody-example","text":"200 { \"_id\" : \"~204804296\" , \"_type\" : \"organisation\" , \"createdAt\" : 1630385478884 , \"createdBy\" : \"admin@thehive.local\" , \"description\" : \"SOC team\" , \"id\" : \"~204804296\" , \"links\" : [], \"name\" : \"soc\" } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Unauthorized action\" }","title":"ResponseBody Example"},{"location":"thehive/api/organisation/list-links/","text":"List links # Query # GET /api/v0/organisation/{idOrName}/links with: idOrName id or name of the organisation Response # Status codes # 200 : if organisation exists 404 : if organisation doesn't exist","title":"List links"},{"location":"thehive/api/organisation/list-links/#list-links","text":"","title":"List links"},{"location":"thehive/api/organisation/list-links/#query","text":"GET /api/v0/organisation/{idOrName}/links with: idOrName id or name of the organisation","title":"Query"},{"location":"thehive/api/organisation/list-links/#response","text":"","title":"Response"},{"location":"thehive/api/organisation/list-links/#status-codes","text":"200 : if organisation exists 404 : if organisation doesn't exist","title":"Status codes"},{"location":"thehive/api/organisation/list/","text":"List/Search # List Organisations . Query # GET /api/v0/query Request # This is a Query API call, where: listOrganisation is the query name { \"query\" : [ { \"_name\" : \"listOrganisation\" }, { \"_fields\" : [ { \"updatedAt\" : \"desc\" } ], \"_name\" : \"sort\" }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] } Response # Status codes # 200 : if query is run successfully 401 : Authentication error ResponseBody Example # 200 [ { \"_createdAt\" : 1630385478884 , \"_createdBy\" : \"admin@thehive.local\" , \"_id\" : \"~204804296\" , \"_type\" : \"Organisation\" , \"_updatedAt\" : 1630415216098 , \"_updatedBy\" : \"admin@thehive.local\" , \"description\" : \"SOC level\" , \"links\" : [ \"cert\" ], \"name\" : \"soc-level1\" }, { \"_createdAt\" : 1606467059596 , \"_createdBy\" : \"admin@thehive.local\" , \"_id\" : \"~4144\" , \"_type\" : \"Organisation\" , \"description\" : \"CERT\" , \"links\" : [ \"soc-level1\" ], \"name\" : \"cert\" }, { \"_createdAt\" : 1606464802479 , \"_createdBy\" : \"system@thehive.local\" , \"_id\" : \"~8408\" , \"_type\" : \"Organisation\" , \"description\" : \"organisation for administration\" , \"links\" : [], \"name\" : \"admin\" } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"List/Search"},{"location":"thehive/api/organisation/list/#listsearch","text":"List Organisations .","title":"List/Search"},{"location":"thehive/api/organisation/list/#query","text":"GET /api/v0/query","title":"Query"},{"location":"thehive/api/organisation/list/#request","text":"This is a Query API call, where: listOrganisation is the query name { \"query\" : [ { \"_name\" : \"listOrganisation\" }, { \"_fields\" : [ { \"updatedAt\" : \"desc\" } ], \"_name\" : \"sort\" }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] }","title":"Request"},{"location":"thehive/api/organisation/list/#response","text":"","title":"Response"},{"location":"thehive/api/organisation/list/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/organisation/list/#responsebody-example","text":"200 [ { \"_createdAt\" : 1630385478884 , \"_createdBy\" : \"admin@thehive.local\" , \"_id\" : \"~204804296\" , \"_type\" : \"Organisation\" , \"_updatedAt\" : 1630415216098 , \"_updatedBy\" : \"admin@thehive.local\" , \"description\" : \"SOC level\" , \"links\" : [ \"cert\" ], \"name\" : \"soc-level1\" }, { \"_createdAt\" : 1606467059596 , \"_createdBy\" : \"admin@thehive.local\" , \"_id\" : \"~4144\" , \"_type\" : \"Organisation\" , \"description\" : \"CERT\" , \"links\" : [ \"soc-level1\" ], \"name\" : \"cert\" }, { \"_createdAt\" : 1606464802479 , \"_createdBy\" : \"system@thehive.local\" , \"_id\" : \"~8408\" , \"_type\" : \"Organisation\" , \"description\" : \"organisation for administration\" , \"links\" : [], \"name\" : \"admin\" } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"ResponseBody Example"},{"location":"thehive/api/organisation/update-links/","text":"Update links # Link orgnisation to one or many other organisations. It sets the list of organisation link to the list provided as input. It overrides the existing list of links. Query # PUT /api/v0/organisation/{idOrName}/links with: idOrName id or name of the organisation Request # Request Body Example # { \"organisations\" : [ \"cert\" , \"csirt\" ] } Fields # organisations ( required ): Array of organisation names Response # Status codes # 201 if the operation completed successfully","title":"Update links"},{"location":"thehive/api/organisation/update-links/#update-links","text":"Link orgnisation to one or many other organisations. It sets the list of organisation link to the list provided as input. It overrides the existing list of links.","title":"Update links"},{"location":"thehive/api/organisation/update-links/#query","text":"PUT /api/v0/organisation/{idOrName}/links with: idOrName id or name of the organisation","title":"Query"},{"location":"thehive/api/organisation/update-links/#request","text":"","title":"Request"},{"location":"thehive/api/organisation/update-links/#request-body-example","text":"{ \"organisations\" : [ \"cert\" , \"csirt\" ] }","title":"Request Body Example"},{"location":"thehive/api/organisation/update-links/#fields","text":"organisations ( required ): Array of organisation names","title":"Fields"},{"location":"thehive/api/organisation/update-links/#response","text":"","title":"Response"},{"location":"thehive/api/organisation/update-links/#status-codes","text":"201 if the operation completed successfully","title":"Status codes"},{"location":"thehive/api/organisation/update/","text":"Update # Query # PATCH /api/v0/organisation/{id} with: id : id or name of the organisation. Authorization # This API requires a super admin user with manageOrganisation permission Request Body Example # { \"description\" : \"SOC level 1 team\" , \"name\" : \"soc-level1\" } Fields # The following fields are editable: name (String) description (String) Response # 204 : if the organisation is updated successfully 401 : Authentication error 403 : Authorization error","title":"Update"},{"location":"thehive/api/organisation/update/#update","text":"","title":"Update"},{"location":"thehive/api/organisation/update/#query","text":"PATCH /api/v0/organisation/{id} with: id : id or name of the organisation.","title":"Query"},{"location":"thehive/api/organisation/update/#authorization","text":"This API requires a super admin user with manageOrganisation permission","title":"Authorization"},{"location":"thehive/api/organisation/update/#request-body-example","text":"{ \"description\" : \"SOC level 1 team\" , \"name\" : \"soc-level1\" }","title":"Request Body Example"},{"location":"thehive/api/organisation/update/#fields","text":"The following fields are editable: name (String) description (String)","title":"Fields"},{"location":"thehive/api/organisation/update/#response","text":"204 : if the organisation is updated successfully 401 : Authentication error 403 : Authorization error","title":"Response"},{"location":"thehive/api/search/","text":"Search APIs # Build queries","title":"Search APIs"},{"location":"thehive/api/search/#search-apis","text":"Build queries","title":"Search APIs"},{"location":"thehive/api/search/filters/","text":"","title":"Filters"},{"location":"thehive/api/search/pagination/","text":"","title":"Pagination"},{"location":"thehive/api/search/query/","text":"Query API # Overview # The Query API is the API used to search for objects with filtering and sorting capabilities. It's an API introduced by TheHive 4 and is optimized for the the new data model. TheHive comes with a list of predefined search Queries like: listOrganisation listUser listAlert listCase Query # POST /api/v0/query Request Body # The Query API request body should be an array of operations of different types: Selection: Required list of objects object by identifier Filtering: optional Sorting: optional Pagination: optional Formatting: optional Examples Simple List { \"query\" : [ { \"_name\" : \"listOrganisation\" } ] } List with filters List organisations called admin { \"query\" : [ { \"_name\" : \"listOrganisation\" }, { \"_like\" : { \"_field\" : \"name\" , \"_value\" : \"admin\" }, \"_name\" : \"filter\" } ] } List with filters and sort List organisations called admin , sorted by ascendant updatedAt value { \"query\" : [ { \"_name\" : \"listOrganisation\" }, { \"_like\" : { \"_field\" : \"name\" , \"_value\" : \"admin\" }, \"_name\" : \"filter\" }, { \"_fields\" : [ { \"updatedAt\" : \"asc\" } ], \"_name\" : \"sort\" } ] } List with pagination List organisations called admin , sorted by ascendant updatedAt value, paginated to display the first 15 items { \"query\" : [ { \"_name\" : \"listOrganisation\" }, { \"_like\" : { \"_field\" : \"name\" , \"_value\" : \"admin\" }, \"_name\" : \"filter\" }, { \"_fields\" : [ { \"updatedAt\" : \"asc\" } ], \"_name\" : \"sort\" }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] }","title":"Query API"},{"location":"thehive/api/search/query/#query-api","text":"","title":"Query API"},{"location":"thehive/api/search/query/#overview","text":"The Query API is the API used to search for objects with filtering and sorting capabilities. It's an API introduced by TheHive 4 and is optimized for the the new data model. TheHive comes with a list of predefined search Queries like: listOrganisation listUser listAlert listCase","title":"Overview"},{"location":"thehive/api/search/query/#query","text":"POST /api/v0/query","title":"Query"},{"location":"thehive/api/search/query/#request-body","text":"The Query API request body should be an array of operations of different types: Selection: Required list of objects object by identifier Filtering: optional Sorting: optional Pagination: optional Formatting: optional Examples Simple List { \"query\" : [ { \"_name\" : \"listOrganisation\" } ] } List with filters List organisations called admin { \"query\" : [ { \"_name\" : \"listOrganisation\" }, { \"_like\" : { \"_field\" : \"name\" , \"_value\" : \"admin\" }, \"_name\" : \"filter\" } ] } List with filters and sort List organisations called admin , sorted by ascendant updatedAt value { \"query\" : [ { \"_name\" : \"listOrganisation\" }, { \"_like\" : { \"_field\" : \"name\" , \"_value\" : \"admin\" }, \"_name\" : \"filter\" }, { \"_fields\" : [ { \"updatedAt\" : \"asc\" } ], \"_name\" : \"sort\" } ] } List with pagination List organisations called admin , sorted by ascendant updatedAt value, paginated to display the first 15 items { \"query\" : [ { \"_name\" : \"listOrganisation\" }, { \"_like\" : { \"_field\" : \"name\" , \"_value\" : \"admin\" }, \"_name\" : \"filter\" }, { \"_fields\" : [ { \"updatedAt\" : \"asc\" } ], \"_name\" : \"sort\" }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] }","title":"Request Body"},{"location":"thehive/api/search/sorting/","text":"","title":"Sorting"},{"location":"thehive/api/task/","text":"Case task APIs # Case task operations # List case tasks Create task Update task Get task details Run responder List responder jobs Case task log oprations # List task logs Create task log Delete task log Run responder on log List responder jobs on log Global task operations # List waiting tasks","title":"Overview"},{"location":"thehive/api/task/#case-task-apis","text":"","title":"Case task APIs"},{"location":"thehive/api/task/#case-task-operations","text":"List case tasks Create task Update task Get task details Run responder List responder jobs","title":"Case task operations"},{"location":"thehive/api/task/#case-task-log-oprations","text":"List task logs Create task log Delete task log Run responder on log List responder jobs on log","title":"Case task log oprations"},{"location":"thehive/api/task/#global-task-operations","text":"List waiting tasks","title":"Global task operations"},{"location":"thehive/api/task/create-log/","text":"Add log # Add a Log to an existing task (requires manageTask permission). Query # POST /api/case/task/{id}/log With: id : Task identifier Request Body Example # { \"message\" : \"The sandbox hasn't detected any suspicious activity\" , \"startDate\" : 1630683608000 , } The only required field is message . If you want to attach a file to the log, you need to use a multipart request curl -XPOST http://THEHIVE/api/v0/case/task/{taskId}/log -F attachment=@report.pdf -F _json=' { \"message\": \"The sandbox report\" } ' Response # Status codes # 201 : if Log is created successfully 401 : Authentication error 403 : Authorization error Response Body Example # 201 { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\u00e7log\" , \"message\" : \"The sandbox hasn't detected any suspicious activity\" , \"startDate\" : 1630683608000 } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create Log, you haven't the permission manageTask\" }","title":"Add log"},{"location":"thehive/api/task/create-log/#add-log","text":"Add a Log to an existing task (requires manageTask permission).","title":"Add log"},{"location":"thehive/api/task/create-log/#query","text":"POST /api/case/task/{id}/log With: id : Task identifier","title":"Query"},{"location":"thehive/api/task/create-log/#request-body-example","text":"{ \"message\" : \"The sandbox hasn't detected any suspicious activity\" , \"startDate\" : 1630683608000 , } The only required field is message . If you want to attach a file to the log, you need to use a multipart request curl -XPOST http://THEHIVE/api/v0/case/task/{taskId}/log -F attachment=@report.pdf -F _json=' { \"message\": \"The sandbox report\" } '","title":"Request Body Example"},{"location":"thehive/api/task/create-log/#response","text":"","title":"Response"},{"location":"thehive/api/task/create-log/#status-codes","text":"201 : if Log is created successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/task/create-log/#response-body-example","text":"201 { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\u00e7log\" , \"message\" : \"The sandbox hasn't detected any suspicious activity\" , \"startDate\" : 1630683608000 } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create Log, you haven't the permission manageTask\" }","title":"Response Body Example"},{"location":"thehive/api/task/create/","text":"Create # Create a Task (requires manageTask permission). Query # POST /api/case/{id}task With: id : Case identifier Request Body Example # { \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"InProgress\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 } The only required field is title . The status can be Waiting , InProgress , Completed or Cancel . Response # Status codes # 201 : if Tasks is created successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 201 { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\" , \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"InProgress\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create Task, you haven't the permission manageTask\" }","title":"Create"},{"location":"thehive/api/task/create/#create","text":"Create a Task (requires manageTask permission).","title":"Create"},{"location":"thehive/api/task/create/#query","text":"POST /api/case/{id}task With: id : Case identifier","title":"Query"},{"location":"thehive/api/task/create/#request-body-example","text":"{ \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"InProgress\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 } The only required field is title . The status can be Waiting , InProgress , Completed or Cancel .","title":"Request Body Example"},{"location":"thehive/api/task/create/#response","text":"","title":"Response"},{"location":"thehive/api/task/create/#status-codes","text":"201 : if Tasks is created successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/task/create/#responsebody-example","text":"201 { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\" , \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"InProgress\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create Task, you haven't the permission manageTask\" }","title":"ResponseBody Example"},{"location":"thehive/api/task/delete-log/","text":"Delete log # Delete a Log of an existing task (requires manageTask permission). Query # DELETE /api/case/task/log/{id} With: id : Log identifier Response # Status codes # 204 : if Log is deleted successfully 401 : Authentication error 403 : Authorization error","title":"Delete log"},{"location":"thehive/api/task/delete-log/#delete-log","text":"Delete a Log of an existing task (requires manageTask permission).","title":"Delete log"},{"location":"thehive/api/task/delete-log/#query","text":"DELETE /api/case/task/log/{id} With: id : Log identifier","title":"Query"},{"location":"thehive/api/task/delete-log/#response","text":"","title":"Response"},{"location":"thehive/api/task/delete-log/#status-codes","text":"204 : if Log is deleted successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/task/get/","text":"Get case task # Get Task of a case. Query # GET /api/case/task/{id} with: id : id of the task. Response # Status codes # 200 : if query is run successfully 401 : Authentication error 404 : The Task is not found ResponseBody Example # 201 { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\" , \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"InProgress\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 404 { \"type\" : \"AuthenticationError\" , \"message\" : \"Task not found\" }","title":"Get case task"},{"location":"thehive/api/task/get/#get-case-task","text":"Get Task of a case.","title":"Get case task"},{"location":"thehive/api/task/get/#query","text":"GET /api/case/task/{id} with: id : id of the task.","title":"Query"},{"location":"thehive/api/task/get/#response","text":"","title":"Response"},{"location":"thehive/api/task/get/#status-codes","text":"200 : if query is run successfully 401 : Authentication error 404 : The Task is not found","title":"Status codes"},{"location":"thehive/api/task/get/#responsebody-example","text":"201 { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\" , \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"InProgress\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 404 { \"type\" : \"AuthenticationError\" , \"message\" : \"Task not found\" }","title":"ResponseBody Example"},{"location":"thehive/api/task/list/","text":"List case tasks # List Task s of a case. Query # POST /api/v0/query Request Body Example # List 15 waiting tasks in case ~25485360. { \"query\" : [ { \"_name\" : \"getCase\" , \"idOrName\" : \"~25485360\" }, { \"_name\" : \"tasks\" }, { \"_name\" : \"filter\" , \"status\" : \"Waiting\" }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] } Response # Status codes # 200 : if query is run successfully 401 : Authentication error ResponseBody Example # 201 [ { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\" , \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"InProgress\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 }, { \"id\" : \"~8360\" , \"_id\" : \"~8360\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630687291729 , \"updatedAt\" : 1630687323936 , \"_type\" : \"case_task\" , \"title\" : \"Block malware URLs in proxy\" , \"group\" : \"containment\" , \"description\" : \"Add identified malicious URLs in proxy black list\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"List case tasks"},{"location":"thehive/api/task/list/#list-case-tasks","text":"List Task s of a case.","title":"List case tasks"},{"location":"thehive/api/task/list/#query","text":"POST /api/v0/query","title":"Query"},{"location":"thehive/api/task/list/#request-body-example","text":"List 15 waiting tasks in case ~25485360. { \"query\" : [ { \"_name\" : \"getCase\" , \"idOrName\" : \"~25485360\" }, { \"_name\" : \"tasks\" }, { \"_name\" : \"filter\" , \"status\" : \"Waiting\" }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] }","title":"Request Body Example"},{"location":"thehive/api/task/list/#response","text":"","title":"Response"},{"location":"thehive/api/task/list/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/task/list/#responsebody-example","text":"201 [ { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\" , \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"InProgress\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 }, { \"id\" : \"~8360\" , \"_id\" : \"~8360\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630687291729 , \"updatedAt\" : 1630687323936 , \"_type\" : \"case_task\" , \"title\" : \"Block malware URLs in proxy\" , \"group\" : \"containment\" , \"description\" : \"Add identified malicious URLs in proxy black list\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"ResponseBody Example"},{"location":"thehive/api/task/log-responder-jobs/","text":"List responder jobs on log # List actions run on a log. Query # GET /api/connector/cortex/action/case_task_log/{id} With: id : Log identifier Response # Status codes # 200 : if query is run successfully 401 : Authentication error Response Body Example # 200 [ { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Log\" , \"objectId\" : \"~25313328\" , \"status\" : \"Success\" , \"startDate\" : 1630917246993 , \"endDate\" : 1630917254406 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":true,\\\"artifacts\\\":[],\\\"operations\\\":[],\\\\\\\"message\\\\\\\":\\\\\\\"Ok\\\\\\\",\\\\\\\"parameters\\\\\\\":{\\\\\\\"organisation\\\\\\\":\\\\\\\"StrangeBee\\\\\\\",\\\\\\\"user\\\\\\\":\\\\\\\"jerome@strangebee.com\\\\\\\"},\\\\\\\"config\\\\\\\":{\\\\\\\"proxy_https\\\\\\\":null,\\\\\\\"cacerts\\\\\\\":null,\\\\\\\"check_tlp\\\\\\\":false,\\\\\\\"max_tlp\\\\\\\":2,\\\\\\\"check_pap\\\\\\\":false,\\\\\\\"max_pap\\\\\\\":2,\\\\\\\"jobTimeout\\\\\\\":30,\\\\\\\"proxy_http\\\\\\\":null}}\\\"}\" } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"List responder jobs on log"},{"location":"thehive/api/task/log-responder-jobs/#list-responder-jobs-on-log","text":"List actions run on a log.","title":"List responder jobs on log"},{"location":"thehive/api/task/log-responder-jobs/#query","text":"GET /api/connector/cortex/action/case_task_log/{id} With: id : Log identifier","title":"Query"},{"location":"thehive/api/task/log-responder-jobs/#response","text":"","title":"Response"},{"location":"thehive/api/task/log-responder-jobs/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/task/log-responder-jobs/#response-body-example","text":"200 [ { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Log\" , \"objectId\" : \"~25313328\" , \"status\" : \"Success\" , \"startDate\" : 1630917246993 , \"endDate\" : 1630917254406 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":true,\\\"artifacts\\\":[],\\\"operations\\\":[],\\\\\\\"message\\\\\\\":\\\\\\\"Ok\\\\\\\",\\\\\\\"parameters\\\\\\\":{\\\\\\\"organisation\\\\\\\":\\\\\\\"StrangeBee\\\\\\\",\\\\\\\"user\\\\\\\":\\\\\\\"jerome@strangebee.com\\\\\\\"},\\\\\\\"config\\\\\\\":{\\\\\\\"proxy_https\\\\\\\":null,\\\\\\\"cacerts\\\\\\\":null,\\\\\\\"check_tlp\\\\\\\":false,\\\\\\\"max_tlp\\\\\\\":2,\\\\\\\"check_pap\\\\\\\":false,\\\\\\\"max_pap\\\\\\\":2,\\\\\\\"jobTimeout\\\\\\\":30,\\\\\\\"proxy_http\\\\\\\":null}}\\\"}\" } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"Response Body Example"},{"location":"thehive/api/task/log-run-responder/","text":"Run responder # Run a responder on a Log (requires manageAction permission). Query # POST /api/connector/cortex/action Request Body Example # { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"cortexId\" : \"local-cortex\" , \"objectType\" : \"case_task_log\" , \"objectId\" : \"~11123\" } The required fields are responderId , objectType and objectId . Response # Status codes # 201 : if responder is started successfully 401 : Authentication error 403 : Authorization error 404 : Log is not found Response Body Example # 201 { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Log\" , \"objectId\" : \"~25313328\" , \"status\" : \"Waiting\" , \"startDate\" : 1630917246993 , \"operations\" : \"[]\" , \"report\" : \"{}\" } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create action, you haven't the permission manageTask\" } 404 { \"type\" : \"AuthenticationError\" , \"message\" : \"Log not found\" }","title":"Run responder"},{"location":"thehive/api/task/log-run-responder/#run-responder","text":"Run a responder on a Log (requires manageAction permission).","title":"Run responder"},{"location":"thehive/api/task/log-run-responder/#query","text":"POST /api/connector/cortex/action","title":"Query"},{"location":"thehive/api/task/log-run-responder/#request-body-example","text":"{ \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"cortexId\" : \"local-cortex\" , \"objectType\" : \"case_task_log\" , \"objectId\" : \"~11123\" } The required fields are responderId , objectType and objectId .","title":"Request Body Example"},{"location":"thehive/api/task/log-run-responder/#response","text":"","title":"Response"},{"location":"thehive/api/task/log-run-responder/#status-codes","text":"201 : if responder is started successfully 401 : Authentication error 403 : Authorization error 404 : Log is not found","title":"Status codes"},{"location":"thehive/api/task/log-run-responder/#response-body-example","text":"201 { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Log\" , \"objectId\" : \"~25313328\" , \"status\" : \"Waiting\" , \"startDate\" : 1630917246993 , \"operations\" : \"[]\" , \"report\" : \"{}\" } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create action, you haven't the permission manageTask\" } 404 { \"type\" : \"AuthenticationError\" , \"message\" : \"Log not found\" }","title":"Response Body Example"},{"location":"thehive/api/task/logs/","text":"List task logs # List Task log s of a Case . Query # POST /api/v1/query?name=case-task-logs Request Body Example # { \"query\" :[{ \"_name\" : \"getTask\" , \"idOrName\" : \"id\" }, { \"_name\" : \"logs\" }, { \"_name\" : \"sort\" , \"_fields\" :[{ \"date\" : \"desc\" }] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 10 , \"extraData\" :[ \"actionCount\" ] }] } Response # Status codes # 200 : if query is run successfully 401 : Authentication error ResponseBody Example # 200 [ { \"_id\" : \"~1421384\" , \"_type\" : \"Log\" , \"_createdBy\" : \"analyst@soc\" , \"_createdAt\" : 1637090593968 , \"message\" : \"42\" , \"date\" : 1637090593968 , \"owner\" : \"analyst@soc\" , \"extraData\" :{ \"actionCount\" : 0 } }, { \"_id\" : \"~1429680\" , \"_type\" : \"Log\" , \"_createdBy\" : \"analyst@soc\" , \"_createdAt\" : 1637090578809 , \"message\" : \"test sample\" , \"date\" : 1637090578809 , \"owner\" : \"analyst@soc\" , \"extraData\" :{ \"actionCount\" : 0 } } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"List task logs"},{"location":"thehive/api/task/logs/#list-task-logs","text":"List Task log s of a Case .","title":"List task logs"},{"location":"thehive/api/task/logs/#query","text":"POST /api/v1/query?name=case-task-logs","title":"Query"},{"location":"thehive/api/task/logs/#request-body-example","text":"{ \"query\" :[{ \"_name\" : \"getTask\" , \"idOrName\" : \"id\" }, { \"_name\" : \"logs\" }, { \"_name\" : \"sort\" , \"_fields\" :[{ \"date\" : \"desc\" }] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 10 , \"extraData\" :[ \"actionCount\" ] }] }","title":"Request Body Example"},{"location":"thehive/api/task/logs/#response","text":"","title":"Response"},{"location":"thehive/api/task/logs/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/task/logs/#responsebody-example","text":"200 [ { \"_id\" : \"~1421384\" , \"_type\" : \"Log\" , \"_createdBy\" : \"analyst@soc\" , \"_createdAt\" : 1637090593968 , \"message\" : \"42\" , \"date\" : 1637090593968 , \"owner\" : \"analyst@soc\" , \"extraData\" :{ \"actionCount\" : 0 } }, { \"_id\" : \"~1429680\" , \"_type\" : \"Log\" , \"_createdBy\" : \"analyst@soc\" , \"_createdAt\" : 1637090578809 , \"message\" : \"test sample\" , \"date\" : 1637090578809 , \"owner\" : \"analyst@soc\" , \"extraData\" :{ \"actionCount\" : 0 } } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"ResponseBody Example"},{"location":"thehive/api/task/responder-jobs/","text":"List responder jobs # List actions run on a task. Query # GET /api/connector/cortex/action/case_task/{id} With: id : Task identifier Response # Status codes # 200 : if query is run successfully 401 : Authentication error Response Body Example # 200 [ { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Task\" , \"objectId\" : \"~25313328\" , \"status\" : \"Success\" , \"startDate\" : 1630917246993 , \"endDate\" : 1630917254406 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":true,\\\"artifacts\\\":[],\\\"operations\\\":[],\\\\\\\"message\\\\\\\":\\\\\\\"Ok\\\\\\\",\\\\\\\"parameters\\\\\\\":{\\\\\\\"organisation\\\\\\\":\\\\\\\"StrangeBee\\\\\\\",\\\\\\\"user\\\\\\\":\\\\\\\"jerome@strangebee.com\\\\\\\"},\\\\\\\"config\\\\\\\":{\\\\\\\"proxy_https\\\\\\\":null,\\\\\\\"cacerts\\\\\\\":null,\\\\\\\"check_tlp\\\\\\\":false,\\\\\\\"max_tlp\\\\\\\":2,\\\\\\\"check_pap\\\\\\\":false,\\\\\\\"max_pap\\\\\\\":2,\\\\\\\"jobTimeout\\\\\\\":30,\\\\\\\"proxy_http\\\\\\\":null}}\\\"}\" } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"List responder jobs"},{"location":"thehive/api/task/responder-jobs/#list-responder-jobs","text":"List actions run on a task.","title":"List responder jobs"},{"location":"thehive/api/task/responder-jobs/#query","text":"GET /api/connector/cortex/action/case_task/{id} With: id : Task identifier","title":"Query"},{"location":"thehive/api/task/responder-jobs/#response","text":"","title":"Response"},{"location":"thehive/api/task/responder-jobs/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/task/responder-jobs/#response-body-example","text":"200 [ { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Task\" , \"objectId\" : \"~25313328\" , \"status\" : \"Success\" , \"startDate\" : 1630917246993 , \"endDate\" : 1630917254406 , \"operations\" : \"[]\" , \"report\" : \"{\\\"summary\\\":{\\\"taxonomies\\\":[]},\\\"full\\\":null,\\\"success\\\":true,\\\"artifacts\\\":[],\\\"operations\\\":[],\\\\\\\"message\\\\\\\":\\\\\\\"Ok\\\\\\\",\\\\\\\"parameters\\\\\\\":{\\\\\\\"organisation\\\\\\\":\\\\\\\"StrangeBee\\\\\\\",\\\\\\\"user\\\\\\\":\\\\\\\"jerome@strangebee.com\\\\\\\"},\\\\\\\"config\\\\\\\":{\\\\\\\"proxy_https\\\\\\\":null,\\\\\\\"cacerts\\\\\\\":null,\\\\\\\"check_tlp\\\\\\\":false,\\\\\\\"max_tlp\\\\\\\":2,\\\\\\\"check_pap\\\\\\\":false,\\\\\\\"max_pap\\\\\\\":2,\\\\\\\"jobTimeout\\\\\\\":30,\\\\\\\"proxy_http\\\\\\\":null}}\\\"}\" } ] 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"Response Body Example"},{"location":"thehive/api/task/run-responder/","text":"Run responder # Run a responder on a Task (requires manageAction permission). Query # POST /api/connector/cortex/action Request Body Example # { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"cortexId\" : \"local-cortex\" , \"objectType\" : \"case_task\" , \"objectId\" : \"~11123\" } The required fields are responderId , objectType and objectId . Response # Status codes # 201 : if responder is started successfully 401 : Authentication error 403 : Authorization error 404 : Task is not found Response Body Example # 201 { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Task\" , \"objectId\" : \"~25313328\" , \"status\" : \"Waiting\" , \"startDate\" : 1630917246993 , \"operations\" : \"[]\" , \"report\" : \"{}\" } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create action, you haven't the permission manageTask\" } 404 { \"type\" : \"AuthenticationError\" , \"message\" : \"Task not found\" }","title":"Run responder"},{"location":"thehive/api/task/run-responder/#run-responder","text":"Run a responder on a Task (requires manageAction permission).","title":"Run responder"},{"location":"thehive/api/task/run-responder/#query","text":"POST /api/connector/cortex/action","title":"Query"},{"location":"thehive/api/task/run-responder/#request-body-example","text":"{ \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"cortexId\" : \"local-cortex\" , \"objectType\" : \"case_task\" , \"objectId\" : \"~11123\" } The required fields are responderId , objectType and objectId .","title":"Request Body Example"},{"location":"thehive/api/task/run-responder/#response","text":"","title":"Response"},{"location":"thehive/api/task/run-responder/#status-codes","text":"201 : if responder is started successfully 401 : Authentication error 403 : Authorization error 404 : Task is not found","title":"Status codes"},{"location":"thehive/api/task/run-responder/#response-body-example","text":"201 { \"responderId\" : \"25dcbbb69d50dd5a5ae4bd55f4ca5903\" , \"responderName\" : \"reponderName_1_0\" , \"responderDefinition\" : \"reponderName_1_0\" , \"cortexId\" : \"local-cortex\" , \"cortexJobId\" : \"408-unsB3SwW9-eEPXXW\" , \"objectType\" : \"Task\" , \"objectId\" : \"~25313328\" , \"status\" : \"Waiting\" , \"startDate\" : 1630917246993 , \"operations\" : \"[]\" , \"report\" : \"{}\" } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to create action, you haven't the permission manageTask\" } 404 { \"type\" : \"AuthenticationError\" , \"message\" : \"Task not found\" }","title":"Response Body Example"},{"location":"thehive/api/task/update/","text":"Update # Update a Task (requires manageTask permission). Query # PATCH /api/case/task/{id} with: id : id of the task. Request Body Example # { \"title\" : \"Block malware URLs in proxy\" , \"group\" : \"containment\" , \"description\" : \"Add identified malicious URLs in proxy black list\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"Waiting\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 5 , \"dueDate\" : 1630694608000 } No fields are required. Response # Status codes # 200 : if Task is updated successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # 201 { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"updatedBy\" : \"jerome@strangebee.com\" , \"updatedAt\" : 1630685486000 , \"_type\" : \"case_task\" , \"title\" : \"Block malware URLs in proxy\" , \"group\" : \"containment\" , \"description\" : \"Add identified malicious URLs in proxy black list\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"Waiting\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 5 , \"dueDate\" : 1630694608000 } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to update Task, you haven't the permission manageTask\" }","title":"Update"},{"location":"thehive/api/task/update/#update","text":"Update a Task (requires manageTask permission).","title":"Update"},{"location":"thehive/api/task/update/#query","text":"PATCH /api/case/task/{id} with: id : id of the task.","title":"Query"},{"location":"thehive/api/task/update/#request-body-example","text":"{ \"title\" : \"Block malware URLs in proxy\" , \"group\" : \"containment\" , \"description\" : \"Add identified malicious URLs in proxy black list\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"Waiting\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 5 , \"dueDate\" : 1630694608000 } No fields are required.","title":"Request Body Example"},{"location":"thehive/api/task/update/#response","text":"","title":"Response"},{"location":"thehive/api/task/update/#status-codes","text":"200 : if Task is updated successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/task/update/#responsebody-example","text":"201 { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"updatedBy\" : \"jerome@strangebee.com\" , \"updatedAt\" : 1630685486000 , \"_type\" : \"case_task\" , \"title\" : \"Block malware URLs in proxy\" , \"group\" : \"containment\" , \"description\" : \"Add identified malicious URLs in proxy black list\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"Waiting\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 5 , \"dueDate\" : 1630694608000 } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" } 403 { \"type\" : \"AuthorizationError\" , \"message\" : \"Your are not authorized to update Task, you haven't the permission manageTask\" }","title":"ResponseBody Example"},{"location":"thehive/api/task/waiting-tasks/","text":"List waiting tasks # List all waiting Task s. Query # POST /api/v0/query Request Body Example # List 15 waiting tasks, sorted by flag and startDate . { \"query\" : [ { \"_name\" : \"waitingTasks\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"flag\" : \"desc\" }, { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] } Response # Status codes # 200 : if query is run successfully 401 : Authentication error ResponseBody Example # 201 [ { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\" , \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"Waiting\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 }, { \"id\" : \"~8360\" , \"_id\" : \"~8360\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630687291729 , \"updatedAt\" : 1630687323936 , \"_type\" : \"case_task\" , \"title\" : \"Block malware URLs in proxy\" , \"group\" : \"containment\" , \"description\" : \"Add identified malicious URLs in proxy black list\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"List waiting tasks"},{"location":"thehive/api/task/waiting-tasks/#list-waiting-tasks","text":"List all waiting Task s.","title":"List waiting tasks"},{"location":"thehive/api/task/waiting-tasks/#query","text":"POST /api/v0/query","title":"Query"},{"location":"thehive/api/task/waiting-tasks/#request-body-example","text":"List 15 waiting tasks, sorted by flag and startDate . { \"query\" : [ { \"_name\" : \"waitingTasks\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"flag\" : \"desc\" }, { \"startDate\" : \"desc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 } ] }","title":"Request Body Example"},{"location":"thehive/api/task/waiting-tasks/#response","text":"","title":"Response"},{"location":"thehive/api/task/waiting-tasks/#status-codes","text":"200 : if query is run successfully 401 : Authentication error","title":"Status codes"},{"location":"thehive/api/task/waiting-tasks/#responsebody-example","text":"201 [ { \"id\" : \"~4264\" , \"_id\" : \"~4264\" , \"createdBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630684502715 , \"_type\" : \"case_task\" , \"title\" : \"Malware analysis\" , \"group\" : \"identification\" , \"description\" : \"Analysis of the file to identify the malware\" , \"owner\" : \"jerome@strangebee.com\" , \"status\" : \"Waiting\" , \"flag\" : false , \"startDate\" : 1630683608000 , \"endDate\" : 1630684608000 , \"order\" : 3 , \"dueDate\" : 1630694608000 }, { \"id\" : \"~8360\" , \"_id\" : \"~8360\" , \"createdBy\" : \"jerome@strangebee.com\" , \"updatedBy\" : \"jerome@strangebee.com\" , \"createdAt\" : 1630687291729 , \"updatedAt\" : 1630687323936 , \"_type\" : \"case_task\" , \"title\" : \"Block malware URLs in proxy\" , \"group\" : \"containment\" , \"description\" : \"Add identified malicious URLs in proxy black list\" , \"status\" : \"Waiting\" , \"flag\" : false , \"order\" : 0 } 401 { \"type\" : \"AuthenticationError\" , \"message\" : \"Authentication failure\" }","title":"ResponseBody Example"},{"location":"thehive/api/ttp/","text":"Tactic, Technique and Procedure APIs # List case TTPs Create TTP Update TTP Delete TTP","title":"Tactic, Technique and Procedure APIs"},{"location":"thehive/api/ttp/#tactic-technique-and-procedure-apis","text":"List case TTPs Create TTP Update TTP Delete TTP","title":"Tactic, Technique and Procedure APIs"},{"location":"thehive/api/ttp/create/","text":"Create # Query # Request Body Example # ResponseBody Example #","title":"Create"},{"location":"thehive/api/ttp/create/#create","text":"","title":"Create"},{"location":"thehive/api/ttp/create/#query","text":"","title":"Query"},{"location":"thehive/api/ttp/create/#request-body-example","text":"","title":"Request Body Example"},{"location":"thehive/api/ttp/create/#responsebody-example","text":"","title":"ResponseBody Example"},{"location":"thehive/api/ttp/delete/","text":"","title":"Delete"},{"location":"thehive/api/ttp/list/","text":"","title":"List"},{"location":"thehive/api/ttp/update/","text":"Update # Query # Request Body Example # ResponseBody Example #","title":"Update"},{"location":"thehive/api/ttp/update/#update","text":"","title":"Update"},{"location":"thehive/api/ttp/update/#query","text":"","title":"Query"},{"location":"thehive/api/ttp/update/#request-body-example","text":"","title":"Request Body Example"},{"location":"thehive/api/ttp/update/#responsebody-example","text":"","title":"ResponseBody Example"},{"location":"thehive/api/user/","text":"User APIs # List users Create a user Update a user Delete a user Lock user Generate API key Get API key Revoke API key Set password","title":"Overview"},{"location":"thehive/api/user/#user-apis","text":"List users Create a user Update a user Delete a user Lock user Generate API key Get API key Revoke API key Set password","title":"User APIs"},{"location":"thehive/api/user/create/","text":"Create # Create an User . Query # POST /api/v1/user Request Body Example # { \"login\" : \"jerome@strangebee.com\" , \"name\" : \"Jerome\" , \"organisation\" : \"StrangeBee\" , \"profile\" : \"org-admin\" , \"email\" : \"jerome@strangebee.com\" , \"password\" : \"my-secret-password\" } The following fields are required: login : (String - email address) name : (String) organisation : (String) profile : [admin|org-admin|analyst|read-only|any customed profile] Response # Status codes # 201 : if User is created successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # { \"_id\" : \"~947527808\" , \"_createdBy\" : \"admin@thehive.local\" , \"_createdAt\" : 1630411433091 , \"login\" : \"jerome@strangebee.com\" , \"name\" : \"Jerome\" , \"hasKey\" : false , \"hasPassword\" : false , \"hasMFA\" : false , \"locked\" : false , \"profile\" : \"analyst\" , \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCase\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ], \"organisation\" : \"StrangeBee\" , \"organisations\" : [], \"email\" : \"jerome@strangebee.com\" }","title":"Create"},{"location":"thehive/api/user/create/#create","text":"Create an User .","title":"Create"},{"location":"thehive/api/user/create/#query","text":"POST /api/v1/user","title":"Query"},{"location":"thehive/api/user/create/#request-body-example","text":"{ \"login\" : \"jerome@strangebee.com\" , \"name\" : \"Jerome\" , \"organisation\" : \"StrangeBee\" , \"profile\" : \"org-admin\" , \"email\" : \"jerome@strangebee.com\" , \"password\" : \"my-secret-password\" } The following fields are required: login : (String - email address) name : (String) organisation : (String) profile : [admin|org-admin|analyst|read-only|any customed profile]","title":"Request Body Example"},{"location":"thehive/api/user/create/#response","text":"","title":"Response"},{"location":"thehive/api/user/create/#status-codes","text":"201 : if User is created successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/user/create/#responsebody-example","text":"{ \"_id\" : \"~947527808\" , \"_createdBy\" : \"admin@thehive.local\" , \"_createdAt\" : 1630411433091 , \"login\" : \"jerome@strangebee.com\" , \"name\" : \"Jerome\" , \"hasKey\" : false , \"hasPassword\" : false , \"hasMFA\" : false , \"locked\" : false , \"profile\" : \"analyst\" , \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCase\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ], \"organisation\" : \"StrangeBee\" , \"organisations\" : [], \"email\" : \"jerome@strangebee.com\" }","title":"ResponseBody Example"},{"location":"thehive/api/user/delete/","text":"Delete # Delete a User . Query # DELETE /api/v1/user/{id}/force?organisation={ORG_NAME} with: id : id or login of the user ORG_NAME : the organisation name from which the user is to be removed Response # Status codes # 204 : if User is successfully deleted 401 : Authentication error 403 : Authorization error","title":"Delete"},{"location":"thehive/api/user/delete/#delete","text":"Delete a User .","title":"Delete"},{"location":"thehive/api/user/delete/#query","text":"DELETE /api/v1/user/{id}/force?organisation={ORG_NAME} with: id : id or login of the user ORG_NAME : the organisation name from which the user is to be removed","title":"Query"},{"location":"thehive/api/user/delete/#response","text":"","title":"Response"},{"location":"thehive/api/user/delete/#status-codes","text":"204 : if User is successfully deleted 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/user/generate-api-key/","text":"Generate API key # Generate an API key for a user. Query # POST /api/v1/user/{id}/key/renew with: id : id or login of the user Request Body Example # The body is empty. Response # Status codes # 200 : if the API key have succesfully been generated 401 : Authentication error 403 : Authorization error ResponseBody Example # The key in plain text. BOXTE+Cq0qrZcHhTK4j0LpT/TVW5auOz","title":"Generate API key"},{"location":"thehive/api/user/generate-api-key/#generate-api-key","text":"Generate an API key for a user.","title":"Generate API key"},{"location":"thehive/api/user/generate-api-key/#query","text":"POST /api/v1/user/{id}/key/renew with: id : id or login of the user","title":"Query"},{"location":"thehive/api/user/generate-api-key/#request-body-example","text":"The body is empty.","title":"Request Body Example"},{"location":"thehive/api/user/generate-api-key/#response","text":"","title":"Response"},{"location":"thehive/api/user/generate-api-key/#status-codes","text":"200 : if the API key have succesfully been generated 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/user/generate-api-key/#responsebody-example","text":"The key in plain text. BOXTE+Cq0qrZcHhTK4j0LpT/TVW5auOz","title":"ResponseBody Example"},{"location":"thehive/api/user/get-api-key/","text":"Get API key # Get the API key of a user. Query # GET /api/v1/user/{id}/key with: id : id or login of the user Response # Status codes # 200 : if the API key have succesfully been generated 401 : Authentication error 403 : Authorization error ResponseBody Example # BOXTE+Cq0qrZcHhTK4j0LpT/TVW5auOz","title":"Get API key"},{"location":"thehive/api/user/get-api-key/#get-api-key","text":"Get the API key of a user.","title":"Get API key"},{"location":"thehive/api/user/get-api-key/#query","text":"GET /api/v1/user/{id}/key with: id : id or login of the user","title":"Query"},{"location":"thehive/api/user/get-api-key/#response","text":"","title":"Response"},{"location":"thehive/api/user/get-api-key/#status-codes","text":"200 : if the API key have succesfully been generated 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/user/get-api-key/#responsebody-example","text":"BOXTE+Cq0qrZcHhTK4j0LpT/TVW5auOz","title":"ResponseBody Example"},{"location":"thehive/api/user/list/","text":"List # List users. Query # POST /api/v1/query Request Body Example # List last 15 users created. { \"query\" : [ { \"_name\" : \"getOrganisation\" , \"idOrName\" : \"StrangeBee\" }, { \"_name\" : \"users\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"login\" : \"asc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 , \"organisation\" : \"StrangeBee\" } ] } Response # Status codes # 200 : if query is run successfully 401 : Authentication error 403 : Authorization error ResponseBody Example # [ { \"_id\" : \"~947527808\" , \"_createdBy\" : \"admin@thehive.local\" , \"_createdAt\" : 1630411433091 , \"login\" : \"jerome@strangebee.com\" , \"name\" : \"Jerome\" , \"hasKey\" : false , \"hasPassword\" : false , \"hasMFA\" : false , \"locked\" : false , \"profile\" : \"analyst\" , \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCase\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ], \"organisation\" : \"StrangeBee\" , \"organisations\" : [] } ]","title":"List"},{"location":"thehive/api/user/list/#list","text":"List users.","title":"List"},{"location":"thehive/api/user/list/#query","text":"POST /api/v1/query","title":"Query"},{"location":"thehive/api/user/list/#request-body-example","text":"List last 15 users created. { \"query\" : [ { \"_name\" : \"getOrganisation\" , \"idOrName\" : \"StrangeBee\" }, { \"_name\" : \"users\" }, { \"_name\" : \"sort\" , \"_fields\" : [ { \"login\" : \"asc\" } ] }, { \"_name\" : \"page\" , \"from\" : 0 , \"to\" : 15 , \"organisation\" : \"StrangeBee\" } ] }","title":"Request Body Example"},{"location":"thehive/api/user/list/#response","text":"","title":"Response"},{"location":"thehive/api/user/list/#status-codes","text":"200 : if query is run successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/user/list/#responsebody-example","text":"[ { \"_id\" : \"~947527808\" , \"_createdBy\" : \"admin@thehive.local\" , \"_createdAt\" : 1630411433091 , \"login\" : \"jerome@strangebee.com\" , \"name\" : \"Jerome\" , \"hasKey\" : false , \"hasPassword\" : false , \"hasMFA\" : false , \"locked\" : false , \"profile\" : \"analyst\" , \"permissions\" : [ \"manageShare\" , \"manageAnalyse\" , \"manageTask\" , \"manageCase\" , \"manageProcedure\" , \"managePage\" , \"manageObservable\" , \"manageAlert\" , \"accessTheHiveFS\" , \"manageAction\" ], \"organisation\" : \"StrangeBee\" , \"organisations\" : [] } ]","title":"ResponseBody Example"},{"location":"thehive/api/user/lock/","text":"Lock / Unlock # Lock a User . Query # PATCH /api/v1/user/{id} With: id : id or login of the user Request Body Example # Lock { \"locked\" : true } Unlock { \"locked\" : false } The following fields are required: locked : (Boolean) Response # Status codes # 204 : if User is locked successfully 401 : Authentication error 403 : Authorization error","title":"Lock / Unlock"},{"location":"thehive/api/user/lock/#lock-unlock","text":"Lock a User .","title":"Lock / Unlock"},{"location":"thehive/api/user/lock/#query","text":"PATCH /api/v1/user/{id} With: id : id or login of the user","title":"Query"},{"location":"thehive/api/user/lock/#request-body-example","text":"Lock { \"locked\" : true } Unlock { \"locked\" : false } The following fields are required: locked : (Boolean)","title":"Request Body Example"},{"location":"thehive/api/user/lock/#response","text":"","title":"Response"},{"location":"thehive/api/user/lock/#status-codes","text":"204 : if User is locked successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/user/revoke-api-key/","text":"Revoke API key # Revoke the API key of a user Query # DELETE /api/v1/user/{id}/key with: id : id or login of the user Response # Status codes # 204 : if API key is successfully revoked 401 : Authentication error 403 : Authorization error","title":"Revoke API key"},{"location":"thehive/api/user/revoke-api-key/#revoke-api-key","text":"Revoke the API key of a user","title":"Revoke API key"},{"location":"thehive/api/user/revoke-api-key/#query","text":"DELETE /api/v1/user/{id}/key with: id : id or login of the user","title":"Query"},{"location":"thehive/api/user/revoke-api-key/#response","text":"","title":"Response"},{"location":"thehive/api/user/revoke-api-key/#status-codes","text":"204 : if API key is successfully revoked 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/user/set-password/","text":"Set password # Set a User 's password. The user making the query needs to be an admin of the platform Query # POST /api/v1/user/{id}/password/set with: id : id of the user Request Body Example # { \"password\" : \"thehive1234\" } The following fields are required: password : (String) Response # Status codes # 204 : if password is set successfully 401 : Authentication error 403 : Authorization error","title":"Set password"},{"location":"thehive/api/user/set-password/#set-password","text":"Set a User 's password. The user making the query needs to be an admin of the platform","title":"Set password"},{"location":"thehive/api/user/set-password/#query","text":"POST /api/v1/user/{id}/password/set with: id : id of the user","title":"Query"},{"location":"thehive/api/user/set-password/#request-body-example","text":"{ \"password\" : \"thehive1234\" } The following fields are required: password : (String)","title":"Request Body Example"},{"location":"thehive/api/user/set-password/#response","text":"","title":"Response"},{"location":"thehive/api/user/set-password/#status-codes","text":"204 : if password is set successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/api/user/update/","text":"Update # Update User 's information. Query # PATCH /api/v1/user/{id} With: id : id or login of the user Request Body Example # { \"name\" : \"Jerome\" , \"profile\" : \"org-admin\" , \"organisation\" : \"StrangeBee\" , \"locked\" : false } The field organisation is used if the profile is updated (the profile of an user depends on the organisation). If not specified, the current organisation is used. No fields are required. Response # Status codes # 204 : if User is updated successfully 401 : Authentication error 403 : Authorization error","title":"Update"},{"location":"thehive/api/user/update/#update","text":"Update User 's information.","title":"Update"},{"location":"thehive/api/user/update/#query","text":"PATCH /api/v1/user/{id} With: id : id or login of the user","title":"Query"},{"location":"thehive/api/user/update/#request-body-example","text":"{ \"name\" : \"Jerome\" , \"profile\" : \"org-admin\" , \"organisation\" : \"StrangeBee\" , \"locked\" : false } The field organisation is used if the profile is updated (the profile of an user depends on the organisation). If not specified, the current organisation is used. No fields are required.","title":"Request Body Example"},{"location":"thehive/api/user/update/#response","text":"","title":"Response"},{"location":"thehive/api/user/update/#status-codes","text":"204 : if User is updated successfully 401 : Authentication error 403 : Authorization error","title":"Status codes"},{"location":"thehive/installation-and-configuration/","text":"Installation & configuration guides # Overview # The scalability of TheHive allows it to be set up as a standalone server or as nodes inside a cluster. Any number of nodes can rely on a database and a file system also setup as standalone servers or a cluster. Before starting installing and configuring, you need to identify and define the targetted architecture. Choose a setup # The modular architecture makes it support several types of database, file storage system and indexing system. The initial choices made with the target architecture and the setup are crucial, especially for the database. If high availability and fault tolerance are necessary, implementing a cluster might be the choice, and this choice determines the database, the file storage and indexing system to install. Hardware Pre-requisites Hardware requirements depends on the number of concurrent users and how they use the system. The following table give some information to choose the hardware. Number of users CPU RAM < 3 2 4-8 < 10 4 8-16 < 20 8 16-32 Choose a database # Once the target setup is identified, the first choice to make is the database. Even of local Berkeley DB and Cassandra database are supported, we recommend using Apache Cassandra , which is a scalable and high available Database, even for standalone servers. Berkeley DB can be enough for testing purposes. Upgradability This choice is decisive as migration from Berkeley DB to Cassandra is not possible . Choose a file storage system # Like for databases, several options exist regarding file system. Basically, for standalone setups, using the local filesystem is the easiest solution. If installing a cluster, there are several options: Using a share NFS folder Using Apache Hadoop , a distributed file system Using a S3-compatible storage service ; for example with Min.IO Upgradability Starting with a standalone server and a local file storage and upgrading to a cluster with S3 of Hadoop is possible. Existing files can be moved to the targetted solutions. Choose an index system # Introduced with TheHive 4.1 to increase performances, TheHive relies on a dedicated indexing process. With a standalone setup, using a local index with Lucene is sufficient. In the case of a cluster, all nodes have to connect to the same index: an instance of Elasticsearch is then required. Upgradability Starting with a standalone server and Lucene and upgrading to a cluster with Elasticsearch is possible. Indices can be rebuilt. However, it can takes some time. Installation Guide # The following Guide let you prepare , install and configure TheHive and its prerequisites for Debian and RPM packages based Operating Systems, as well as for other systems and using our binary packages. If you want to build TheHive from sources, you can follow this guide . Configuration Guides # The configuration of TheHive is in files stored in the /etc/thehive folder: application.conf contains all parameters and options logback.xml is dedicated to log management /etc/thehive \u251c\u2500\u2500 application.conf \u251c\u2500\u2500 logback.xml \u2514\u2500\u2500 secret.conf A separate secret.conf file is automatically created by Debian or RPM packages. This file should contain a secret that should be used by one instance. Various aspects can configured in the application.conf file: database and indexing File storage Akka Authentication Connectors Cortex: connecting to one or more organisation MISP: connecting to one or more organisation Webhooks Other service parameters Uses Cases # Basic stand alone server # Follow the installation guides for you prefered operating system. Cluster with 3 TheHive nodes # The folling guide details all the installation and configuration steps to get a cluster with 3 nodes working. The cluster is composed of: 3 TheHive servers 3 Cassandra servers 3 Min.IO servers","title":"Overview"},{"location":"thehive/installation-and-configuration/#installation-configuration-guides","text":"","title":"Installation &amp; configuration guides"},{"location":"thehive/installation-and-configuration/#overview","text":"The scalability of TheHive allows it to be set up as a standalone server or as nodes inside a cluster. Any number of nodes can rely on a database and a file system also setup as standalone servers or a cluster. Before starting installing and configuring, you need to identify and define the targetted architecture.","title":"Overview"},{"location":"thehive/installation-and-configuration/#choose-a-setup","text":"The modular architecture makes it support several types of database, file storage system and indexing system. The initial choices made with the target architecture and the setup are crucial, especially for the database. If high availability and fault tolerance are necessary, implementing a cluster might be the choice, and this choice determines the database, the file storage and indexing system to install. Hardware Pre-requisites Hardware requirements depends on the number of concurrent users and how they use the system. The following table give some information to choose the hardware. Number of users CPU RAM < 3 2 4-8 < 10 4 8-16 < 20 8 16-32","title":"Choose a setup"},{"location":"thehive/installation-and-configuration/#choose-a-database","text":"Once the target setup is identified, the first choice to make is the database. Even of local Berkeley DB and Cassandra database are supported, we recommend using Apache Cassandra , which is a scalable and high available Database, even for standalone servers. Berkeley DB can be enough for testing purposes. Upgradability This choice is decisive as migration from Berkeley DB to Cassandra is not possible .","title":"Choose a database"},{"location":"thehive/installation-and-configuration/#choose-a-file-storage-system","text":"Like for databases, several options exist regarding file system. Basically, for standalone setups, using the local filesystem is the easiest solution. If installing a cluster, there are several options: Using a share NFS folder Using Apache Hadoop , a distributed file system Using a S3-compatible storage service ; for example with Min.IO Upgradability Starting with a standalone server and a local file storage and upgrading to a cluster with S3 of Hadoop is possible. Existing files can be moved to the targetted solutions.","title":"Choose a file storage system"},{"location":"thehive/installation-and-configuration/#choose-an-index-system","text":"Introduced with TheHive 4.1 to increase performances, TheHive relies on a dedicated indexing process. With a standalone setup, using a local index with Lucene is sufficient. In the case of a cluster, all nodes have to connect to the same index: an instance of Elasticsearch is then required. Upgradability Starting with a standalone server and Lucene and upgrading to a cluster with Elasticsearch is possible. Indices can be rebuilt. However, it can takes some time.","title":"Choose an index system"},{"location":"thehive/installation-and-configuration/#installation-guide","text":"The following Guide let you prepare , install and configure TheHive and its prerequisites for Debian and RPM packages based Operating Systems, as well as for other systems and using our binary packages. If you want to build TheHive from sources, you can follow this guide .","title":"Installation Guide"},{"location":"thehive/installation-and-configuration/#configuration-guides","text":"The configuration of TheHive is in files stored in the /etc/thehive folder: application.conf contains all parameters and options logback.xml is dedicated to log management /etc/thehive \u251c\u2500\u2500 application.conf \u251c\u2500\u2500 logback.xml \u2514\u2500\u2500 secret.conf A separate secret.conf file is automatically created by Debian or RPM packages. This file should contain a secret that should be used by one instance. Various aspects can configured in the application.conf file: database and indexing File storage Akka Authentication Connectors Cortex: connecting to one or more organisation MISP: connecting to one or more organisation Webhooks Other service parameters","title":"Configuration Guides"},{"location":"thehive/installation-and-configuration/#uses-cases","text":"","title":"Uses Cases"},{"location":"thehive/installation-and-configuration/#basic-stand-alone-server","text":"Follow the installation guides for you prefered operating system.","title":"Basic stand alone server"},{"location":"thehive/installation-and-configuration/#cluster-with-3-thehive-nodes","text":"The folling guide details all the installation and configuration steps to get a cluster with 3 nodes working. The cluster is composed of: 3 TheHive servers 3 Cassandra servers 3 Min.IO servers","title":"Cluster with 3 TheHive nodes"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/","text":"Use TheHive as a cluster # This guide provides configuration examples for TheHive, Cassandra and MinIO to build a fault-tolerant cluster of 3 active nodes. Prerequisite # 3 servers with TheHive and Cassandra installed. TheHive # In this guide, we are considering the node 1 to be the master node. Start by configuring akka component by editing the /etc/thehive/application.conf file of each node like this: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<My IP address>\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@<Node 1 IP address>:2551\", \"akka://application@<Node 2 IP address>:2551\", \"akka://application@<Node 3 IP address>:2551\" ] } Cassandra # We are considering setting up a cluster of 3 active nodes of Cassandra with a replication factor of 3. That means that all nodes are active and the data is present on each node. This setup is tolerant to a 1 node failure. For the rest of this part, we are considering that all nodes sit on the same network. Configuration # Nodes configuration # For each node, update configuration files with the following parameters: /etc/cassandra/cassandra.yml cluster_name: 'thp' num_tokens: 256 authenticator: PasswordAuthenticator authorizer: CassandraAuthorizer role_manager: CassandraRoleManager data_file_directories: - /var/lib/cassandra/data commitlog_directory: /var/lib/cassandra/commitlog saved_caches_directory: /var/lib/cassandra/saved_caches seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"<ip node 1>, <ip node 2>, <ip node 3>\" listen_interface : eth0 rpc_interface: eth0 endpoint_snitch: SimpleSnitch Ensure to setup the right interface name. delete file /etc/cassandra/cassandra-topology.properties rm /etc/cassandra/cassandra-topology.properties Start nodes # On each node, start the service: service cassandra start Ensure that all nodes are up and running: root@cassandra:/# nodetool status Datacenter: dc1 =============== Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN <ip node 1> 776.53 KiB 256 100.0% a79c9a8c-c99b-4d74-8e78-6b0c252abd86 rack1 UN <ip node 2> 671.72 KiB 256 100.0% 8fda2906-2097-4d62-91f8-005e33d3e839 rack1 UN <ip node 3> 611.54 KiB 256 100.0% 201ab99c-8e16-49b1-9b66-5444044fb1cd rack1 Initialise the database # On one node run (default password for cassandra account is cassandra ): cqlsh <ip node X> -u cassandra Start by changing the password of superadmin named cassandra : ALTER USER cassandra WITH PASSWORD 'NEWPASSWORD' ; exit and reconnect. Ensure user accounts are duplicated on all nodes ALTER KEYSPACE system_auth WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ; Create keyspace named thehive CREATE KEYSPACE thehive WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : '3' } AND durable_writes = 'true' ; Create role thehive and grant permissions on thehive keyspace (choose a password) CREATE ROLE thehive WITH LOGIN = true AND PASSWORD = 'PASSWORD' ; GRANT ALL PERMISSIONS ON KEYSPACE thehive TO 'thehive' ; TheHive associated configuration # Update the configuration of thehive accordingly in /etc/thehive/application.conf : ## Database configuration db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"<ip node 1>\", \"<ip node 2>\", \"<ip node 3>\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"PASSWORD\" cql { cluster-name: thp keyspace: thehive } } Troubleshooting # InvalidRequest: code=2200 [Invalid query] message=\u201dorg.apache.cassandra.auth.CassandraRoleManager doesn\u2019t support PASSWORD\u201d. set the value authenticator: PasswordAuthenticator in cassandra.yaml Caused by: java.util.concurrent.ExecutionException: com.datastax.driver.core.exceptions.UnauthorizedException: Unable to perform authorization of permissions: Unable to perform authorization of super-user permission: Cannot achieve consistency level LOCAL_ONE ALTER KEYSPACE system_auth WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ; MinIO # MinIO distributed mode requires fresh directories. Here is an example of implementation of MinIO with TheHive. The following procedure should be performed on all servers belonging the the cluster. We are considering the setup where the cluster is composed of 3 servers named minio1, minio2 & minio3. Create a dedicated system account # Create a dedicated user with /opt/minio as homedir. adduser minio Create at least 2 data volumes on each server # Create 2 folders on each server: mkdir -p /srv/minio/{1,2} chown -R minio:minio /srv/minio Setup hosts files # Edit /etc/hosts of all servers ip-minio-1 minio1 ip-minio-2 minio2 ip-minio-3 minio3 installation # cd /opt/minio mkdir /opt/minio/{bin,etc} wget -O /opt/minio/bin https://dl.minio.io/server/minio/release/linux-amd64/minio chown -R minio:minio /opt/minio Configuration # Create or edit file `/opt/minio/etc/minio.conf MINIO_OPTS=\"server --address :9100 http://minio{1...3}/srv/minio/{1...2}\" MINIO_ACCESS_KEY=\"<ACCESS_KEY>\" MINIO_SECRET_KEY=\"<SECRET_KEY>\" Create a service file named /usr/lib/systemd/system/minio.service [Unit] Description=minio Documentation=https://docs.min.io Wants=network-online.target After=network-online.target AssertFileIsExecutable=/opt/minio/bin/minio [Service] WorkingDirectory=/opt/minio User=minio Group=minio EnvironmentFile=/opt/minio/etc/minio.conf ExecStart=/opt/minio/bin/minio $MINIO_OPTS Restart=always LimitNOFILE=65536 TimeoutStopSec=0 SendSIGKILL=no [Install] WantedBy=multi-user.target Enable and start the service # systemctl daemon-reload systemctl enable minio systemctl start minio.service Prepare the service for TheHive # Following operations should be performed once all servers are up and running. A new server CAN NOT be added afterward. Connect using the access key and secret key to one server with your browser on port 9100: http://minio:9100 Create a bucket named thehive The bucket should be created and available on all your servers. TheHive associated configuration # For each TheHive node of the cluster, add the relevant storage configuration. Example for the first node thehive1: storage { provider: s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_MINIO_1>:9100\" accessKey = \"<MINIO ACCESS KEY>\" secretKey = \"<MINIO SECRET KEY>\" region = \"us-east-1\" } } alpakka.s3.path-style-access = force us-east-1 is the default region if none has been specified in MinIO configuration. In this case, this parameter is optional. Each TheHive server can connect to one MinIO server.","title":"Use TheHive as a cluster"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#use-thehive-as-a-cluster","text":"This guide provides configuration examples for TheHive, Cassandra and MinIO to build a fault-tolerant cluster of 3 active nodes.","title":"Use TheHive as a cluster"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#prerequisite","text":"3 servers with TheHive and Cassandra installed.","title":"Prerequisite"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#thehive","text":"In this guide, we are considering the node 1 to be the master node. Start by configuring akka component by editing the /etc/thehive/application.conf file of each node like this: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<My IP address>\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@<Node 1 IP address>:2551\", \"akka://application@<Node 2 IP address>:2551\", \"akka://application@<Node 3 IP address>:2551\" ] }","title":"TheHive"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#cassandra","text":"We are considering setting up a cluster of 3 active nodes of Cassandra with a replication factor of 3. That means that all nodes are active and the data is present on each node. This setup is tolerant to a 1 node failure. For the rest of this part, we are considering that all nodes sit on the same network.","title":"Cassandra"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#configuration","text":"","title":"Configuration"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#nodes-configuration","text":"For each node, update configuration files with the following parameters: /etc/cassandra/cassandra.yml cluster_name: 'thp' num_tokens: 256 authenticator: PasswordAuthenticator authorizer: CassandraAuthorizer role_manager: CassandraRoleManager data_file_directories: - /var/lib/cassandra/data commitlog_directory: /var/lib/cassandra/commitlog saved_caches_directory: /var/lib/cassandra/saved_caches seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: - seeds: \"<ip node 1>, <ip node 2>, <ip node 3>\" listen_interface : eth0 rpc_interface: eth0 endpoint_snitch: SimpleSnitch Ensure to setup the right interface name. delete file /etc/cassandra/cassandra-topology.properties rm /etc/cassandra/cassandra-topology.properties","title":"Nodes configuration"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#start-nodes","text":"On each node, start the service: service cassandra start Ensure that all nodes are up and running: root@cassandra:/# nodetool status Datacenter: dc1 =============== Status=Up/Down |/ State=Normal/Leaving/Joining/Moving -- Address Load Tokens Owns (effective) Host ID Rack UN <ip node 1> 776.53 KiB 256 100.0% a79c9a8c-c99b-4d74-8e78-6b0c252abd86 rack1 UN <ip node 2> 671.72 KiB 256 100.0% 8fda2906-2097-4d62-91f8-005e33d3e839 rack1 UN <ip node 3> 611.54 KiB 256 100.0% 201ab99c-8e16-49b1-9b66-5444044fb1cd rack1","title":"Start nodes"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#initialise-the-database","text":"On one node run (default password for cassandra account is cassandra ): cqlsh <ip node X> -u cassandra Start by changing the password of superadmin named cassandra : ALTER USER cassandra WITH PASSWORD 'NEWPASSWORD' ; exit and reconnect. Ensure user accounts are duplicated on all nodes ALTER KEYSPACE system_auth WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ; Create keyspace named thehive CREATE KEYSPACE thehive WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : '3' } AND durable_writes = 'true' ; Create role thehive and grant permissions on thehive keyspace (choose a password) CREATE ROLE thehive WITH LOGIN = true AND PASSWORD = 'PASSWORD' ; GRANT ALL PERMISSIONS ON KEYSPACE thehive TO 'thehive' ;","title":"Initialise the database"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#thehive-associated-configuration","text":"Update the configuration of thehive accordingly in /etc/thehive/application.conf : ## Database configuration db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"<ip node 1>\", \"<ip node 2>\", \"<ip node 3>\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"PASSWORD\" cql { cluster-name: thp keyspace: thehive } }","title":"TheHive associated configuration"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#troubleshooting","text":"InvalidRequest: code=2200 [Invalid query] message=\u201dorg.apache.cassandra.auth.CassandraRoleManager doesn\u2019t support PASSWORD\u201d. set the value authenticator: PasswordAuthenticator in cassandra.yaml Caused by: java.util.concurrent.ExecutionException: com.datastax.driver.core.exceptions.UnauthorizedException: Unable to perform authorization of permissions: Unable to perform authorization of super-user permission: Cannot achieve consistency level LOCAL_ONE ALTER KEYSPACE system_auth WITH replication = { 'class' : 'SimpleStrategy' , 'replication_factor' : 3 } ;","title":"Troubleshooting"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#minio","text":"MinIO distributed mode requires fresh directories. Here is an example of implementation of MinIO with TheHive. The following procedure should be performed on all servers belonging the the cluster. We are considering the setup where the cluster is composed of 3 servers named minio1, minio2 & minio3.","title":"MinIO"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#create-a-dedicated-system-account","text":"Create a dedicated user with /opt/minio as homedir. adduser minio","title":"Create a dedicated system account"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#create-at-least-2-data-volumes-on-each-server","text":"Create 2 folders on each server: mkdir -p /srv/minio/{1,2} chown -R minio:minio /srv/minio","title":"Create at least 2 data volumes on each server"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#setup-hosts-files","text":"Edit /etc/hosts of all servers ip-minio-1 minio1 ip-minio-2 minio2 ip-minio-3 minio3","title":"Setup hosts files"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#installation","text":"cd /opt/minio mkdir /opt/minio/{bin,etc} wget -O /opt/minio/bin https://dl.minio.io/server/minio/release/linux-amd64/minio chown -R minio:minio /opt/minio","title":"installation"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#configuration_1","text":"Create or edit file `/opt/minio/etc/minio.conf MINIO_OPTS=\"server --address :9100 http://minio{1...3}/srv/minio/{1...2}\" MINIO_ACCESS_KEY=\"<ACCESS_KEY>\" MINIO_SECRET_KEY=\"<SECRET_KEY>\" Create a service file named /usr/lib/systemd/system/minio.service [Unit] Description=minio Documentation=https://docs.min.io Wants=network-online.target After=network-online.target AssertFileIsExecutable=/opt/minio/bin/minio [Service] WorkingDirectory=/opt/minio User=minio Group=minio EnvironmentFile=/opt/minio/etc/minio.conf ExecStart=/opt/minio/bin/minio $MINIO_OPTS Restart=always LimitNOFILE=65536 TimeoutStopSec=0 SendSIGKILL=no [Install] WantedBy=multi-user.target","title":"Configuration"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#enable-and-start-the-service","text":"systemctl daemon-reload systemctl enable minio systemctl start minio.service","title":"Enable and start the service"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#prepare-the-service-for-thehive","text":"Following operations should be performed once all servers are up and running. A new server CAN NOT be added afterward. Connect using the access key and secret key to one server with your browser on port 9100: http://minio:9100 Create a bucket named thehive The bucket should be created and available on all your servers.","title":"Prepare the service for TheHive"},{"location":"thehive/installation-and-configuration/architecture/3_nodes_cluster/#thehive-associated-configuration_1","text":"For each TheHive node of the cluster, add the relevant storage configuration. Example for the first node thehive1: storage { provider: s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_MINIO_1>:9100\" accessKey = \"<MINIO ACCESS KEY>\" secretKey = \"<MINIO SECRET KEY>\" region = \"us-east-1\" } } alpakka.s3.path-style-access = force us-east-1 is the default region if none has been specified in MinIO configuration. In this case, this parameter is optional. Each TheHive server can connect to one MinIO server.","title":"TheHive associated configuration"},{"location":"thehive/installation-and-configuration/configuration/akka/","text":"Cluster # Quote Akka is a toolkit for building highly concurrent, distributed, and resilient message-driven applications for Java and Scala -- https://akka.io/ Akka is used to make several nodes of TheHive work together and offer a smooth user experience. A good cluster setup requires at least 3 nodes of THeHive applications. For each node, Akka must be configured like this: ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<HOSTNAME OR IP_ADDRESS>\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ] } with: remote.artery.hostname containing the hostname or IP address of the node, cluster.seed-nodes containing the list of akka nodes and beeing the same on all nodes Configuration of a Cluster with 3 nodes Node 1 Akka configuration for Node 1: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.1\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Node 2 Akka configuration for Node 2: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.2\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Node 3 Akka configuration for Node 3: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.3\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } SSL/TLS # Akka supports SSL/TLS to encrypt communications between nodes. A typical configuration with SSL support : ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { transport = tls-tcp canonical { hostname = \"<HOSTNAME OR IP_ADDRESS>\" port = 2551 } ssl.config-ssl-engine { key-store = \"<PATH TO KEYSTORE>\" trust-store = \"<PATH TO TRUSTSTORE>\" key-store-password = \"chamgeme\" key-password = \"chamgeme\" trust-store-password = \"chamgeme\" protocol = \"TLSv1.2\" } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ] } Note Note that akka.remote.artery.transport has changed and akka.ssl.config-ssl-engine needs to be configured. Reference : https://doc.akka.io/docs/akka/current/remoting-artery.html#remote-security About Certificates Use your own internal PKI, or keytool commands to generate your certificates. Reference : https://lightbend.github.io/ssl-config/CertificateGeneration.html#using-keytool Your server certificates should contain various KeyUsage and ExtendedkeyUsage extensions to make everything work properly: KeyUsage extensions nonRepudiation dataEncipherment digitalSignature keyEncipherment ExtendedkeyUsage extensions serverAuth clientAuth Akka configuration with SSL for Node 1 ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { transport = tls-tcp canonical { hostname = \"10.1.2.1\" port = 2551 } ssl.config-ssl-engine { key-store = \"/etc/thehive/application.conf.d/certs/10.1.2.1.jks\" trust-store = \"/etc/thehive/application.conf.d/certs/internal_ca.jks\" key-store-password = \"chamgeme\" key-password = \"chamgeme\" trust-store-password = \"chamgeme\" protocol = \"TLSv1.2\" } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Apply the same principle for the other nodes, and restart all services.","title":"Cluster"},{"location":"thehive/installation-and-configuration/configuration/akka/#cluster","text":"Quote Akka is a toolkit for building highly concurrent, distributed, and resilient message-driven applications for Java and Scala -- https://akka.io/ Akka is used to make several nodes of TheHive work together and offer a smooth user experience. A good cluster setup requires at least 3 nodes of THeHive applications. For each node, Akka must be configured like this: ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"<HOSTNAME OR IP_ADDRESS>\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ] } with: remote.artery.hostname containing the hostname or IP address of the node, cluster.seed-nodes containing the list of akka nodes and beeing the same on all nodes Configuration of a Cluster with 3 nodes Node 1 Akka configuration for Node 1: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.1\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Node 2 Akka configuration for Node 2: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.2\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Node 3 Akka configuration for Node 3: akka { cluster.enable = on actor { provider = cluster } remote.artery { canonical { hostname = \"10.1.2.3\" port = 2551 } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] }","title":"Cluster"},{"location":"thehive/installation-and-configuration/configuration/akka/#ssltls","text":"Akka supports SSL/TLS to encrypt communications between nodes. A typical configuration with SSL support : ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { transport = tls-tcp canonical { hostname = \"<HOSTNAME OR IP_ADDRESS>\" port = 2551 } ssl.config-ssl-engine { key-store = \"<PATH TO KEYSTORE>\" trust-store = \"<PATH TO TRUSTSTORE>\" key-store-password = \"chamgeme\" key-password = \"chamgeme\" trust-store-password = \"chamgeme\" protocol = \"TLSv1.2\" } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@HOSTNAME1:2551\", \"akka://application@HOSTNAME2:2551\", \"akka://application@HOSTNAME3:2551\" ] } Note Note that akka.remote.artery.transport has changed and akka.ssl.config-ssl-engine needs to be configured. Reference : https://doc.akka.io/docs/akka/current/remoting-artery.html#remote-security About Certificates Use your own internal PKI, or keytool commands to generate your certificates. Reference : https://lightbend.github.io/ssl-config/CertificateGeneration.html#using-keytool Your server certificates should contain various KeyUsage and ExtendedkeyUsage extensions to make everything work properly: KeyUsage extensions nonRepudiation dataEncipherment digitalSignature keyEncipherment ExtendedkeyUsage extensions serverAuth clientAuth Akka configuration with SSL for Node 1 ## Akka server akka { cluster.enable = on actor { provider = cluster } remote.artery { transport = tls-tcp canonical { hostname = \"10.1.2.1\" port = 2551 } ssl.config-ssl-engine { key-store = \"/etc/thehive/application.conf.d/certs/10.1.2.1.jks\" trust-store = \"/etc/thehive/application.conf.d/certs/internal_ca.jks\" key-store-password = \"chamgeme\" key-password = \"chamgeme\" trust-store-password = \"chamgeme\" protocol = \"TLSv1.2\" } } # seed node list contains at least one active node cluster.seed-nodes = [ \"akka://application@10.1.2.1:2551\", \"akka://application@10.1.2.2:2551\", \"akka://application@10.1.2.3:2551\" ] } Apply the same principle for the other nodes, and restart all services.","title":"SSL/TLS"},{"location":"thehive/installation-and-configuration/configuration/authentication/","text":"Authentication # Authentication consists of a set of module. Each one tries to authenticate the user. If it fails, the next one in the list is tried until the end of the list. The default configuration for authentication is: auth { providers = [ {name : session} {name : basic, realm : thehive} {name : local} {name : key} ] } Below are the available authentication modules: session # Authenticates HTTP requests using a cookie. This module manage the cookie creation and expiration. It accepts the following configuration parameters: Parameter Type Description inactivity duration the maximum time of user inactivity before the session is closed warning duration the time before the expiration TheHive returns a warning message Example auth { providers = [ { name : session inactivity : 600 minutes warning : 10 minutes } ] } local # Create a session if the provided login and password, or API key is correct according to the local user database. key # Authenticates HTTP requests using API key provided in the authorization header. The format is Authorization: Bearer xxx (xxx is replaced by the API key). The key is searched using other authentication modules (currently, only local authentication module can validate the key). basic # Authenticates HTTP requests using the login and password provided in authorization header using basic authentication format (Base64). Password is checked from the local user database. Parameter Type Description realm string name of the realm. Without this parameter, the browser doesn't ask to authenticate header # Authenticates HTTP requests using a HTTP header containing the user login. This is used to delegate authentication in a reverse proxy. This module accepts the configuration: Parameter Type Description userHeader string the name of the header that contain the user login ad # Use Microsoft ActiveDirectory to authenticate the user. The configuration is: Parameter Type Description hosts list of string the addresses of the domain controllers. If missing, the dnsDomain is used winDomain string the Windows domain name ( MYDOMAIN ) dnsDomain string the Windows domain name in DNS format ( mydomain.local ) useSSL boolean indicate if SSL must be used to connect to domain controller. The global trust store of the JVM is used to validate remote certificate ( JAVA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/truststore.jks\" ) Example auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : ad , hosts : [ \"dc.mydomain.local\" ], dnsDomain : \"mydomain.local\" , winDomain : \"MYDOMAIN\" , } ] } ldap # Use LDAP directory server to authenticate the user. The configuration is: Parameter Type Description hosts list of string the addresses of the LDAP servers bindDN string DN of the service account in LDAP. This account is used to search the user bindPW string password of the service account baseDN string DN where the users are located in filter string filter used to search the user. \"{0}\" is replaced by the user login. A valid filter is: (&(uid={0})(objectClass=posixAccount)) useSSL boolean indicate if SSL must be used to connect to LDAP server. The global trust store of the JVM is used to validate remote certificate ( JAVA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/truststore.jks\" ) Example auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : ldap hosts : [ ldap1.mydomain.local , ldap2.mydomain.local ] bindDN : \"cn=thehive,ou=services,dc=mydomain,dc=local\" bindPW : \"SuperSecretPassword\" baseDN : \"ou=users,dc=mydomain,dc=local\" filter : \"(cn={0})\" useSSL : true } ] } oauth2 # Authenticate the user using an external OAuth2 authenticator server. It accepts the following configuration parameters: Parameter Type Description clientId string client ID in the OAuth2 server clientSecret string client secret in the OAuth2 server redirectUri string the url of TheHive AOuth2 page ( xxx/api/ssoLogin ) responseType string type of the response. Currently only \"code\" is accepted grantType string type of the grant. Currently only \"authorization_code\" is accepted authorizationUrl string the url of the OAuth2 server tokenUrl string the token url of the OAuth2 server userUrl string the url to get user information in OAuth2 server scope list of string list of scope userIdField string the field that contains the id of the user in user info organisationField string (optional) the field that contains the organisation name in user info defaultOrganisation string (optional) the default organisation used to login if not present on user info authorizationHeader string prefix of the authorization header to get user info: Bearer, token, ... Example Keycloak ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : \"code\" grantType : \"authorization_code\" authorizationUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/auth\" authorizationHeader : \"Bearer\" tokenUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/token\" userUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/userinfo\" scope : [ \"openid\" , \"email\" ] userIdField : \"email\" } ] } Okta ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : \"code\" grantType : \"authorization_code\" authorizationUrl : \"https://OKTA/oauth2/v1/authorize\" authorizationHeader : \"Bearer\" tokenUrl : \"http://OKTA/oauth2/v1/token\" userUrl : \"http://OKTA/oauth2/v1/userinfo\" scope : [ \"openid\" , \"email\" ] userIdField : \"email\" } ] } Github ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://github.com/login/oauth/authorize\" authorizationHeader : \"token\" tokenUrl : \"https://github.com/login/oauth/access_token\" userUrl : \"https://api.github.com/user\" scope : [ \"user\" ] userIdField : \"email\" #userOrganisation : \"\" } ] } Note CLIENT_ID and CLIENT_SECRET are created in the OAuth Apps section at https://github.com/settings/developers . this configuration requires that users set the Public email in their Public Profile on https://github.com/settings/profile . Microsoft 365 ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://login.microsoftonline.com/TENANT/oauth2/v2.0/authorize\" authorizationHeader : \"Bearer \" tokenUrl : \"https://login.microsoftonline.com/TENANT/oauth2/v2.0/token\" userUrl : \"https://graph.microsoft.com/v1.0/me\" scope : [ \"User.Read\" ] userIdField : \"mail\" #userOrganisation : \"\" ## if not existing in the response, use default organisation } ] } Note To create CLIENT_ID , CLIENT_SECRET and TENANT , register a new app at https://aad.portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/RegisteredApps . Google Note CLIENT_ID and CLIENT_SECRET are created in the _APIs & Services_ > _Credentials_ section of the GCP Console Instructions on how to create Oauth2 credentials at https://support.google.com/cloud/answer/6158849 For the latest reference for Google auth URLs please check Google's .well-known/openid-configuration User autocreation # To allow users to login without previously creating them, you can enable autocreation by adding user.autoCreateOnSso=true to the top level of your configuration. Example user.autoCreateOnSso : true user.profileFieldName : profile user.organisationFieldName : organisation user.defaults.profile : analyst user.defaults.organisation : cert Multi-Factor Authentication # Multi-Factor Authentication is enabled by default. This means users can configure their MFA through their User Settings page (top-Right corner button > Settings). User administrators can: See which users have activated MFA Reset MFA settings of any user This feature can be **disabled** by setting a config property to false : Example auth.multifactor.enabled = false","title":"Authentication"},{"location":"thehive/installation-and-configuration/configuration/authentication/#authentication","text":"Authentication consists of a set of module. Each one tries to authenticate the user. If it fails, the next one in the list is tried until the end of the list. The default configuration for authentication is: auth { providers = [ {name : session} {name : basic, realm : thehive} {name : local} {name : key} ] } Below are the available authentication modules:","title":"Authentication"},{"location":"thehive/installation-and-configuration/configuration/authentication/#session","text":"Authenticates HTTP requests using a cookie. This module manage the cookie creation and expiration. It accepts the following configuration parameters: Parameter Type Description inactivity duration the maximum time of user inactivity before the session is closed warning duration the time before the expiration TheHive returns a warning message Example auth { providers = [ { name : session inactivity : 600 minutes warning : 10 minutes } ] }","title":"session"},{"location":"thehive/installation-and-configuration/configuration/authentication/#local","text":"Create a session if the provided login and password, or API key is correct according to the local user database.","title":"local"},{"location":"thehive/installation-and-configuration/configuration/authentication/#key","text":"Authenticates HTTP requests using API key provided in the authorization header. The format is Authorization: Bearer xxx (xxx is replaced by the API key). The key is searched using other authentication modules (currently, only local authentication module can validate the key).","title":"key"},{"location":"thehive/installation-and-configuration/configuration/authentication/#basic","text":"Authenticates HTTP requests using the login and password provided in authorization header using basic authentication format (Base64). Password is checked from the local user database. Parameter Type Description realm string name of the realm. Without this parameter, the browser doesn't ask to authenticate","title":"basic"},{"location":"thehive/installation-and-configuration/configuration/authentication/#header","text":"Authenticates HTTP requests using a HTTP header containing the user login. This is used to delegate authentication in a reverse proxy. This module accepts the configuration: Parameter Type Description userHeader string the name of the header that contain the user login","title":"header"},{"location":"thehive/installation-and-configuration/configuration/authentication/#ad","text":"Use Microsoft ActiveDirectory to authenticate the user. The configuration is: Parameter Type Description hosts list of string the addresses of the domain controllers. If missing, the dnsDomain is used winDomain string the Windows domain name ( MYDOMAIN ) dnsDomain string the Windows domain name in DNS format ( mydomain.local ) useSSL boolean indicate if SSL must be used to connect to domain controller. The global trust store of the JVM is used to validate remote certificate ( JAVA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/truststore.jks\" ) Example auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : ad , hosts : [ \"dc.mydomain.local\" ], dnsDomain : \"mydomain.local\" , winDomain : \"MYDOMAIN\" , } ] }","title":"ad"},{"location":"thehive/installation-and-configuration/configuration/authentication/#ldap","text":"Use LDAP directory server to authenticate the user. The configuration is: Parameter Type Description hosts list of string the addresses of the LDAP servers bindDN string DN of the service account in LDAP. This account is used to search the user bindPW string password of the service account baseDN string DN where the users are located in filter string filter used to search the user. \"{0}\" is replaced by the user login. A valid filter is: (&(uid={0})(objectClass=posixAccount)) useSSL boolean indicate if SSL must be used to connect to LDAP server. The global trust store of the JVM is used to validate remote certificate ( JAVA_OPTS=\"-Djavax.net.ssl.trustStore=/path/to/truststore.jks\" ) Example auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : ldap hosts : [ ldap1.mydomain.local , ldap2.mydomain.local ] bindDN : \"cn=thehive,ou=services,dc=mydomain,dc=local\" bindPW : \"SuperSecretPassword\" baseDN : \"ou=users,dc=mydomain,dc=local\" filter : \"(cn={0})\" useSSL : true } ] }","title":"ldap"},{"location":"thehive/installation-and-configuration/configuration/authentication/#oauth2","text":"Authenticate the user using an external OAuth2 authenticator server. It accepts the following configuration parameters: Parameter Type Description clientId string client ID in the OAuth2 server clientSecret string client secret in the OAuth2 server redirectUri string the url of TheHive AOuth2 page ( xxx/api/ssoLogin ) responseType string type of the response. Currently only \"code\" is accepted grantType string type of the grant. Currently only \"authorization_code\" is accepted authorizationUrl string the url of the OAuth2 server tokenUrl string the token url of the OAuth2 server userUrl string the url to get user information in OAuth2 server scope list of string list of scope userIdField string the field that contains the id of the user in user info organisationField string (optional) the field that contains the organisation name in user info defaultOrganisation string (optional) the default organisation used to login if not present on user info authorizationHeader string prefix of the authorization header to get user info: Bearer, token, ... Example Keycloak ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : \"code\" grantType : \"authorization_code\" authorizationUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/auth\" authorizationHeader : \"Bearer\" tokenUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/token\" userUrl : \"http://KEYCLOAK/auth/realms/TENANT/protocol/openid-connect/userinfo\" scope : [ \"openid\" , \"email\" ] userIdField : \"email\" } ] } Okta ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : \"code\" grantType : \"authorization_code\" authorizationUrl : \"https://OKTA/oauth2/v1/authorize\" authorizationHeader : \"Bearer\" tokenUrl : \"http://OKTA/oauth2/v1/token\" userUrl : \"http://OKTA/oauth2/v1/userinfo\" scope : [ \"openid\" , \"email\" ] userIdField : \"email\" } ] } Github ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://github.com/login/oauth/authorize\" authorizationHeader : \"token\" tokenUrl : \"https://github.com/login/oauth/access_token\" userUrl : \"https://api.github.com/user\" scope : [ \"user\" ] userIdField : \"email\" #userOrganisation : \"\" } ] } Note CLIENT_ID and CLIENT_SECRET are created in the OAuth Apps section at https://github.com/settings/developers . this configuration requires that users set the Public email in their Public Profile on https://github.com/settings/profile . Microsoft 365 ## Authentication auth { providers : [ { name : session } { name : basic , realm : thehive } { name : local } { name : key } { name : oauth2 clientId : \"CLIENT_ID\" clientSecret : \"CLIENT_SECRET\" redirectUri : \"http://THEHIVE_URL/api/ssoLogin\" responseType : code grantType : \"authorization_code\" authorizationUrl : \"https://login.microsoftonline.com/TENANT/oauth2/v2.0/authorize\" authorizationHeader : \"Bearer \" tokenUrl : \"https://login.microsoftonline.com/TENANT/oauth2/v2.0/token\" userUrl : \"https://graph.microsoft.com/v1.0/me\" scope : [ \"User.Read\" ] userIdField : \"mail\" #userOrganisation : \"\" ## if not existing in the response, use default organisation } ] } Note To create CLIENT_ID , CLIENT_SECRET and TENANT , register a new app at https://aad.portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/RegisteredApps . Google Note CLIENT_ID and CLIENT_SECRET are created in the _APIs & Services_ > _Credentials_ section of the GCP Console Instructions on how to create Oauth2 credentials at https://support.google.com/cloud/answer/6158849 For the latest reference for Google auth URLs please check Google's .well-known/openid-configuration","title":"oauth2"},{"location":"thehive/installation-and-configuration/configuration/authentication/#user-autocreation","text":"To allow users to login without previously creating them, you can enable autocreation by adding user.autoCreateOnSso=true to the top level of your configuration. Example user.autoCreateOnSso : true user.profileFieldName : profile user.organisationFieldName : organisation user.defaults.profile : analyst user.defaults.organisation : cert","title":"User autocreation"},{"location":"thehive/installation-and-configuration/configuration/authentication/#multi-factor-authentication","text":"Multi-Factor Authentication is enabled by default. This means users can configure their MFA through their User Settings page (top-Right corner button > Settings). User administrators can: See which users have activated MFA Reset MFA settings of any user This feature can be **disabled** by setting a config property to false : Example auth.multifactor.enabled = false","title":"Multi-Factor Authentication"},{"location":"thehive/installation-and-configuration/configuration/connectors-cortex/","text":"TheHive connector: Cortex # Enable the connector # The Cortex connector module needs to be enabled to allow TheHive work with Cortex. Enable the module with this line of configuration: play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule Configure one connection # TheHive is able to connect more than one Cortex organisation. Several parameters can be configured for one server : Parameter Type Description name string name given to the Cortex instance (eg: Cortex-Internal ) url string url to connect to the Cortex instance auth dict method used to authenticate on the server ( bearer if using API keys) wsConfig dict network configuration dedicated to Play Framework for SSL and proxy refreshDelay duration frequency of job updates checks (default: 5 seconds ) maxRetryOnError integer maximum number of successive errors before give up (default: 3 ) statusCheckInterval duration check remote Cortex status time interval (default: 1 minute ) includedTheHiveOrganisations list of string list of TheHive organisations which can use this Cortex server (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this Cortex server (default: None ( [] ) ) This configuration has to be added to TheHive conf/application.conf file. ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = local url = \"http://localhost:9001\" auth { type = \"bearer\" key = \"[REDACTED]\" } wsConfig {} includedTheHiveOrganisations = [\"*\"] excludedTheHiveOrganisations = [] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 1 minute } Note By default, adding a Cortex server in TheHive configuration make it available for all organisations added on the instance. Example 1 server Configuration with one Cortex connection: ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = Cortex url = \"http://cortex1:9001\" auth { type = \"bearer\" key = \"tkjjyfsdgrKuPttaaasdDWSEzClKuPt\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } includedTheHiveOrganisations = [\"ORG1\", \"ORG2\"] excludedTheHiveOrganisations = [] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 1 minute } more servers Configuration with 2 Cortex connections: ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = Cortex1 url = \"http://cortex1:9001\" auth { type = \"bearer\" key = \"tkjjyfsdgrKuPttaaasdDWSEzClKuPt\" } wsConfig {} includedTheHiveOrganisations = [\"ORG1\", \"ORG2\"] excludedTheHiveOrganisations = [] } { name = Cortex2 url = \"http://cortex2:9001\" auth { type = \"bearer\" key = \"lSDkjDGGGHtipueroBHOroNJKLbpi\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } includedTheHiveOrganisations = [\"ORG2\", \"ORG3\"] excludedTheHiveOrganisations = [\"ORG1\"] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 5 minutes }","title":"Cortex connector"},{"location":"thehive/installation-and-configuration/configuration/connectors-cortex/#thehive-connector-cortex","text":"","title":"TheHive connector: Cortex"},{"location":"thehive/installation-and-configuration/configuration/connectors-cortex/#enable-the-connector","text":"The Cortex connector module needs to be enabled to allow TheHive work with Cortex. Enable the module with this line of configuration: play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule","title":"Enable the connector"},{"location":"thehive/installation-and-configuration/configuration/connectors-cortex/#configure-one-connection","text":"TheHive is able to connect more than one Cortex organisation. Several parameters can be configured for one server : Parameter Type Description name string name given to the Cortex instance (eg: Cortex-Internal ) url string url to connect to the Cortex instance auth dict method used to authenticate on the server ( bearer if using API keys) wsConfig dict network configuration dedicated to Play Framework for SSL and proxy refreshDelay duration frequency of job updates checks (default: 5 seconds ) maxRetryOnError integer maximum number of successive errors before give up (default: 3 ) statusCheckInterval duration check remote Cortex status time interval (default: 1 minute ) includedTheHiveOrganisations list of string list of TheHive organisations which can use this Cortex server (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this Cortex server (default: None ( [] ) ) This configuration has to be added to TheHive conf/application.conf file. ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = local url = \"http://localhost:9001\" auth { type = \"bearer\" key = \"[REDACTED]\" } wsConfig {} includedTheHiveOrganisations = [\"*\"] excludedTheHiveOrganisations = [] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 1 minute } Note By default, adding a Cortex server in TheHive configuration make it available for all organisations added on the instance. Example 1 server Configuration with one Cortex connection: ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = Cortex url = \"http://cortex1:9001\" auth { type = \"bearer\" key = \"tkjjyfsdgrKuPttaaasdDWSEzClKuPt\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } includedTheHiveOrganisations = [\"ORG1\", \"ORG2\"] excludedTheHiveOrganisations = [] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 1 minute } more servers Configuration with 2 Cortex connections: ## Cortex configuration play.modules.enabled += org.thp.thehive.connector.cortex.CortexModule cortex { servers = [ { name = Cortex1 url = \"http://cortex1:9001\" auth { type = \"bearer\" key = \"tkjjyfsdgrKuPttaaasdDWSEzClKuPt\" } wsConfig {} includedTheHiveOrganisations = [\"ORG1\", \"ORG2\"] excludedTheHiveOrganisations = [] } { name = Cortex2 url = \"http://cortex2:9001\" auth { type = \"bearer\" key = \"lSDkjDGGGHtipueroBHOroNJKLbpi\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } includedTheHiveOrganisations = [\"ORG2\", \"ORG3\"] excludedTheHiveOrganisations = [\"ORG1\"] } ] refreshDelay = 5 seconds maxRetryOnError = 3 statusCheckInterval = 5 minutes }","title":"Configure one connection"},{"location":"thehive/installation-and-configuration/configuration/connectors-misp/","text":"TheHive connector: MISP # Enable MISP connector # The MISP connector module needs to be enabled to allow TheHive to interact with MISP. Enable the module with this line of configuration: play.modules.enabled += org.thp.thehive.connector.misp.MispModule Configuration # TheHive is able to connect to more than one MISP server for pulling, pushing or both. Several parameters can be configured for one server : Parameter Type Description interval duration delay between to pull/push events to remote MISP servers. This is a common parameter for all configured server name string name given to the MISP instance (eg: MISP-MyOrg ) url string url to connect to the MISP instance auth dict method used to authenticate on the server ( key if using API keys) purpose string define the purpose of the server MISP: ImportOnly , ExportOnly or ImportAndExport (default: ImportAndExport ) wsConfig dict network configuration dedicated to Play Framework for SSL and proxy caseTemplate string case template used by default in TheHive to import events as Alerts tags list of string tags to be added to events imported as Alerts in TheHive exportCaseTags boolean indicate if the tags of the case should be exported to MISP event (default: false) Optional parameters can be added to filter out some events coming into TheHive: Parameter Type Description exclusion.organisations list of string list of MISP organisation from which event will not be imported exclusion.tags list of string don't import MISP events which have one of these tags whitelist.organisations list of string import only events from these MISP organisations whitelist.tags list of string import only MISP events which have one of these tags max-age duration maximum age of the last publish date of event to be imported in TheHive includedTheHiveOrganisations list of string list of TheHive organisations which can use this MISP server (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this MISP server (default: None ( [] ) ) Additionally, some organisations or tags from MISP can be defined to exclude events. Note By default, adding a MISP server in TheHive configuration make it available for all organisations added on the instance. This configuration has to be added to TheHive conf/application.conf file: ## MISP configuration # More information at https://github.com/TheHive-Project/TheHiveDocs/TheHive4/Administration/Connectors.md # Enable MISP connector play.modules.enabled += org.thp.thehive.connector.misp.MispModule misp { interval : 1 hour servers : [ { name = \"local\" url = \"http : //localhost/\" auth { type = key key = \"***\" } wsConfig {} caseTemplate = \"<Template_Name_goes_here>\" tags = [ \"misp-server-id\" ] max-age = 7 days exclusion { organisations = [ \"bad organisation\" , \"other orga\" ] tags = [ \"tag1\" , \"tag2\" ] } whitelist { tags = [ \"tag1\" , \"tag2\" ] } includedTheHiveOrganisations = [ \"*\" ] excludedTheHiveOrganisations = [] } ] } Example Connection with 1 MISP server: ## MISP configuration # More information at https://github.com/TheHive-Project/TheHiveDocs/TheHive4/Administration/Connectors.md # Enable MISP connector play.modules.enabled += org.thp.thehive.connector.misp.MispModule misp { interval : 1 hour servers : [ { name = \"MISP Server\" url = \"https : //misp.server\" auth { type = key key = \"XhtropikjthiuGIORWUHHlLhlfeerljta\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } tags = [ \"tag1\" , \"tag2\" , \"tag3\" ] caseTemplate = \"misp\" includedTheHiveOrganisations = [ \"ORG1\" , \"ORG2\" ] } ] }","title":"MISP connector"},{"location":"thehive/installation-and-configuration/configuration/connectors-misp/#thehive-connector-misp","text":"","title":"TheHive connector: MISP"},{"location":"thehive/installation-and-configuration/configuration/connectors-misp/#enable-misp-connector","text":"The MISP connector module needs to be enabled to allow TheHive to interact with MISP. Enable the module with this line of configuration: play.modules.enabled += org.thp.thehive.connector.misp.MispModule","title":"Enable MISP connector"},{"location":"thehive/installation-and-configuration/configuration/connectors-misp/#configuration","text":"TheHive is able to connect to more than one MISP server for pulling, pushing or both. Several parameters can be configured for one server : Parameter Type Description interval duration delay between to pull/push events to remote MISP servers. This is a common parameter for all configured server name string name given to the MISP instance (eg: MISP-MyOrg ) url string url to connect to the MISP instance auth dict method used to authenticate on the server ( key if using API keys) purpose string define the purpose of the server MISP: ImportOnly , ExportOnly or ImportAndExport (default: ImportAndExport ) wsConfig dict network configuration dedicated to Play Framework for SSL and proxy caseTemplate string case template used by default in TheHive to import events as Alerts tags list of string tags to be added to events imported as Alerts in TheHive exportCaseTags boolean indicate if the tags of the case should be exported to MISP event (default: false) Optional parameters can be added to filter out some events coming into TheHive: Parameter Type Description exclusion.organisations list of string list of MISP organisation from which event will not be imported exclusion.tags list of string don't import MISP events which have one of these tags whitelist.organisations list of string import only events from these MISP organisations whitelist.tags list of string import only MISP events which have one of these tags max-age duration maximum age of the last publish date of event to be imported in TheHive includedTheHiveOrganisations list of string list of TheHive organisations which can use this MISP server (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this MISP server (default: None ( [] ) ) Additionally, some organisations or tags from MISP can be defined to exclude events. Note By default, adding a MISP server in TheHive configuration make it available for all organisations added on the instance. This configuration has to be added to TheHive conf/application.conf file: ## MISP configuration # More information at https://github.com/TheHive-Project/TheHiveDocs/TheHive4/Administration/Connectors.md # Enable MISP connector play.modules.enabled += org.thp.thehive.connector.misp.MispModule misp { interval : 1 hour servers : [ { name = \"local\" url = \"http : //localhost/\" auth { type = key key = \"***\" } wsConfig {} caseTemplate = \"<Template_Name_goes_here>\" tags = [ \"misp-server-id\" ] max-age = 7 days exclusion { organisations = [ \"bad organisation\" , \"other orga\" ] tags = [ \"tag1\" , \"tag2\" ] } whitelist { tags = [ \"tag1\" , \"tag2\" ] } includedTheHiveOrganisations = [ \"*\" ] excludedTheHiveOrganisations = [] } ] } Example Connection with 1 MISP server: ## MISP configuration # More information at https://github.com/TheHive-Project/TheHiveDocs/TheHive4/Administration/Connectors.md # Enable MISP connector play.modules.enabled += org.thp.thehive.connector.misp.MispModule misp { interval : 1 hour servers : [ { name = \"MISP Server\" url = \"https : //misp.server\" auth { type = key key = \"XhtropikjthiuGIORWUHHlLhlfeerljta\" } wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } tags = [ \"tag1\" , \"tag2\" , \"tag3\" ] caseTemplate = \"misp\" includedTheHiveOrganisations = [ \"ORG1\" , \"ORG2\" ] } ] }","title":"Configuration"},{"location":"thehive/installation-and-configuration/configuration/database/","text":"Database and index configuration # TheHive can be configured to connect to local Berkeley database or Cassandra database. Tip Using Cassandra is strongly recommended for production use while Berkeley DB can be prefered for testing and training purpose. Starting with TheHive 4.1.0, indexes are managed by a dedicated engine. According to the setup, the instance can use: A local engine, Lucene driven by TheHive A centralised engine, Elasticsearch . Configuation # A typical database configuration for TheHive looks like this: ## Database configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"IP_ADDRESS\"] cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : lucene directory: /path/to/index/folder } } } } List of possible parameters # Parameter Type Description provider string provider name. Default: janusgraph storage dict storage configuration storage.backend string storage type. Can be cql or berkeleyje storage.hostname list of string list of IP addresses or hostnames when using cql backend storage.directory string local path for data when using berkeleyje backend storage.username string account username with cql backend if Cassandra auth is configured storage.password string account password with cql backend if Cassandra auth is configured storage.port integer port number with cql backend ( 9042 by default). Change this if using an alternate port or a dedicated port number when using SSL with Cassandra storage.cql dict configuration for cql backend if used storage.cql.cluster-name string name of the cluster name used in the configuration of Apache Cassandra storage.cql.keyspace string Keyspace name used to store TheHive data in Apache Cassandra storage.cql.ssl.enabled boolean false by default. set it to true if SSL is used with Cassandra storage.cql.ssl.truststore.location string path the the truststore. Specify it when using SSL with Cassandra storage.cql.ssl.password string password to access the truststore storage.cql.ssl.client-authentication-enabled boolean Enables use of a client key to authenticate with Cassandra storage.cql.ssl.keystore.location string path the the keystore. Specify it when using SSL and client auth. with Cassandra storage.cql.ssl.keystore.keypassword string password to access the key in the keystore storage.cql.ssl.truststore.storepassword string password the access the keystore index.search dict configuration for indexes index.search.backend string index engine. Default: lucene provided with TheHive. Can also be elasticsearch index.search.directory string path to folder where indexes should be stored, when using lucene engine index.search.hostname list of string list of IP addresses or hostnames when using elasticsearch engine index.search.index-name string name of index, when using elasticseach engine index.search.elasticsearch.http.auth.type: basic string basic is the only possible value index.search.elasticsearch.http.auth.basic.username string Username account on Elasticsearch index.search.elasticsearch.http.auth.basic.password string Password of the account on Elasticsearch index.search.elasticsearch.ssl.enabled boolean Enable SSL true/false index.search.elasticsearch.ssl.truststore.location string Location of the truststore index.search.elasticsearch.ssl.truststore.password string Password of the truststore index.search.elasticsearch.ssl.keystore.location string Location of the keystore for client authentication index.search.elasticsearch.ssl.keystore.storepassword string Password of the keystore index.search.elasticsearch.ssl.keystore.keypassword string Password of the client certificate index.search.elasticsearch.ssl.disable-hostname-verification boolean Disable SSL verification true/false index.search.elasticsearch.ssl.allow-self-signed-certificates boolean Allow self signe certificates true/false Warning Using Elasticsearch to manage indexes is required if you are setting up TheHive as a cluster. The initial start , or first start after configuring indexes might take some time if the database contains a large amount of data. This time is due to the indexes creation More information on configuration for Elasticsearch connection: https://docs.janusgraph.org/index-backend/elasticsearch/ . Use cases # Database and index engine can be different, depending on the use case and target setup: Example Testing server For such use cases, local database and indexes are adequate: Create a dedicated folder for data and for indexes. These folders should belong to the user thehive:thehive . mkdir /opt/thp/thehive/ { data, index } chown -R thehive:thehive /opt/thp/thehive/ { data, index } Configure TheHive accordingly: ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: berkeleyje directory: /opt/thp/thehive/database } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Standalone server with Cassandra Install a Cassandra server locally Create a dedicated folder for indexes. This folder should belong to the user thehive:thehive mkdir /opt/thp/thehive/index chown -R thehive:thehive /opt/thp/thehive/index Configure TheHive accordingly ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"127.0.0.1\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Cluster with Cassandra & Elasticsearch Install a cluster of Cassandra servers Get access to an Elasticsearch server Configure TheHive accordingly ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"10.1.2.1\", \"10.1.2.2\", \"10.1.2.3\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : elasticsearch hostname : [\"10.1.2.5\"] index-name : thehive elasticsearch { http { auth { type: basic basic { username: httpuser password: httppassword } } } ssl { enabled: true truststore { location: /path/to/your/truststore.jks password: truststorepwd } } } } } } } Warning In this configuration, all TheHive nodes should have the same configuration. Elasticsearch configuration should use the default value for script.allowed_types , or contain the following configuration line: script.allowed_types : inline,stored","title":"Database & indexes"},{"location":"thehive/installation-and-configuration/configuration/database/#database-and-index-configuration","text":"TheHive can be configured to connect to local Berkeley database or Cassandra database. Tip Using Cassandra is strongly recommended for production use while Berkeley DB can be prefered for testing and training purpose. Starting with TheHive 4.1.0, indexes are managed by a dedicated engine. According to the setup, the instance can use: A local engine, Lucene driven by TheHive A centralised engine, Elasticsearch .","title":"Database and index configuration"},{"location":"thehive/installation-and-configuration/configuration/database/#configuation","text":"A typical database configuration for TheHive looks like this: ## Database configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"IP_ADDRESS\"] cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : lucene directory: /path/to/index/folder } } } }","title":"Configuation"},{"location":"thehive/installation-and-configuration/configuration/database/#list-of-possible-parameters","text":"Parameter Type Description provider string provider name. Default: janusgraph storage dict storage configuration storage.backend string storage type. Can be cql or berkeleyje storage.hostname list of string list of IP addresses or hostnames when using cql backend storage.directory string local path for data when using berkeleyje backend storage.username string account username with cql backend if Cassandra auth is configured storage.password string account password with cql backend if Cassandra auth is configured storage.port integer port number with cql backend ( 9042 by default). Change this if using an alternate port or a dedicated port number when using SSL with Cassandra storage.cql dict configuration for cql backend if used storage.cql.cluster-name string name of the cluster name used in the configuration of Apache Cassandra storage.cql.keyspace string Keyspace name used to store TheHive data in Apache Cassandra storage.cql.ssl.enabled boolean false by default. set it to true if SSL is used with Cassandra storage.cql.ssl.truststore.location string path the the truststore. Specify it when using SSL with Cassandra storage.cql.ssl.password string password to access the truststore storage.cql.ssl.client-authentication-enabled boolean Enables use of a client key to authenticate with Cassandra storage.cql.ssl.keystore.location string path the the keystore. Specify it when using SSL and client auth. with Cassandra storage.cql.ssl.keystore.keypassword string password to access the key in the keystore storage.cql.ssl.truststore.storepassword string password the access the keystore index.search dict configuration for indexes index.search.backend string index engine. Default: lucene provided with TheHive. Can also be elasticsearch index.search.directory string path to folder where indexes should be stored, when using lucene engine index.search.hostname list of string list of IP addresses or hostnames when using elasticsearch engine index.search.index-name string name of index, when using elasticseach engine index.search.elasticsearch.http.auth.type: basic string basic is the only possible value index.search.elasticsearch.http.auth.basic.username string Username account on Elasticsearch index.search.elasticsearch.http.auth.basic.password string Password of the account on Elasticsearch index.search.elasticsearch.ssl.enabled boolean Enable SSL true/false index.search.elasticsearch.ssl.truststore.location string Location of the truststore index.search.elasticsearch.ssl.truststore.password string Password of the truststore index.search.elasticsearch.ssl.keystore.location string Location of the keystore for client authentication index.search.elasticsearch.ssl.keystore.storepassword string Password of the keystore index.search.elasticsearch.ssl.keystore.keypassword string Password of the client certificate index.search.elasticsearch.ssl.disable-hostname-verification boolean Disable SSL verification true/false index.search.elasticsearch.ssl.allow-self-signed-certificates boolean Allow self signe certificates true/false Warning Using Elasticsearch to manage indexes is required if you are setting up TheHive as a cluster. The initial start , or first start after configuring indexes might take some time if the database contains a large amount of data. This time is due to the indexes creation More information on configuration for Elasticsearch connection: https://docs.janusgraph.org/index-backend/elasticsearch/ .","title":"List of possible parameters"},{"location":"thehive/installation-and-configuration/configuration/database/#use-cases","text":"Database and index engine can be different, depending on the use case and target setup: Example Testing server For such use cases, local database and indexes are adequate: Create a dedicated folder for data and for indexes. These folders should belong to the user thehive:thehive . mkdir /opt/thp/thehive/ { data, index } chown -R thehive:thehive /opt/thp/thehive/ { data, index } Configure TheHive accordingly: ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: berkeleyje directory: /opt/thp/thehive/database } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Standalone server with Cassandra Install a Cassandra server locally Create a dedicated folder for indexes. This folder should belong to the user thehive:thehive mkdir /opt/thp/thehive/index chown -R thehive:thehive /opt/thp/thehive/index Configure TheHive accordingly ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"127.0.0.1\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Cluster with Cassandra & Elasticsearch Install a cluster of Cassandra servers Get access to an Elasticsearch server Configure TheHive accordingly ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"10.1.2.1\", \"10.1.2.2\", \"10.1.2.3\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : elasticsearch hostname : [\"10.1.2.5\"] index-name : thehive elasticsearch { http { auth { type: basic basic { username: httpuser password: httppassword } } } ssl { enabled: true truststore { location: /path/to/your/truststore.jks password: truststorepwd } } } } } } } Warning In this configuration, all TheHive nodes should have the same configuration. Elasticsearch configuration should use the default value for script.allowed_types , or contain the following configuration line: script.allowed_types : inline,stored","title":"Use cases"},{"location":"thehive/installation-and-configuration/configuration/file-storage/","text":"File storage configuration # TheHive can be configured to use local or distributed filesystems. Example Local or NFS Create dedicated folder ; it should belong to user and group thehive:thehive . mkdir /opt/thp/thehive/files chown thehive:thehive /opt/thp/thehive/files Configure TheHive accordingly: ## Attachment storage configuration storage { ## Local filesystem provider : localfs localfs { location : /opt/thp/thehive/files } } Min.IO Install a Min.IO cluster Configure each node of TheHive accordingly: ## Attachment storage configuration storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://10.1.2.4:9100\" accessKey = \"thehive\" secretKey = \"minio_password\" region = \"us-east-1\" } } alpakka.s3.path-style-access = force us-east-1 is the default region if none has been specified in MinIO configuration. In this case, this parameter is optional. Apache Hadoop Install an Apache Hadoop server Configure each node of TheHive accordingly ( /etc/thehive/application.conf ): ## Attachment storage configuration ## Hadoop filesystem (HDFS) provider : hdfs hdfs { root : \"hdfs://10.1.2.4:10000\" # namenode server hostname location : \"/thehive\" # location inside HDFS username : thehive # file owner } }","title":"File Storage"},{"location":"thehive/installation-and-configuration/configuration/file-storage/#file-storage-configuration","text":"TheHive can be configured to use local or distributed filesystems. Example Local or NFS Create dedicated folder ; it should belong to user and group thehive:thehive . mkdir /opt/thp/thehive/files chown thehive:thehive /opt/thp/thehive/files Configure TheHive accordingly: ## Attachment storage configuration storage { ## Local filesystem provider : localfs localfs { location : /opt/thp/thehive/files } } Min.IO Install a Min.IO cluster Configure each node of TheHive accordingly: ## Attachment storage configuration storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://10.1.2.4:9100\" accessKey = \"thehive\" secretKey = \"minio_password\" region = \"us-east-1\" } } alpakka.s3.path-style-access = force us-east-1 is the default region if none has been specified in MinIO configuration. In this case, this parameter is optional. Apache Hadoop Install an Apache Hadoop server Configure each node of TheHive accordingly ( /etc/thehive/application.conf ): ## Attachment storage configuration ## Hadoop filesystem (HDFS) provider : hdfs hdfs { root : \"hdfs://10.1.2.4:10000\" # namenode server hostname location : \"/thehive\" # location inside HDFS username : thehive # file owner } }","title":"File storage configuration"},{"location":"thehive/installation-and-configuration/configuration/logs/","text":"","title":"Logs"},{"location":"thehive/installation-and-configuration/configuration/manage-configuration/","text":"Manage configuration files # TheHive uses HOCON as configuration file format. This format gives enough flexibility to structure and organise the configuration of TheHive. TheHive is delivered with following files, in the folder /etc/thehive : logback.xml containing the log policy secret.conf containing a secret key used to create sessions. This key should be unique per instance (in the case of a cluster, this key should be the same for all nodes of this cluster) application.conf HOCON file format let you organise the configuration to have separate files for each purpose. It is the possible to create a /etc/thehive/application.conf.d folder and have several files inside that will be included in the main file /etc/thehive/application.conf . At the end, the following configuration structure is possible: /etc/thehive |-- application.conf |-- application.conf.d | |-- secret.conf | |-- service.conf | |-- ssl.conf | |-- proxy.conf | |-- database.conf | |-- storage.conf | |-- cluster.conf | |-- authentication.conf | |-- cortex.conf | |-- misp.conf | `-- webhooks.conf `-- logback.xml And the content of /etc/thehive/application.conf : ### ## Documentation is available at https://docs.thehive-project.org ### ## Include Play secret key # More information on secret key at https://www.playframework.com/documentation/2.8.x/ApplicationSecret include \"/etc/thehive/application.conf.d/secret.conf\" ## Service include \"/etc/thehive/application.conf.d/service.conf\" ## SSL settings include \"/etc/thehive/application.conf.d/ssl.conf\" ## PROXY settings include \"/etc/thehive/application.conf.d/proxy.conf\" ## Database include \"/etc/thehive/application.conf.d/database.conf\" ## Storage include \"/etc/thehive/application.conf.d/storage.conf\" ## Cluster include \"/etc/thehive/application.conf.d/cluster.conf\" ## Authentication include \"/etc/thehive/application.conf.d/authentication.conf\" ## Cortex include \"/etc/thehive/application.conf.d/cortex.conf\" ## MISP include \"/etc/thehive/application.conf.d/misp.conf\" ## Webhooks include \"/etc/thehive/application.conf.d/webhooks.conf\"","title":"Manage Configuration"},{"location":"thehive/installation-and-configuration/configuration/manage-configuration/#manage-configuration-files","text":"TheHive uses HOCON as configuration file format. This format gives enough flexibility to structure and organise the configuration of TheHive. TheHive is delivered with following files, in the folder /etc/thehive : logback.xml containing the log policy secret.conf containing a secret key used to create sessions. This key should be unique per instance (in the case of a cluster, this key should be the same for all nodes of this cluster) application.conf HOCON file format let you organise the configuration to have separate files for each purpose. It is the possible to create a /etc/thehive/application.conf.d folder and have several files inside that will be included in the main file /etc/thehive/application.conf . At the end, the following configuration structure is possible: /etc/thehive |-- application.conf |-- application.conf.d | |-- secret.conf | |-- service.conf | |-- ssl.conf | |-- proxy.conf | |-- database.conf | |-- storage.conf | |-- cluster.conf | |-- authentication.conf | |-- cortex.conf | |-- misp.conf | `-- webhooks.conf `-- logback.xml And the content of /etc/thehive/application.conf : ### ## Documentation is available at https://docs.thehive-project.org ### ## Include Play secret key # More information on secret key at https://www.playframework.com/documentation/2.8.x/ApplicationSecret include \"/etc/thehive/application.conf.d/secret.conf\" ## Service include \"/etc/thehive/application.conf.d/service.conf\" ## SSL settings include \"/etc/thehive/application.conf.d/ssl.conf\" ## PROXY settings include \"/etc/thehive/application.conf.d/proxy.conf\" ## Database include \"/etc/thehive/application.conf.d/database.conf\" ## Storage include \"/etc/thehive/application.conf.d/storage.conf\" ## Cluster include \"/etc/thehive/application.conf.d/cluster.conf\" ## Authentication include \"/etc/thehive/application.conf.d/authentication.conf\" ## Cortex include \"/etc/thehive/application.conf.d/cortex.conf\" ## MISP include \"/etc/thehive/application.conf.d/misp.conf\" ## Webhooks include \"/etc/thehive/application.conf.d/webhooks.conf\"","title":"Manage configuration files"},{"location":"thehive/installation-and-configuration/configuration/proxy/","text":"Proxy settings # Proxy for connectors # Refer to Cortex or MISP configuration to setup specific proxy configuration for these remote services. Proxy for global application # Proxy can be used. By default, the proxy configured in JVM is used but one can configured specific configurations for each HTTP client. Parameter Type Description wsConfig.proxy.host string The hostname of the proxy server wsConfig.proxy.port integer The port of the proxy server wsConfig.proxy.protocol string The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified wsConfig.proxy.user string The username of the credentials for the proxy server wsConfig.proxy.password string The password for the credentials for the proxy server wsConfig.proxy.ntlmDomain string The NTLM domain wsConfig.proxy.encoding string The realm's charset wsConfig.proxy.nonProxyHosts list The list of hosts on which proxy must not be used","title":"Proxy"},{"location":"thehive/installation-and-configuration/configuration/proxy/#proxy-settings","text":"","title":"Proxy settings"},{"location":"thehive/installation-and-configuration/configuration/proxy/#proxy-for-connectors","text":"Refer to Cortex or MISP configuration to setup specific proxy configuration for these remote services.","title":"Proxy for connectors"},{"location":"thehive/installation-and-configuration/configuration/proxy/#proxy-for-global-application","text":"Proxy can be used. By default, the proxy configured in JVM is used but one can configured specific configurations for each HTTP client. Parameter Type Description wsConfig.proxy.host string The hostname of the proxy server wsConfig.proxy.port integer The port of the proxy server wsConfig.proxy.protocol string The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified wsConfig.proxy.user string The username of the credentials for the proxy server wsConfig.proxy.password string The password for the credentials for the proxy server wsConfig.proxy.ntlmDomain string The NTLM domain wsConfig.proxy.encoding string The realm's charset wsConfig.proxy.nonProxyHosts list The list of hosts on which proxy must not be used","title":"Proxy for global application"},{"location":"thehive/installation-and-configuration/configuration/secret/","text":"secret.conf file # This file contains a secret that is used to define cookies used to manage the users session. As a result, one instance of TheHive should use a unique secret key. Example ## Play secret key play.http.secret.key=\"dgngu325mbnbc39cxas4l5kb24503836y2vsvsg465989fbsvop9d09ds6df6\" Warning In the case of a cluster of TheHive nodes, all nodes should have the same secret.conf file with the same secret key. The secret is used to generate user sessions.","title":"Secret key"},{"location":"thehive/installation-and-configuration/configuration/secret/#secretconf-file","text":"This file contains a secret that is used to define cookies used to manage the users session. As a result, one instance of TheHive should use a unique secret key. Example ## Play secret key play.http.secret.key=\"dgngu325mbnbc39cxas4l5kb24503836y2vsvsg465989fbsvop9d09ds6df6\" Warning In the case of a cluster of TheHive nodes, all nodes should have the same secret.conf file with the same secret key. The secret is used to generate user sessions.","title":"secret.conf file"},{"location":"thehive/installation-and-configuration/configuration/service/","text":"Service # Listen address & port # By default the application listens on all interfaces and port 9000 . This is possible to specify listen address and ports with following parameters in the application.conf file: http.address=127.0.0.1 http.port=9000 Context # If you are using a reverse proxy, and you want to specify a location (ex: /thehive ), updating the configuration of TheHive is also required Example play.http.context: \"/thehive\" Specific configuration for streams # If you are using a reverse proxy like Nginx, you might receive error popups with the following message: StreamSrv 504 Gateway Time-Out . You need to change default setting for long polling refresh, Set stream.longPolling.refresh accordingly. Example stream.longPolling.refresh: 45 seconds Manage content length # Content length of text and files managed by the application are limited by default. Before TheHive v4.1.1 , the Play framework sets the HTTP body size limit to 100KB by default for textual content (json, xml, text, form data) and 10MB for file uploads. Since TheHive v4.1.1 , these values are set with default parameters: # Max file size play.http.parser.maxDiskBuffer : 128MB # Max textual content length play.http.parser.maxMemoryBuffer : 256kB If you feel that these should be updated, edit /etc/thehive/application.conf file and update these parameters accordingly. Tip if you are using a NGINX reverse proxy in front of TheHive, be aware that it doesn't distinguish between text data and a file upload. So, you should also set the client_max_body_size parameter in your NGINX server configuration to the highest value among the two: file upload and text size defined in TheHive application.conf file.","title":"Service"},{"location":"thehive/installation-and-configuration/configuration/service/#service","text":"","title":"Service"},{"location":"thehive/installation-and-configuration/configuration/service/#listen-address-port","text":"By default the application listens on all interfaces and port 9000 . This is possible to specify listen address and ports with following parameters in the application.conf file: http.address=127.0.0.1 http.port=9000","title":"Listen address &amp; port"},{"location":"thehive/installation-and-configuration/configuration/service/#context","text":"If you are using a reverse proxy, and you want to specify a location (ex: /thehive ), updating the configuration of TheHive is also required Example play.http.context: \"/thehive\"","title":"Context"},{"location":"thehive/installation-and-configuration/configuration/service/#specific-configuration-for-streams","text":"If you are using a reverse proxy like Nginx, you might receive error popups with the following message: StreamSrv 504 Gateway Time-Out . You need to change default setting for long polling refresh, Set stream.longPolling.refresh accordingly. Example stream.longPolling.refresh: 45 seconds","title":"Specific configuration for streams"},{"location":"thehive/installation-and-configuration/configuration/service/#manage-content-length","text":"Content length of text and files managed by the application are limited by default. Before TheHive v4.1.1 , the Play framework sets the HTTP body size limit to 100KB by default for textual content (json, xml, text, form data) and 10MB for file uploads. Since TheHive v4.1.1 , these values are set with default parameters: # Max file size play.http.parser.maxDiskBuffer : 128MB # Max textual content length play.http.parser.maxMemoryBuffer : 256kB If you feel that these should be updated, edit /etc/thehive/application.conf file and update these parameters accordingly. Tip if you are using a NGINX reverse proxy in front of TheHive, be aware that it doesn't distinguish between text data and a file upload. So, you should also set the client_max_body_size parameter in your NGINX server configuration to the highest value among the two: file upload and text size defined in TheHive application.conf file.","title":"Manage content length"},{"location":"thehive/installation-and-configuration/configuration/ssl/","text":"SSL # Server configuration # We recommend using a reverse proxy to manage SSL layer. Connectors # Refer to Cortex or MISP configuration to setup specific SSL configuration of these remote services. Client configuration # SSL configuration might be requis required to connect remote services. Following parameters can be defined: Parameter Type Description wsConfig.ssl.keyManager.stores list Stores client certificates (see #certificate-manager ) wsConfig.ssl.trustManager.stores list Stored custom Certificate Authorities (see #certificate-manager wsConfig.ssl.protocol string Defines a different default protocol (see #protocols ) wsConfig.ssl.enabledProtocols list List of enabled protocols (see #protocols ) wsConfig.ssl.enabledCipherSuites list List of enabled cipher suites (see #ciphers ) wsConfig.ssl.loose.acceptAnyCertificate boolean Accept any certificates true / false Certificate manager # Certificate manager is used to store client certificates and certificate authorities. Custom Certificate Authority # Global configuration # If setting up a custom Certificate Authority (to connect web proxies, remote services ...) is required globally in the application, the better solution consists of installing it on the OS and restarting TheHive. Debian apt-get install -y ca-certificates-java mkdir /usr/share/ca-certificates/extra cp mysctomcert.crt /usr/share/ca-certificates/extra dpkg-reconfigure ca-certificates service thehive restart Use dedicated trust stores # the other way, is to use the trustManager key in TheHive configuration. It is used to establish a secure connection with remote host. Server certificate must be signed by a trusted certificate authority. wsConfig.ssl.trustManager { stores = [ { type: \"JKS\" // JKS or PEM path: \"keystore.jks\" password: \"password1\" } ] } Client certificates # keyManager indicates which certificate HTTP client can use to authenticate itself on remote host (when certificate based authentication is used) wsConfig.ssl.keyManager { stores = [ { type: \"pkcs12\" // JKS or PEM path: \"mycert.p12\" password: \"password1\" } ] } Protocols # If you want to define a different default protocol, you can set it specifically in the client: wsConfig.ssl.protocol = \"TLSv1.2\" If you want to define the list of enabled protocols, you can do so explicitly: wsConfig.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"] Ciphers # Cipher suites can be configured using wsConfig.ssl.enabledCipherSuites : wsConfig.ssl.enabledCipherSuites = [ \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", ] Debugging # To debug the key manager / trust manager, set the following flags: wsConfig.ssl.debug = { ssl = true trustmanager = true keymanager = true sslctx = true handshake = true verbose = true data = true certpath = true }","title":"SSL"},{"location":"thehive/installation-and-configuration/configuration/ssl/#ssl","text":"","title":"SSL"},{"location":"thehive/installation-and-configuration/configuration/ssl/#server-configuration","text":"We recommend using a reverse proxy to manage SSL layer.","title":"Server configuration"},{"location":"thehive/installation-and-configuration/configuration/ssl/#connectors","text":"Refer to Cortex or MISP configuration to setup specific SSL configuration of these remote services.","title":"Connectors"},{"location":"thehive/installation-and-configuration/configuration/ssl/#client-configuration","text":"SSL configuration might be requis required to connect remote services. Following parameters can be defined: Parameter Type Description wsConfig.ssl.keyManager.stores list Stores client certificates (see #certificate-manager ) wsConfig.ssl.trustManager.stores list Stored custom Certificate Authorities (see #certificate-manager wsConfig.ssl.protocol string Defines a different default protocol (see #protocols ) wsConfig.ssl.enabledProtocols list List of enabled protocols (see #protocols ) wsConfig.ssl.enabledCipherSuites list List of enabled cipher suites (see #ciphers ) wsConfig.ssl.loose.acceptAnyCertificate boolean Accept any certificates true / false","title":"Client configuration"},{"location":"thehive/installation-and-configuration/configuration/ssl/#certificate-manager","text":"Certificate manager is used to store client certificates and certificate authorities.","title":"Certificate manager"},{"location":"thehive/installation-and-configuration/configuration/ssl/#custom-certificate-authority","text":"","title":"Custom Certificate Authority"},{"location":"thehive/installation-and-configuration/configuration/ssl/#global-configuration","text":"If setting up a custom Certificate Authority (to connect web proxies, remote services ...) is required globally in the application, the better solution consists of installing it on the OS and restarting TheHive. Debian apt-get install -y ca-certificates-java mkdir /usr/share/ca-certificates/extra cp mysctomcert.crt /usr/share/ca-certificates/extra dpkg-reconfigure ca-certificates service thehive restart","title":"Global configuration"},{"location":"thehive/installation-and-configuration/configuration/ssl/#use-dedicated-trust-stores","text":"the other way, is to use the trustManager key in TheHive configuration. It is used to establish a secure connection with remote host. Server certificate must be signed by a trusted certificate authority. wsConfig.ssl.trustManager { stores = [ { type: \"JKS\" // JKS or PEM path: \"keystore.jks\" password: \"password1\" } ] }","title":"Use dedicated trust stores"},{"location":"thehive/installation-and-configuration/configuration/ssl/#client-certificates","text":"keyManager indicates which certificate HTTP client can use to authenticate itself on remote host (when certificate based authentication is used) wsConfig.ssl.keyManager { stores = [ { type: \"pkcs12\" // JKS or PEM path: \"mycert.p12\" password: \"password1\" } ] }","title":"Client certificates"},{"location":"thehive/installation-and-configuration/configuration/ssl/#protocols","text":"If you want to define a different default protocol, you can set it specifically in the client: wsConfig.ssl.protocol = \"TLSv1.2\" If you want to define the list of enabled protocols, you can do so explicitly: wsConfig.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"]","title":"Protocols"},{"location":"thehive/installation-and-configuration/configuration/ssl/#ciphers","text":"Cipher suites can be configured using wsConfig.ssl.enabledCipherSuites : wsConfig.ssl.enabledCipherSuites = [ \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", ]","title":"Ciphers"},{"location":"thehive/installation-and-configuration/configuration/ssl/#debugging","text":"To debug the key manager / trust manager, set the following flags: wsConfig.ssl.debug = { ssl = true trustmanager = true keymanager = true sslctx = true handshake = true verbose = true data = true certpath = true }","title":"Debugging"},{"location":"thehive/installation-and-configuration/configuration/webhooks/","text":"TheHive webhooks # TheHive can notify external system of modification events (case creation, alert update, task assignment, ...). To use webhooks notifications, 2 steps are required: configure a notification, and activate it. 1. Define webhook endpoints # The configuration can accept following parameters: Parameter Type Description name string the identifier of the endpoint. It is used when the webhook is setup for an organisation version integer defines the format of the message. If version is 0 , TheHive will send messages with the same format as TheHive3. Currently TheHive only supports version 0. wsConfig dict the configuration of HTTP client. It contains proxy, SSL and timeout configuration. auth dict the configuration of authenticationI. It contains type, and additional options. includedTheHiveOrganisations list of string list of TheHive organisations which can use this endpoint (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this endpoint (default: None ( [] ) ) The following section should be added in application.conf : ## Webhook notification notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig : {} auth : { type : \"none\" } includedTheHiveOrganisations : [ \"*\" ] excludedTheHiveOrganisations : [] } ] Use a proxy # Wehbook call can go through a proxy, in which case, Webhooks configuration requires a wsConfig config notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } auth : { type : \"none\" } includedTheHiveOrganisations : [ \"*\" ] excludedTheHiveOrganisations : [] } ] Use an authentication method # Webhook endpoints can be authenticated, in this case, Webhook configuration requires a auth setting. Supported methods are: No Auth (Default) auth : { type : \"none\" } Basic Auth auth : { type : \"basic\" , username : \"foo\" , password : \"bar\" } Beared Auth auth : { type : \"bearer\" , key : \"foobar\" } Key Auth auth : { type : \"key\" , key : \"foobar\" } Warning In 4.1.0 release, the auth config is REQUIRED . Examples # Example ## Webhook notification notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } auth : { type : \"bearer\" , key : \"API_KEY\" } includedTheHiveOrganisations : [ \"ORG1\" , \"ORG2\" ] excludedTheHiveOrganisations : [ \"ORG3\" ] } ] 2. Activate webhooks # This action must be done by an organisation admin (with permission manageConfig ) and requires to run a curl command: read -p 'Enter the URL of TheHive: ' thehive_url read -p 'Enter your login: ' thehive_user read -s -p 'Enter your password: ' thehive_password curl -XPUT -u $thehive_user : $thehive_password -H 'Content-type: application/json' $thehive_url /api/config/organisation/notification -d ' { \"value\": [ { \"delegate\": false, \"trigger\": { \"name\": \"AnyEvent\"}, \"notifier\": { \"name\": \"webhook\", \"endpoint\": \"local\" } } ] }'","title":"Webhooks"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#thehive-webhooks","text":"TheHive can notify external system of modification events (case creation, alert update, task assignment, ...). To use webhooks notifications, 2 steps are required: configure a notification, and activate it.","title":"TheHive webhooks"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#1-define-webhook-endpoints","text":"The configuration can accept following parameters: Parameter Type Description name string the identifier of the endpoint. It is used when the webhook is setup for an organisation version integer defines the format of the message. If version is 0 , TheHive will send messages with the same format as TheHive3. Currently TheHive only supports version 0. wsConfig dict the configuration of HTTP client. It contains proxy, SSL and timeout configuration. auth dict the configuration of authenticationI. It contains type, and additional options. includedTheHiveOrganisations list of string list of TheHive organisations which can use this endpoint (default: all ( [*] ) excludedTheHiveOrganisations list of string list of TheHive organisations which cannot use this endpoint (default: None ( [] ) ) The following section should be added in application.conf : ## Webhook notification notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig : {} auth : { type : \"none\" } includedTheHiveOrganisations : [ \"*\" ] excludedTheHiveOrganisations : [] } ]","title":"1. Define webhook endpoints"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#use-a-proxy","text":"Wehbook call can go through a proxy, in which case, Webhooks configuration requires a wsConfig config notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } auth : { type : \"none\" } includedTheHiveOrganisations : [ \"*\" ] excludedTheHiveOrganisations : [] } ]","title":"Use a proxy"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#use-an-authentication-method","text":"Webhook endpoints can be authenticated, in this case, Webhook configuration requires a auth setting. Supported methods are: No Auth (Default) auth : { type : \"none\" } Basic Auth auth : { type : \"basic\" , username : \"foo\" , password : \"bar\" } Beared Auth auth : { type : \"bearer\" , key : \"foobar\" } Key Auth auth : { type : \"key\" , key : \"foobar\" } Warning In 4.1.0 release, the auth config is REQUIRED .","title":"Use an authentication method"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#examples","text":"Example ## Webhook notification notification.webhook.endpoints = [ { name : local url : \"http://127.0.0.1:5000/\" version : 0 wsConfig { proxy { host : \"10.1.2.10\" port : 8080 } } auth : { type : \"bearer\" , key : \"API_KEY\" } includedTheHiveOrganisations : [ \"ORG1\" , \"ORG2\" ] excludedTheHiveOrganisations : [ \"ORG3\" ] } ]","title":"Examples"},{"location":"thehive/installation-and-configuration/configuration/webhooks/#2-activate-webhooks","text":"This action must be done by an organisation admin (with permission manageConfig ) and requires to run a curl command: read -p 'Enter the URL of TheHive: ' thehive_url read -p 'Enter your login: ' thehive_user read -s -p 'Enter your password: ' thehive_password curl -XPUT -u $thehive_user : $thehive_password -H 'Content-type: application/json' $thehive_url /api/config/organisation/notification -d ' { \"value\": [ { \"delegate\": false, \"trigger\": { \"name\": \"AnyEvent\"}, \"notifier\": { \"name\": \"webhook\", \"endpoint\": \"local\" } } ] }'","title":"2. Activate webhooks"},{"location":"thehive/installation-and-configuration/installation/build-sources/","text":"Installing and running from sources # Dependencies # System packages # apt-get install apt-transport-https NPM # curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.0/install.sh | bash Bower and Grunt # nvm install --lts npm install -g bower grunt Build # The backend cd /opt git clone https://github.com/TheHive-Project/TheHive.git cd TheHive git checkout scalligraph git submodule init git submodule update ./sbt stage The UI cd /opt/TheHive/frontend npm install bower install grunt build","title":"Build sources"},{"location":"thehive/installation-and-configuration/installation/build-sources/#installing-and-running-from-sources","text":"","title":"Installing and running from sources"},{"location":"thehive/installation-and-configuration/installation/build-sources/#dependencies","text":"","title":"Dependencies"},{"location":"thehive/installation-and-configuration/installation/build-sources/#system-packages","text":"apt-get install apt-transport-https","title":"System packages"},{"location":"thehive/installation-and-configuration/installation/build-sources/#npm","text":"curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.35.0/install.sh | bash","title":"NPM"},{"location":"thehive/installation-and-configuration/installation/build-sources/#bower-and-grunt","text":"nvm install --lts npm install -g bower grunt","title":"Bower and Grunt"},{"location":"thehive/installation-and-configuration/installation/build-sources/#build","text":"The backend cd /opt git clone https://github.com/TheHive-Project/TheHive.git cd TheHive git checkout scalligraph git submodule init git submodule update ./sbt stage The UI cd /opt/TheHive/frontend npm install bower install grunt build","title":"Build"},{"location":"thehive/installation-and-configuration/installation/hadoop/","text":"Hadoop: installation and configuration # This guide proposes an example of installation and configuration of Apache Hadoop . Installation # Download hadoop distribution from https://hadoop.apache.org/releases.html and uncompress. cd /tmp wget https://downloads.apache.org/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz cd /opt tar zxf /tmp/hadoop-3.1.3.tar.gz ln -s hadoop-3.1.3 hadoop Create a user and update permissions useradd hadoop chown hadoop:root -R /opt/hadoop* Create a datastore and set permissions mkdir /opt/thp/thehive/hdfs chown hadoop:root -R /opt/thp/thehive/hdfs Create ssh keys for hadoop user: su - hadoop ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys chmod 0600 ~/.ssh/authorized_keys Update .bashrc file for hadoop user. Add following lines: export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export HADOOP_HOME=/opt/hadoop export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin export HADOOP_MAPRED_HOME=$HADOOP_HOME export HADOOP_COMMON_HOME=$HADOOP_HOME export HADOOP_HDFS_HOME=$HADOOP_HOME export YARN_HOME=$HADOOP_HOME Note : Apache has a well detailed documentation for more advanced configuration with Hadoop. Configuration the Hadoop Master # Configuration files are located in etc/hadoop ( /opt/hadoop/etc/hadoop ). They must be identical in all nodes. Notes : The configuration described there is for a single node server. This node is the master node, namenode and datanode (refer to Hadoop documentation for more information). After validating this node is running successfully, refer to the related guide to add nodes; Ensure you update the port value to something different than 9000 as it is already reserved for TheHive application service; Edit the file core-site.xml : <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name> fs.defaultFS </name> <value> hdfs://thehive1:10000 </value> </property> <property> <name> hadoop.tmp.dir </name> <value> /opt/thp/thehive/hdfs/temp </value> </property> <property> <name> dfs.client.block.write.replace-datanode-on-failure.best-effort </name> <value> true </value> </property> </configuration> Edit the file hdfs-site.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name> dfs.replication </name> <value> 2 </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> /opt/thp/thehive/hdfs/namenode/data </value> </property> <property> <name> dfs.datanode.name.dir </name> <value> /opt/thp/thehive/hdfs/datanode/data </value> </property> <property> <name> dfs.namenode.checkpoint.dir </name> <value> /opt/thp/thehive/hdfs/checkpoint </value> </property> <property> <name> dfs.namenode.http-address </name> <value> 0.0.0.0:9870 </value> </property> <!-- <property> <name>dfs.client.block.write.replace-datanode-on-failure.best-effort</name> <value>true</value> </property> --> <property> <name> dfs.client.block.write.replace-datanode-on-failure.policy </name> <value> NEVER </value> </property> </configuration> Format the volume and start services # Format the volume su - hadoop cd /opt/hadoop bin/hdfs namenode -format Run it as a service # Create the /etc/systemd/system/hadoop.service file with the following content: [Unit] Description=Hadoop Documentation=https://hadoop.apache.org/docs/current/index.html Wants=network-online.target After=network-online.target [Service] WorkingDirectory=/opt/hadoop Type=forking User=hadoop Group=hadoop Environment=JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 Environment=HADOOP_HOME=/opt/hadoop Environment=YARN_HOME=/opt/hadoop Environment=HADOOP_COMMON_HOME=/opt/hadoop Environment=HADOOP_HDFS_HOME=/opt/hadoop Environment=HADOOP_MAPRED_HOME=/opt/hadoop Restart=on-failure TimeoutStartSec=2min ExecStart=/opt/hadoop/sbin/start-all.sh ExecStop=/opt/hadoop/sbin/stop-all.sh StandardOutput=null StandardError=null # Specifies the maximum file descriptor number that can be opened by this process LimitNOFILE=65536 # Disable timeout logic and wait until process is stopped TimeoutStopSec=0 # SIGTERM signal is used to stop the Java process KillSignal=SIGTERM # Java process is never killed SendSIGKILL=no [Install] WantedBy=multi-user.target Start the service # service hadoop start You can check cluster status in http://thehive1:9870 Add nodes # To add Hadoop nodes, refer the the related guide .","title":"Hadoop: installation and configuration"},{"location":"thehive/installation-and-configuration/installation/hadoop/#hadoop-installation-and-configuration","text":"This guide proposes an example of installation and configuration of Apache Hadoop .","title":"Hadoop: installation and configuration"},{"location":"thehive/installation-and-configuration/installation/hadoop/#installation","text":"Download hadoop distribution from https://hadoop.apache.org/releases.html and uncompress. cd /tmp wget https://downloads.apache.org/hadoop/common/hadoop-3.1.3/hadoop-3.1.3.tar.gz cd /opt tar zxf /tmp/hadoop-3.1.3.tar.gz ln -s hadoop-3.1.3 hadoop Create a user and update permissions useradd hadoop chown hadoop:root -R /opt/hadoop* Create a datastore and set permissions mkdir /opt/thp/thehive/hdfs chown hadoop:root -R /opt/thp/thehive/hdfs Create ssh keys for hadoop user: su - hadoop ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys chmod 0600 ~/.ssh/authorized_keys Update .bashrc file for hadoop user. Add following lines: export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 export HADOOP_HOME=/opt/hadoop export PATH=$PATH:$HADOOP_HOME/bin export PATH=$PATH:$HADOOP_HOME/sbin export HADOOP_MAPRED_HOME=$HADOOP_HOME export HADOOP_COMMON_HOME=$HADOOP_HOME export HADOOP_HDFS_HOME=$HADOOP_HOME export YARN_HOME=$HADOOP_HOME Note : Apache has a well detailed documentation for more advanced configuration with Hadoop.","title":"Installation"},{"location":"thehive/installation-and-configuration/installation/hadoop/#configuration-the-hadoop-master","text":"Configuration files are located in etc/hadoop ( /opt/hadoop/etc/hadoop ). They must be identical in all nodes. Notes : The configuration described there is for a single node server. This node is the master node, namenode and datanode (refer to Hadoop documentation for more information). After validating this node is running successfully, refer to the related guide to add nodes; Ensure you update the port value to something different than 9000 as it is already reserved for TheHive application service; Edit the file core-site.xml : <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name> fs.defaultFS </name> <value> hdfs://thehive1:10000 </value> </property> <property> <name> hadoop.tmp.dir </name> <value> /opt/thp/thehive/hdfs/temp </value> </property> <property> <name> dfs.client.block.write.replace-datanode-on-failure.best-effort </name> <value> true </value> </property> </configuration> Edit the file hdfs-site.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name> dfs.replication </name> <value> 2 </value> </property> <property> <name> dfs.namenode.name.dir </name> <value> /opt/thp/thehive/hdfs/namenode/data </value> </property> <property> <name> dfs.datanode.name.dir </name> <value> /opt/thp/thehive/hdfs/datanode/data </value> </property> <property> <name> dfs.namenode.checkpoint.dir </name> <value> /opt/thp/thehive/hdfs/checkpoint </value> </property> <property> <name> dfs.namenode.http-address </name> <value> 0.0.0.0:9870 </value> </property> <!-- <property> <name>dfs.client.block.write.replace-datanode-on-failure.best-effort</name> <value>true</value> </property> --> <property> <name> dfs.client.block.write.replace-datanode-on-failure.policy </name> <value> NEVER </value> </property> </configuration>","title":"Configuration the Hadoop Master"},{"location":"thehive/installation-and-configuration/installation/hadoop/#format-the-volume-and-start-services","text":"Format the volume su - hadoop cd /opt/hadoop bin/hdfs namenode -format","title":"Format the volume and start services"},{"location":"thehive/installation-and-configuration/installation/hadoop/#run-it-as-a-service","text":"Create the /etc/systemd/system/hadoop.service file with the following content: [Unit] Description=Hadoop Documentation=https://hadoop.apache.org/docs/current/index.html Wants=network-online.target After=network-online.target [Service] WorkingDirectory=/opt/hadoop Type=forking User=hadoop Group=hadoop Environment=JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 Environment=HADOOP_HOME=/opt/hadoop Environment=YARN_HOME=/opt/hadoop Environment=HADOOP_COMMON_HOME=/opt/hadoop Environment=HADOOP_HDFS_HOME=/opt/hadoop Environment=HADOOP_MAPRED_HOME=/opt/hadoop Restart=on-failure TimeoutStartSec=2min ExecStart=/opt/hadoop/sbin/start-all.sh ExecStop=/opt/hadoop/sbin/stop-all.sh StandardOutput=null StandardError=null # Specifies the maximum file descriptor number that can be opened by this process LimitNOFILE=65536 # Disable timeout logic and wait until process is stopped TimeoutStopSec=0 # SIGTERM signal is used to stop the Java process KillSignal=SIGTERM # Java process is never killed SendSIGKILL=no [Install] WantedBy=multi-user.target","title":"Run it as a service"},{"location":"thehive/installation-and-configuration/installation/hadoop/#start-the-service","text":"service hadoop start You can check cluster status in http://thehive1:9870","title":"Start the service"},{"location":"thehive/installation-and-configuration/installation/hadoop/#add-nodes","text":"To add Hadoop nodes, refer the the related guide .","title":"Add nodes"},{"location":"thehive/installation-and-configuration/installation/minio/","text":"","title":"Minio"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/","text":"Step-by-Step guide # This page is a step by step installation and configuration guide to get an TheHive 4 instance up and running. This guide is illustrated with examples for Debian and RPM packages based systems and for installation from binary packages. Java Virtual Machine # Debian apt-get install -y openjdk-8-jre-headless echo JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" >> /etc/environment export JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" RPM yum install -y java-1.8.0-openjdk-headless.x86_64 echo JAVA_HOME = \"/usr/lib/jvm/jre-1.8.0\" >> /etc/environment export JAVA_HOME = \"/usr/lib/jvm/jre-1.8.0\" Other The installation requires Java 8, so refer to your system documentation to install it. Note TheHive can be loaded by Java 11, but not the stable version of Cassandra, which still requires Java 8. If you set up a cluster for the database distinct from TheHive servers: Cassandra nodes can be loaded by Java 8 TheHive nodes can be loaded by Java 11 For standalone servers, with TheHive and Cassandra on the same OS, we recommend having only Java 8 installed for both applications. Cassandra database # Apache Cassandra is a scalable and high available database. TheHive supports the latest stable version 3.11.x of Cassandra. Install from repository # Debian Add Apache repository references curl -fsSL https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add - echo \"deb http://www.apache.org/dist/cassandra/debian 311x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list Install the package sudo apt update sudo apt install cassandra RPM Add the Apache repository of Cassandra to /etc/yum.repos.d/cassandra.repo [ cassandra ] name = Apache Cassandra baseurl = https://downloads.apache.org/cassandra/redhat/311x/ gpgcheck = 1 repo_gpgcheck = 1 gpgkey = https://downloads.apache.org/cassandra/KEYS Install the package yum install -y cassandra Other Download and untgz archive from http://cassandra.apache.org/download/ in the folder of your choice. By default, data is stored in /var/lib/cassandra . Configuration # Start by changing the cluster_name with thp . Run the command cqlsh : cqlsh localhost 9042 cqlsh > UPDATE system . local SET cluster_name = 'thp' where key = 'local' ; Exit and then run: nodetool flush Configure Cassandra by editing /etc/cassandra/cassandra.yaml file. # content from /etc/cassandra/cassandra.yaml cluster_name: 'thp' listen_address: 'xx.xx.xx.xx' # address for nodes rpc_address: 'xx.xx.xx.xx' # address for clients seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: # Ex: \"<ip1>,<ip2>,<ip3>\" - seeds: 'xx.xx.xx.xx' # self for the first node data_file_directories: - '/var/lib/cassandra/data' commitlog_directory: '/var/lib/cassandra/commitlog' saved_caches_directory: '/var/lib/cassandra/saved_caches' hints_directory: - '/var/lib/cassandra/hints' Then restart the service: Debian service cassandra restart RPM Run the service and ensure it restart after a reboot: systemctl daemon-reload service cassandra start chkconfig cassandra on Warning Cassandra service does not start well with the new systemd version. There is an existing issue and a fix on Apache website: https://issues.apache.org/jira/browse/CASSANDRA-15273 By default Cassandra listens on 7000/tcp (inter-node), 9042/tcp (client). Additional configuration # For additional configuration options, refer to: Cassandra documentation page Datastax documentation page Security # To add security measures in Cassandra , refer the the related administration guide . Add nodes # To add Cassandra nodes, refer the the related administration guide . Indexing engine # Starting from TheHive 4.1.0, a solution to store data indexes is required. These indexes should be unique and the same for all nodes of TheHive cluster. TheHive embed a Lucene engine you can use for standalone server For clusters setups, an instance of Elasticsearch is required Local lucene engine Create a folder dedicated to host indexes for TheHive: mkdir /opt/thp/thehive/index chown thehive:thehive -R /opt/thp/thehive/index Elasticsearch Use an existing Elasticsearch instance or install a new one. This instance should be reachable by all nodes of a cluster. Warning Elasticsearch configuration should use the default value for script.allowed_types , or contain the following configuration line: script.allowed_types : inline,stored Note Indexes will be created at the first start of TheHive. It can take a certain amount of time, depending the size of the database Like data and files, indexes should be part of the backup policy Indexes can removed and created again File storage # Files uploaded in TheHive (in task logs or in observables ) can be stores in localsystem, in a Hadoop filesystem (recommended) or in the graph database. For standalone production and test servers , we recommends using local filesystem. If you think about building a cluster with TheHive, you have several possible solutions: using Hadoop or S3 services ; see the related guide for more details and an example with MinIO servers. Local Filesystem Warning This option is perfect for standalone servers . If you intend to build a cluster for your instance of TheHive 4 we recommend: using a NFS share, common to all nodes having a look at storage solutions implementing S3 or HDFS. To store files on the local filesystem, start by choosing the dedicated folder: mkdir -p /opt/thp/thehive/files This path will be used in the configuration of TheHive. Later, after having installed TheHive, ensure the user thehive owns the path chosen for storing files: chown -R thehive:thehive /opt/thp/thehive/files S3 with Min.io An example of installing, configuring and use Min.IO is detailed in this documentation . HDFS with Hadoop An example of installing, configuring and use Apache Hadoop is detailed in this documentation . TheHive # This part contains instructions to install TheHive and then configure it. Warning TheHive4 can't be installed on the same server than older versions. We recommend installing it on a new server, especially if a migration is foreseen. Installation # All packages are published on our packages repository. We support Debian and RPM packages as well as binary packages (zip archive). All packages are signed using our GPG key 562CBC1C . Its fingerprint is 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C . Debian curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - RPM sudo rpm --import https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY We also release stable and beta version of the applications. Stable versions # Install TheHive 4.x package of the stable version by using the following commands: Debian echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive4 RPM Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/release/noarch gpgcheck = 1 Then install the package using yum : yum install thehive4 Other Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive4-latest.zip unzip thehive4-latest.zip ln -s thehive4-x.x.x thehive Prepare the system It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: addgroup thehive adduser --system thehive chown -R thehive:thehive /opt/thehive mkdir /etc/thehive touch /etc/thehive/application.conf chown root:thehive /etc/thehive chgrp thehive /etc/thehive/application.conf chmod 640 /etc/thehive/application.conf Copy the systemd script in /etc/systemd/system/thehive.service . cd /tmp wget https://github.com/TheHive-Project/TheHive/blob/master/package/thehive.service cp thehive.service /etc/systemd/system/thehive.service Beta versions # To install beta versions of TheHive4, use the following setup: Debian echo 'deb https://deb.thehive-project.org beta main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive4 RPM setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/beta/noarch gpgcheck = 1 Then install the package using yum : yum install thehive4 Other Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive4-beta-latest.zip unzip thehive4-beta-latest.zip ln -s thehive4-x.x.x thehive Prepare the system It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: addgroup thehive adduser --system thehive chown -R thehive:thehive /opt/thehive mkdir /etc/thehive touch /etc/thehive/application.conf chown root:thehive /etc/thehive chgrp thehive /etc/thehive/application.conf chmod 640 /etc/thehive/application.conf Copy the systemd script in /etc/systemd/system/thehive.service . cd /tmp wget https://github.com/TheHive-Project/TheHive/blob/master/package/thehive.service cp thehive.service /etc/systemd/system/thehive.service Warning We recommend using or playing with Beta version for testing purpose only . Configuration # Following configurations are required to start TheHive successfully: Secret key configuration Database configuration File storage configuration Secret key configuration # Debian The secret key is automatically generated and stored in /etc/thehive/secret.conf by package installation script. RPM The secret key is automatically generated and stored in /etc/thehive/secret.conf by package installation script. Other Setup a secret key in the /etc/thehive/secret.conf file by running the following command: cat > /etc/thehive/secret.conf << _EOF_ play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\" _EOF_ Database # To use Cassandra database, TheHive configuration file ( /etc/thehive/application.conf ) has to be edited and updated with following lines: db { provider : janusgraph janusgraph { storage { backend : cql hostname : [ \"127.0.0.1\" ] # seed node ip addresses #username: \"<cassandra_username>\" # login to connect to database (if configured in Cassandra) #password: \"<cassandra_passowrd\" cql { cluster-name : thp # cluster name keyspace : thehive # name of the keyspace local-datacenter : datacenter1 # name of the datacenter where TheHive runs (relevant only on multi datacenter setup) # replication-factor: 2 # number of replica read-consistency-level : ONE write-consistency-level : ONE } } } } Indexes # Update db.storage configuration part in /etc/thehive/application.conf accordingly to your setup. Lucene If your setup is a standalone server or you are using a common NFS share, configure TheHive like this: db { provider : janusgraph janusgraph { storage { [..] } ## Index configuration index.search { backend : lucene directory : /opt/thp/thehive/index } } } Elasticsearch If you decided to have access to a centralised index with Elasticsearch, configure TheHive like this: db { provider : janusgraph janusgraph { storage { [..] } ## Index configuration index.search { backend : elasticsearch hostname : [ \"10.1.2.20\" ] index-name : thehive } } } Filesystem # Local filesystem If you chose to store files on the local filesystem: Ensure permission of the folder chown -R thehive:thehive /opt/thp/thehive/files add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider = localfs localfs.location = /opt/thp/thehive/files } S3 If you chose MinIO and a S3 object storage system to store files in a filesystem, add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_ADDRESS>:9100\" accessKey = \"<MINIO ACCESS KEY>\" secretKey = \"<MINIO SECRET KEY>\" } } HDFS If you chose Apache Hadoop and a HDFS filesystem to store files in a distrubuted filesystem, add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider : hdfs hdfs { root : \"hdfs://thehive1:10000\" # namenode server location : \"/thehive\" username : thehive } } Run # Save configuration file and run the service: service thehive start Please note that the service may take some time to start. Once it is started, you may launch your browser and connect to http://YOUR_SERVER_ADDRESS:9000/ . The default admin user is admin@thehive.local with password secret . It is recommended to change the default password. Advanced configuration # For additional configuration options, please refer to the Configuration Guides .","title":"Step by step guide"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#step-by-step-guide","text":"This page is a step by step installation and configuration guide to get an TheHive 4 instance up and running. This guide is illustrated with examples for Debian and RPM packages based systems and for installation from binary packages.","title":"Step-by-Step guide"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#java-virtual-machine","text":"Debian apt-get install -y openjdk-8-jre-headless echo JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" >> /etc/environment export JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\" RPM yum install -y java-1.8.0-openjdk-headless.x86_64 echo JAVA_HOME = \"/usr/lib/jvm/jre-1.8.0\" >> /etc/environment export JAVA_HOME = \"/usr/lib/jvm/jre-1.8.0\" Other The installation requires Java 8, so refer to your system documentation to install it. Note TheHive can be loaded by Java 11, but not the stable version of Cassandra, which still requires Java 8. If you set up a cluster for the database distinct from TheHive servers: Cassandra nodes can be loaded by Java 8 TheHive nodes can be loaded by Java 11 For standalone servers, with TheHive and Cassandra on the same OS, we recommend having only Java 8 installed for both applications.","title":"Java Virtual Machine"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#cassandra-database","text":"Apache Cassandra is a scalable and high available database. TheHive supports the latest stable version 3.11.x of Cassandra.","title":"Cassandra database"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#install-from-repository","text":"Debian Add Apache repository references curl -fsSL https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add - echo \"deb http://www.apache.org/dist/cassandra/debian 311x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list Install the package sudo apt update sudo apt install cassandra RPM Add the Apache repository of Cassandra to /etc/yum.repos.d/cassandra.repo [ cassandra ] name = Apache Cassandra baseurl = https://downloads.apache.org/cassandra/redhat/311x/ gpgcheck = 1 repo_gpgcheck = 1 gpgkey = https://downloads.apache.org/cassandra/KEYS Install the package yum install -y cassandra Other Download and untgz archive from http://cassandra.apache.org/download/ in the folder of your choice. By default, data is stored in /var/lib/cassandra .","title":"Install from repository"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#configuration","text":"Start by changing the cluster_name with thp . Run the command cqlsh : cqlsh localhost 9042 cqlsh > UPDATE system . local SET cluster_name = 'thp' where key = 'local' ; Exit and then run: nodetool flush Configure Cassandra by editing /etc/cassandra/cassandra.yaml file. # content from /etc/cassandra/cassandra.yaml cluster_name: 'thp' listen_address: 'xx.xx.xx.xx' # address for nodes rpc_address: 'xx.xx.xx.xx' # address for clients seed_provider: - class_name: org.apache.cassandra.locator.SimpleSeedProvider parameters: # Ex: \"<ip1>,<ip2>,<ip3>\" - seeds: 'xx.xx.xx.xx' # self for the first node data_file_directories: - '/var/lib/cassandra/data' commitlog_directory: '/var/lib/cassandra/commitlog' saved_caches_directory: '/var/lib/cassandra/saved_caches' hints_directory: - '/var/lib/cassandra/hints' Then restart the service: Debian service cassandra restart RPM Run the service and ensure it restart after a reboot: systemctl daemon-reload service cassandra start chkconfig cassandra on Warning Cassandra service does not start well with the new systemd version. There is an existing issue and a fix on Apache website: https://issues.apache.org/jira/browse/CASSANDRA-15273 By default Cassandra listens on 7000/tcp (inter-node), 9042/tcp (client).","title":"Configuration"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#additional-configuration","text":"For additional configuration options, refer to: Cassandra documentation page Datastax documentation page","title":"Additional configuration"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#security","text":"To add security measures in Cassandra , refer the the related administration guide .","title":"Security"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#add-nodes","text":"To add Cassandra nodes, refer the the related administration guide .","title":"Add nodes"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#indexing-engine","text":"Starting from TheHive 4.1.0, a solution to store data indexes is required. These indexes should be unique and the same for all nodes of TheHive cluster. TheHive embed a Lucene engine you can use for standalone server For clusters setups, an instance of Elasticsearch is required Local lucene engine Create a folder dedicated to host indexes for TheHive: mkdir /opt/thp/thehive/index chown thehive:thehive -R /opt/thp/thehive/index Elasticsearch Use an existing Elasticsearch instance or install a new one. This instance should be reachable by all nodes of a cluster. Warning Elasticsearch configuration should use the default value for script.allowed_types , or contain the following configuration line: script.allowed_types : inline,stored Note Indexes will be created at the first start of TheHive. It can take a certain amount of time, depending the size of the database Like data and files, indexes should be part of the backup policy Indexes can removed and created again","title":"Indexing engine"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#file-storage","text":"Files uploaded in TheHive (in task logs or in observables ) can be stores in localsystem, in a Hadoop filesystem (recommended) or in the graph database. For standalone production and test servers , we recommends using local filesystem. If you think about building a cluster with TheHive, you have several possible solutions: using Hadoop or S3 services ; see the related guide for more details and an example with MinIO servers. Local Filesystem Warning This option is perfect for standalone servers . If you intend to build a cluster for your instance of TheHive 4 we recommend: using a NFS share, common to all nodes having a look at storage solutions implementing S3 or HDFS. To store files on the local filesystem, start by choosing the dedicated folder: mkdir -p /opt/thp/thehive/files This path will be used in the configuration of TheHive. Later, after having installed TheHive, ensure the user thehive owns the path chosen for storing files: chown -R thehive:thehive /opt/thp/thehive/files S3 with Min.io An example of installing, configuring and use Min.IO is detailed in this documentation . HDFS with Hadoop An example of installing, configuring and use Apache Hadoop is detailed in this documentation .","title":"File storage"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#thehive","text":"This part contains instructions to install TheHive and then configure it. Warning TheHive4 can't be installed on the same server than older versions. We recommend installing it on a new server, especially if a migration is foreseen.","title":"TheHive"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#installation","text":"All packages are published on our packages repository. We support Debian and RPM packages as well as binary packages (zip archive). All packages are signed using our GPG key 562CBC1C . Its fingerprint is 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C . Debian curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - RPM sudo rpm --import https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY We also release stable and beta version of the applications.","title":"Installation"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#stable-versions","text":"Install TheHive 4.x package of the stable version by using the following commands: Debian echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive4 RPM Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/release/noarch gpgcheck = 1 Then install the package using yum : yum install thehive4 Other Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive4-latest.zip unzip thehive4-latest.zip ln -s thehive4-x.x.x thehive Prepare the system It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: addgroup thehive adduser --system thehive chown -R thehive:thehive /opt/thehive mkdir /etc/thehive touch /etc/thehive/application.conf chown root:thehive /etc/thehive chgrp thehive /etc/thehive/application.conf chmod 640 /etc/thehive/application.conf Copy the systemd script in /etc/systemd/system/thehive.service . cd /tmp wget https://github.com/TheHive-Project/TheHive/blob/master/package/thehive.service cp thehive.service /etc/systemd/system/thehive.service","title":"Stable versions"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#beta-versions","text":"To install beta versions of TheHive4, use the following setup: Debian echo 'deb https://deb.thehive-project.org beta main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive4 RPM setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/beta/noarch gpgcheck = 1 Then install the package using yum : yum install thehive4 Other Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive4-beta-latest.zip unzip thehive4-beta-latest.zip ln -s thehive4-x.x.x thehive Prepare the system It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: addgroup thehive adduser --system thehive chown -R thehive:thehive /opt/thehive mkdir /etc/thehive touch /etc/thehive/application.conf chown root:thehive /etc/thehive chgrp thehive /etc/thehive/application.conf chmod 640 /etc/thehive/application.conf Copy the systemd script in /etc/systemd/system/thehive.service . cd /tmp wget https://github.com/TheHive-Project/TheHive/blob/master/package/thehive.service cp thehive.service /etc/systemd/system/thehive.service Warning We recommend using or playing with Beta version for testing purpose only .","title":"Beta versions"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#configuration_1","text":"Following configurations are required to start TheHive successfully: Secret key configuration Database configuration File storage configuration","title":"Configuration"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#secret-key-configuration","text":"Debian The secret key is automatically generated and stored in /etc/thehive/secret.conf by package installation script. RPM The secret key is automatically generated and stored in /etc/thehive/secret.conf by package installation script. Other Setup a secret key in the /etc/thehive/secret.conf file by running the following command: cat > /etc/thehive/secret.conf << _EOF_ play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\" _EOF_","title":"Secret key configuration"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#database","text":"To use Cassandra database, TheHive configuration file ( /etc/thehive/application.conf ) has to be edited and updated with following lines: db { provider : janusgraph janusgraph { storage { backend : cql hostname : [ \"127.0.0.1\" ] # seed node ip addresses #username: \"<cassandra_username>\" # login to connect to database (if configured in Cassandra) #password: \"<cassandra_passowrd\" cql { cluster-name : thp # cluster name keyspace : thehive # name of the keyspace local-datacenter : datacenter1 # name of the datacenter where TheHive runs (relevant only on multi datacenter setup) # replication-factor: 2 # number of replica read-consistency-level : ONE write-consistency-level : ONE } } } }","title":"Database"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#indexes","text":"Update db.storage configuration part in /etc/thehive/application.conf accordingly to your setup. Lucene If your setup is a standalone server or you are using a common NFS share, configure TheHive like this: db { provider : janusgraph janusgraph { storage { [..] } ## Index configuration index.search { backend : lucene directory : /opt/thp/thehive/index } } } Elasticsearch If you decided to have access to a centralised index with Elasticsearch, configure TheHive like this: db { provider : janusgraph janusgraph { storage { [..] } ## Index configuration index.search { backend : elasticsearch hostname : [ \"10.1.2.20\" ] index-name : thehive } } }","title":"Indexes"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#filesystem","text":"Local filesystem If you chose to store files on the local filesystem: Ensure permission of the folder chown -R thehive:thehive /opt/thp/thehive/files add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider = localfs localfs.location = /opt/thp/thehive/files } S3 If you chose MinIO and a S3 object storage system to store files in a filesystem, add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider : s3 s3 { bucket = \"thehive\" readTimeout = 1 minute writeTimeout = 1 minute chunkSize = 1 MB endpoint = \"http://<IP_ADDRESS>:9100\" accessKey = \"<MINIO ACCESS KEY>\" secretKey = \"<MINIO SECRET KEY>\" } } HDFS If you chose Apache Hadoop and a HDFS filesystem to store files in a distrubuted filesystem, add following lines to TheHive configuration file ( /etc/thehive/application.conf ) ## Storage configuration storage { provider : hdfs hdfs { root : \"hdfs://thehive1:10000\" # namenode server location : \"/thehive\" username : thehive } }","title":"Filesystem"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#run","text":"Save configuration file and run the service: service thehive start Please note that the service may take some time to start. Once it is started, you may launch your browser and connect to http://YOUR_SERVER_ADDRESS:9000/ . The default admin user is admin@thehive.local with password secret . It is recommended to change the default password.","title":"Run"},{"location":"thehive/installation-and-configuration/installation/step-by-step-guide/#advanced-configuration","text":"For additional configuration options, please refer to the Configuration Guides .","title":"Advanced configuration"},{"location":"thehive/legacy/thehive3/","text":"TheHive is a scalable 4-in-1 open source and free security incident response platform designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly. Thanks to Cortex , our powerful free and open source analysis engine, you can analyze (and triage) observables at scale using more than 100 analyzers. Additionally and starting from TheHive 3.1.0, you can actively respond to threats and interact with your constituency and other parties thanks to Cortex responders. Last but not least, TheHive is highly integrated with MISP , the de facto standard of threat sharing, as it can pull events from several MISP instances and export investigation cases back to one or several ones. It also has additional features such as MISP extended events and health checking. This is TheHive's documentation repository. If you are looking for its source code, please visit https://github.com/TheHive-Project/TheHive/ . Hardware Pre-requisites # TheHive uses ElasticSearch to store data. Both software use a Java VM. We recommend using a virtual machine with 8vCPU, 8 GB of RAM and 60 GB of disk. You can also use a physical machine with similar specifications. Guides # Installation Guide Administration Guide Configuration Guide Webhooks Cluster Configuration Updating Backup & Restore Migration Guide API Documentation (incomplete) Miscellaneous Information # Feature Set (In Progress) Changelog Training Material Additional Resources Single Sign-On on TheHive with X.509 Certificates (Experimental Feature) License # TheHive is an open source and free software released under the AGPL (Affero General Public License). We, TheHive Project, are committed to ensure that TheHive will remain a free and open source project on the long-run. Updates # Information, news and updates are regularly posted on TheHive Project Twitter account and on the blog . Contributing # We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests using issues . We do have a Code of conduct . Make sure to check it out before contributing. Support # Please open an issue on GitHub if you'd like to report a bug or request a feature. We are also available on Gitter to help you out. If you need to contact the Project's team, send an email to support@thehive-project.org . Important Note : If you have problems with TheHive4py , please open an issue on its dedicated repository . If you encounter an issue with Cortex or would like to request a Cortex-related feature, please open an issue on its dedicated GitHub repository . If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the analyzers' dedicated GitHub repository . Community Discussions # We have set up a Google forum at https://groups.google.com/a/thehive-project.org/d/forum/users . To request access, you need a Google account. You may create one using a Gmail address or without it . Website # https://thehive-project.org/","title":"Index"},{"location":"thehive/legacy/thehive3/#hardware-pre-requisites","text":"TheHive uses ElasticSearch to store data. Both software use a Java VM. We recommend using a virtual machine with 8vCPU, 8 GB of RAM and 60 GB of disk. You can also use a physical machine with similar specifications.","title":"Hardware Pre-requisites"},{"location":"thehive/legacy/thehive3/#guides","text":"Installation Guide Administration Guide Configuration Guide Webhooks Cluster Configuration Updating Backup & Restore Migration Guide API Documentation (incomplete)","title":"Guides"},{"location":"thehive/legacy/thehive3/#miscellaneous-information","text":"Feature Set (In Progress) Changelog Training Material Additional Resources Single Sign-On on TheHive with X.509 Certificates (Experimental Feature)","title":"Miscellaneous Information"},{"location":"thehive/legacy/thehive3/#license","text":"TheHive is an open source and free software released under the AGPL (Affero General Public License). We, TheHive Project, are committed to ensure that TheHive will remain a free and open source project on the long-run.","title":"License"},{"location":"thehive/legacy/thehive3/#updates","text":"Information, news and updates are regularly posted on TheHive Project Twitter account and on the blog .","title":"Updates"},{"location":"thehive/legacy/thehive3/#contributing","text":"We welcome your contributions. Please feel free to fork the code, play with it, make some patches and send us pull requests using issues . We do have a Code of conduct . Make sure to check it out before contributing.","title":"Contributing"},{"location":"thehive/legacy/thehive3/#support","text":"Please open an issue on GitHub if you'd like to report a bug or request a feature. We are also available on Gitter to help you out. If you need to contact the Project's team, send an email to support@thehive-project.org . Important Note : If you have problems with TheHive4py , please open an issue on its dedicated repository . If you encounter an issue with Cortex or would like to request a Cortex-related feature, please open an issue on its dedicated GitHub repository . If you have troubles with a Cortex analyzer or would like to request a new one or an improvement to an existing analyzer, please open an issue on the analyzers' dedicated GitHub repository .","title":"Support"},{"location":"thehive/legacy/thehive3/#community-discussions","text":"We have set up a Google forum at https://groups.google.com/a/thehive-project.org/d/forum/users . To request access, you need a Google account. You may create one using a Gmail address or without it .","title":"Community Discussions"},{"location":"thehive/legacy/thehive3/#website","text":"https://thehive-project.org/","title":"Website"},{"location":"thehive/legacy/thehive3/feature-set/","text":"Feature set # This document lists the features provided by TheHive in either the UI or APIs and Webhooks. TheHive comes with the native support of integrating: one or more Cortex instances one or more MISP instances Authentication # TheHive supports multiple authentication methods: Local authentication using a local user collection AD authentication LDAP authentication SSO authentication X.509 certificates authentication Case Management # List and filter cases Create new cases from scratch or using case templates Add custom fields to cases Add metrics to cases Find linked cases to a given case based shared observables Add tasks and task groups to cases Assign tasks to a given user Add logs to tasks, including attachment to task logs Add observables to a case Execute Cortex responders against cases tasks task logs Delete cases by administrators only Alert Management # Alerts are a sort of incident not yet qualified as a Case. The Alerts sections allows: Listing and searching for alerts Marking alerts as read Ignoring alert updates Previewing alert details Display alert details and editable custom fields Display alerts observables Display similar cases Importing an alert as an emtpty case or using a case template Merging an alert into an existing case MISP Integration # MISP is natively integrated to TheHive allowing: The declaration of one or more MISP instances Each instance can be used to Import and/or Export events from MISP or cases to MISP Imported MISP events are made available as Alerts Imporing is configurable using filters (configuration files) Feeders # Feeders are external tools designed to send alerts to TheHive leveraging the REST APIs Thehive offers Feeders can be written and any programming language as long as it is compatible with TheHive APIs Feeders can be written in Python and use TheHive4Py Search capabilities # The search section provided by TheHive allows searching for the following objects using dynamic forms: cases tasks observables logs alerts Dashboarding # The dashboards section allows: creating private dashboards per user creating shared dashboads visible by all users adding widgets to dashboards using a drag & drop capabilities creating widgets that target cases, tasks, observables, alerts, jobs configuring widgets in a granular manner Administration # Case templates # Create case templates Add tasks to templates Add metrics to templates Add custom fields to templates Define default values for custom fields, metrics and tasks Export case template definitions Import case template definitions Metrics # List and Create metrics Custom fields # Create custom fields Update custom fields Users # List users Create/Edit users Set a user password Set a user API key Revoke a user's API key Lock a user Analyzer report templates # Report templates are used to display the raw reports from Cortex in a text format. This section allows: Importing short and long reports Customize short and long reports for each analyzer Cortex integration # TheHive uses Cortex to have access to analyzers and responsders Analyzers can be launched against observables to get more details about a given observable Responders can be launched against case, tasks, observables, logs, and alerts to execute an action One or more Cortex instances can be connected to TheHive Database migration # TheHive provides a mechanism to upgrade the Elasticsearch database by copying the index and making transformations on it.","title":"Feature set"},{"location":"thehive/legacy/thehive3/feature-set/#feature-set","text":"This document lists the features provided by TheHive in either the UI or APIs and Webhooks. TheHive comes with the native support of integrating: one or more Cortex instances one or more MISP instances","title":"Feature set"},{"location":"thehive/legacy/thehive3/feature-set/#authentication","text":"TheHive supports multiple authentication methods: Local authentication using a local user collection AD authentication LDAP authentication SSO authentication X.509 certificates authentication","title":"Authentication"},{"location":"thehive/legacy/thehive3/feature-set/#case-management","text":"List and filter cases Create new cases from scratch or using case templates Add custom fields to cases Add metrics to cases Find linked cases to a given case based shared observables Add tasks and task groups to cases Assign tasks to a given user Add logs to tasks, including attachment to task logs Add observables to a case Execute Cortex responders against cases tasks task logs Delete cases by administrators only","title":"Case Management"},{"location":"thehive/legacy/thehive3/feature-set/#alert-management","text":"Alerts are a sort of incident not yet qualified as a Case. The Alerts sections allows: Listing and searching for alerts Marking alerts as read Ignoring alert updates Previewing alert details Display alert details and editable custom fields Display alerts observables Display similar cases Importing an alert as an emtpty case or using a case template Merging an alert into an existing case","title":"Alert Management"},{"location":"thehive/legacy/thehive3/feature-set/#misp-integration","text":"MISP is natively integrated to TheHive allowing: The declaration of one or more MISP instances Each instance can be used to Import and/or Export events from MISP or cases to MISP Imported MISP events are made available as Alerts Imporing is configurable using filters (configuration files)","title":"MISP Integration"},{"location":"thehive/legacy/thehive3/feature-set/#feeders","text":"Feeders are external tools designed to send alerts to TheHive leveraging the REST APIs Thehive offers Feeders can be written and any programming language as long as it is compatible with TheHive APIs Feeders can be written in Python and use TheHive4Py","title":"Feeders"},{"location":"thehive/legacy/thehive3/feature-set/#search-capabilities","text":"The search section provided by TheHive allows searching for the following objects using dynamic forms: cases tasks observables logs alerts","title":"Search capabilities"},{"location":"thehive/legacy/thehive3/feature-set/#dashboarding","text":"The dashboards section allows: creating private dashboards per user creating shared dashboads visible by all users adding widgets to dashboards using a drag & drop capabilities creating widgets that target cases, tasks, observables, alerts, jobs configuring widgets in a granular manner","title":"Dashboarding"},{"location":"thehive/legacy/thehive3/feature-set/#administration","text":"","title":"Administration"},{"location":"thehive/legacy/thehive3/feature-set/#case-templates","text":"Create case templates Add tasks to templates Add metrics to templates Add custom fields to templates Define default values for custom fields, metrics and tasks Export case template definitions Import case template definitions","title":"Case templates"},{"location":"thehive/legacy/thehive3/feature-set/#metrics","text":"List and Create metrics","title":"Metrics"},{"location":"thehive/legacy/thehive3/feature-set/#custom-fields","text":"Create custom fields Update custom fields","title":"Custom fields"},{"location":"thehive/legacy/thehive3/feature-set/#users","text":"List users Create/Edit users Set a user password Set a user API key Revoke a user's API key Lock a user","title":"Users"},{"location":"thehive/legacy/thehive3/feature-set/#analyzer-report-templates","text":"Report templates are used to display the raw reports from Cortex in a text format. This section allows: Importing short and long reports Customize short and long reports for each analyzer","title":"Analyzer report templates"},{"location":"thehive/legacy/thehive3/feature-set/#cortex-integration","text":"TheHive uses Cortex to have access to analyzers and responsders Analyzers can be launched against observables to get more details about a given observable Responders can be launched against case, tasks, observables, logs, and alerts to execute an action One or more Cortex instances can be connected to TheHive","title":"Cortex integration"},{"location":"thehive/legacy/thehive3/feature-set/#database-migration","text":"TheHive provides a mechanism to upgrade the Elasticsearch database by copying the index and making transformations on it.","title":"Database migration"},{"location":"thehive/legacy/thehive3/migration-guide/","text":"Migration guide # From 3.4.x to 3.5.0 # Taking into account the EoL of version 6.x. of Elasticsearch, TheHive 3.5.0 is the first version to support Elasticsearch 7.x. This version introduce breaking changes. This time, we had no choice, we were not able to make TheHive support smoothly the ES upgrade. TheHive 3.5.0 supports Elasticsearch 7.x ONLY . This first steps before starting the upgrade process are: Identify the version of Elasticsearch which created your index Stop TheHive service Stop Elasticsearch service How to identify the version of Elasticsearch which created your database index ? # The software jq is required to manipulate JSON and create new indexes. More information at https://stedolan.github.io/jq/ . Run the following command : curl -s http://127.0.0.1:9200/the_hive_15?human | jq '.the_hive_15.settings.index.version.created_string' if the output is similar to \"5.x\" then your database index has been created with Elasticsearch 5.x reindexing is required, you should follow a dedicated process to upgrade . If it is \"6.x\" then your database has been created with Elasticsearch 6. Your database was created with Elasticsearch 5.x or earlier # This is where things might be complicated. This upgrade progress requires handling the database index by updating parameters, and reindex before updating Elasticsearch, and updating TheHive. Read carefully the dedicated documentation . It should help you run this specific actions on your Elasticsearch database, and also install or update application whether you are using DEB, RPM or binary packages, and even docker images. Your database was created with Elasticsearch 6.x # If you started using TheHive with Elasticsearch 6.x, then you just need to update the configuration of Elasticsearch to reflect this one: [..] http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive script.allowed_types: inline thread_pool.search.queue_size: 100000 thread_pool.write.queue_size: 10000 Following parameters * are not accepted anymore by Elasticsearch 7: thread_pool.index.queue_size thread_pool.bulk.queue_size With TheHive service stopped, ensure the new version of Elasticsearch starts. If everything is ok, then TheHive 3.5.0 can be installed. To run this operation successfully, you need to update your repository configuration if you are using DEB and RPM packages, or specify the right version to install if using docker. Read carefully the installation guide . From 3.3.x to 3.4.0 # Starting from version 3.4.0-RC1, TheHive supports Elasticsearch 6 and will continue to work with Elasticsearch 5.x. TheHive 3.4.0-RC1 and later versions communicate with Elasticsearch using its HTTP service (9200/tcp by default) instead of its legacy binary protocol (9300/tcp by default). If you have a firewall between TheHive and Elasticsearch, you probably need to update its rules to change to the new port number. The configuration file ( application.conf ) needs some modifications to reflect the protocol change: The setting search.host is replaced by search.uri The general format of the URI is: http(s)://host:port,host:port(/prefix)?querystring . Multiple host:port combinations can be specified, separated by commas. Options can be specified using a standard URI query string syntax, eg. cluster.name=hive . The search.cluster setting is no longer used. Authentication can be configured with the search.user and search.password settings. When SSL/TLS is enabled, you can set a truststore and a keystore. The truststore contains the certificate authorities used to validate remote certificates. The keystore contains the certificate and the private key used to connect to the Elasticsearch cluster. The configuration is: search { keyStore { path: \"/path/to/keystore/file\" type: \"JKS\" # or PKCS12 password: \"secret.password.of.keystore\" } trustStore { path: \"/path/to/truststore/file\" type: \"JKS\" password: \"secret.password.of.truststore\" } } The Elasticsearch client also accepts the following settings: - circularRedirectsAllowed ( true / false ) - connectionRequestTimeout (number of seconds) - connectTimeout - contentCompressionEnabled.foreach(requestConfigBuilder.setContentCompressionEnabled) - search.cookieSpec (??) - expectContinueEnabled ( true / false ) - maxRedirects (number) - proxy -- not yet supported - proxyPreferredAuthSchemes -- not yet supported - redirectsEnabled ( true / false ) - relativeRedirectsAllowed ( true / false ) - socketTimeout (number of seconds) - targetPreferredAuthSchemes (??) The configuration items keepalive , pageSize , nbshards and nbreplicas are still valid. For practical details, you can have a look here for an example of migration of TheHive and Elasticsearch. From 3.0.x to 3.0.4 # TheHive 3.0.4 (Cerana 0.4) comes with new MISP settings to filter events that will be imported as alerts. Please refer to MISP event filters configuration section. The maximum number of custom fields and metrics in a case is 50 by default. If you try to put more, ElasticSearch will raise an error. You can now increase the limit by adding in your application.conf: index { settings { # Maximum number of nested fields mapping.nested_fields.limit = 100 } } The data schema has been changed in Cerana to support some dashboard features. At the first connection, TheHive will ask you to migrate the data. A new index, called the_hive_13 by default, will be created then. See Updating . From 2.13.x to 3.0.0 # The schema of data has been changed in Cerana to integrate dashboard. At the first request, TheHive will ask you to migrate the data. A new index, called the_hive_12 by default, will be created then. See Updating . From 2.13.0 or 2.13.1 to 2.13.2 # At the first connection to TheHive 2.13.2, a migration of the database will be asked. This will create a new ElasticSearch index ( the_hive_11 by default). See Updating . From 2.12.x to 2.13.x # Configuration updates # play.crypto.secret is deprecated, use play.http.secret.key instead. auth.type is deprecated, use auth.provider instead. Basic authentication is disabled by default. We strongly recommand to update the clients that rely on the API to interact with TheHive to use the new API key authentication method . This feature has been added in this release. If you need to enable basic authentication, use auth.method.basic=true in application.conf Note that the TheHive4Py 1.3.0 Python library also adds API key authentication support. Alert role # A new role \"alert\" has been added. Only users with this role can create an alert. If you have tool that uses TheHive API to create alerts, you must give the ability to do it in user administration. ElasticSearch # TheHive 2.13 uses ElasticSearch 5.x. Our tests have been done on ElasticSearch 5.5. So we recommend to use this specific version, even if TheHive should work perfectly with ElasticSearch 5.6 that doesn't introduce breaking changes. Data structure migration # Before upgrading ElasticSearch, backup all your indices . Then remove all indices except the last index of TheHive (most probably the_hive_10). You can list all indices with the following command: curl http://127.0.0.1:9200/_cat/indices ElasticSearch has changed the structure of its data directory (please refer to Path to data on disk ). The node name in the path where data are stored (DATA_DIR) must be removed. Stop ElasticSearch and execute the following lines to change the directory structure: echo -n 'Enter the path of ElasticSearch data: ' read DATA_DIR echo -n 'Enter the name of your cluster [hive]: ' read CLUSTER_NAME mv ${DATA_DIR}/${CLUSTER_NAME:=hive}/* ${DATA_DIR} rmdir ${DATA_DIR}/${CLUSTER_NAME} System requirements # ElasticSearch 5.x requires at least 262144 memory map areas (vm.max_map_count). Run sysctl -w vm.max_map_count=262144. To make this setting persistent after a server restart, add vm.max_map_count = 262144 in /etc/sysctl.conf (or to /etc/sysctl.d/80-elasticsearch.conf ) Configuration # The configuration of ElasticSearch should contain the following settings: http.host: 127.0.0.1 transport.host: 127.0.0.1 cluster.name: hive script.inline: true thread_pool.index.queue_size: 100000 thread_pool.search.queue_size: 100000 thread_pool.bulk.queue_size: 100000 Adapt http.host and transport.host to your environment. Docker # The default ElasticSearch image has been deprecated. It is recommended to use the docker image from Elastic.co . The new image doesn't use the same user ID so you need to change the owner of the data files. You can simply run chown -R 1000.1000 $DATA_DIR (DATA_DIR is the folder which contains ElasticSearch data). Then you can use the following script: docker run \\ --name elasticsearch \\ --hostname elasticsearch \\ --rm \\ --publish 127.0.0.1:9200:9200 \\ --publish 127.0.0.1:9300:9300 \\ --volume ***DATA_DIR***:/usr/share/elasticsearch/data \\ -e \"http.host=0.0.0.0\" \\ -e \"transport.host=0.0.0.0\" \\ -e \"xpack.security.enabled=false\" \\ -e \"cluster.name=hive\" \\ -e \"script.inline=true\" \\ -e \"thread_pool.index.queue_size=100000\" \\ -e \"thread_pool.search.queue_size=100000\" \\ -e \"thread_pool.bulk.queue_size=100000\" \\ docker.elastic.co/elasticsearch/elasticsearch:5.5.2 Note : TheHive doesn't support X-Pack. Don't enable it . Warnings You Can Safely Ignore with ES 5.5 # ElasticSearch 5.5 will output the following warnings: - unexpected docvalues type NONE for field '_parent' (expected one of [SORTED, SORTED_SET]). Re-index with correct docvalues type. You can safely ignore this message. For more information see issues #25849 and #26341 - License [will expire] on [***]. If you have a new license, please update it. Ignore this warning as TheHive doesn't use Elasticsearch's commercial features. Note : ElasticSearch 5.6 fixes those warnings. From 2.11.x to 2.12.x # Database migration # At the first connection to TheHive 2.12, a migration of the database will be asked. This will create a new ElasticSearch index (the_hive_10). See Updating . From 2.10.x to 2.11.x # Database migration # At the first connection to TheHive 2.11, a migration of the database will be asked. This will create a new ElastciSearch index (the_hive_9). See Updating . MISP to alert # MISP synchronization is now done using alerting framework. MISP events are seen like other alert. You can use TheHive4py to create your own alert. Configuration changes # MISP certificate authority deprecated # Specifying certificate authority in MISP configuration using \"cert\" key is now deprecated. You must replace it by - before: misp { [...] cert = \"/path/to/truststore.jks\" } - after: misp { [...] ws.ssl.trustManager.stores = [ { type: \"JKS\" path: \"/path/to/truststore.jks\" } ] } ws key can be placed in MISP server section or in global MISP section. In the latter, ws configuration will be applied on all MISP instances. Cortex and MISP HTTP client options # HTTP client used by Cortex and MISP is more configurable. Proxy can be configured, with or without authentication. Refer to configuration for all possible options. Packages # New RPM and DEB packages # RPM and DEB packages are now available. This makes the installation easier than using a binary package (ZIP). See the Installation Guide for reference. Docker # All-in-One docker (containing TheHive and Cortex) is not provided any longer. New TheHive docker image doesn't contain ElasticSearch. We recommend to use docker-compose to link TheHive, ElasticSearch and Cortex dockers. For more information, see the Installation Guide for reference. TheHive configuration is located in /etc/thehive/application.conf for all packages. If you use docker package you must update its location (previously was /opt/docker/conf/application.conf ).","title":"Migration guide"},{"location":"thehive/legacy/thehive3/migration-guide/#migration-guide","text":"","title":"Migration guide"},{"location":"thehive/legacy/thehive3/migration-guide/#from-34x-to-350","text":"Taking into account the EoL of version 6.x. of Elasticsearch, TheHive 3.5.0 is the first version to support Elasticsearch 7.x. This version introduce breaking changes. This time, we had no choice, we were not able to make TheHive support smoothly the ES upgrade. TheHive 3.5.0 supports Elasticsearch 7.x ONLY . This first steps before starting the upgrade process are: Identify the version of Elasticsearch which created your index Stop TheHive service Stop Elasticsearch service","title":"From 3.4.x to 3.5.0"},{"location":"thehive/legacy/thehive3/migration-guide/#how-to-identify-the-version-of-elasticsearch-which-created-your-database-index","text":"The software jq is required to manipulate JSON and create new indexes. More information at https://stedolan.github.io/jq/ . Run the following command : curl -s http://127.0.0.1:9200/the_hive_15?human | jq '.the_hive_15.settings.index.version.created_string' if the output is similar to \"5.x\" then your database index has been created with Elasticsearch 5.x reindexing is required, you should follow a dedicated process to upgrade . If it is \"6.x\" then your database has been created with Elasticsearch 6.","title":"How to identify the version of Elasticsearch which created your database index ?"},{"location":"thehive/legacy/thehive3/migration-guide/#your-database-was-created-with-elasticsearch-5x-or-earlier","text":"This is where things might be complicated. This upgrade progress requires handling the database index by updating parameters, and reindex before updating Elasticsearch, and updating TheHive. Read carefully the dedicated documentation . It should help you run this specific actions on your Elasticsearch database, and also install or update application whether you are using DEB, RPM or binary packages, and even docker images.","title":"Your database was created with Elasticsearch 5.x or earlier"},{"location":"thehive/legacy/thehive3/migration-guide/#your-database-was-created-with-elasticsearch-6x","text":"If you started using TheHive with Elasticsearch 6.x, then you just need to update the configuration of Elasticsearch to reflect this one: [..] http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive script.allowed_types: inline thread_pool.search.queue_size: 100000 thread_pool.write.queue_size: 10000 Following parameters * are not accepted anymore by Elasticsearch 7: thread_pool.index.queue_size thread_pool.bulk.queue_size With TheHive service stopped, ensure the new version of Elasticsearch starts. If everything is ok, then TheHive 3.5.0 can be installed. To run this operation successfully, you need to update your repository configuration if you are using DEB and RPM packages, or specify the right version to install if using docker. Read carefully the installation guide .","title":"Your database was created with Elasticsearch 6.x"},{"location":"thehive/legacy/thehive3/migration-guide/#from-33x-to-340","text":"Starting from version 3.4.0-RC1, TheHive supports Elasticsearch 6 and will continue to work with Elasticsearch 5.x. TheHive 3.4.0-RC1 and later versions communicate with Elasticsearch using its HTTP service (9200/tcp by default) instead of its legacy binary protocol (9300/tcp by default). If you have a firewall between TheHive and Elasticsearch, you probably need to update its rules to change to the new port number. The configuration file ( application.conf ) needs some modifications to reflect the protocol change: The setting search.host is replaced by search.uri The general format of the URI is: http(s)://host:port,host:port(/prefix)?querystring . Multiple host:port combinations can be specified, separated by commas. Options can be specified using a standard URI query string syntax, eg. cluster.name=hive . The search.cluster setting is no longer used. Authentication can be configured with the search.user and search.password settings. When SSL/TLS is enabled, you can set a truststore and a keystore. The truststore contains the certificate authorities used to validate remote certificates. The keystore contains the certificate and the private key used to connect to the Elasticsearch cluster. The configuration is: search { keyStore { path: \"/path/to/keystore/file\" type: \"JKS\" # or PKCS12 password: \"secret.password.of.keystore\" } trustStore { path: \"/path/to/truststore/file\" type: \"JKS\" password: \"secret.password.of.truststore\" } } The Elasticsearch client also accepts the following settings: - circularRedirectsAllowed ( true / false ) - connectionRequestTimeout (number of seconds) - connectTimeout - contentCompressionEnabled.foreach(requestConfigBuilder.setContentCompressionEnabled) - search.cookieSpec (??) - expectContinueEnabled ( true / false ) - maxRedirects (number) - proxy -- not yet supported - proxyPreferredAuthSchemes -- not yet supported - redirectsEnabled ( true / false ) - relativeRedirectsAllowed ( true / false ) - socketTimeout (number of seconds) - targetPreferredAuthSchemes (??) The configuration items keepalive , pageSize , nbshards and nbreplicas are still valid. For practical details, you can have a look here for an example of migration of TheHive and Elasticsearch.","title":"From 3.3.x to 3.4.0"},{"location":"thehive/legacy/thehive3/migration-guide/#from-30x-to-304","text":"TheHive 3.0.4 (Cerana 0.4) comes with new MISP settings to filter events that will be imported as alerts. Please refer to MISP event filters configuration section. The maximum number of custom fields and metrics in a case is 50 by default. If you try to put more, ElasticSearch will raise an error. You can now increase the limit by adding in your application.conf: index { settings { # Maximum number of nested fields mapping.nested_fields.limit = 100 } } The data schema has been changed in Cerana to support some dashboard features. At the first connection, TheHive will ask you to migrate the data. A new index, called the_hive_13 by default, will be created then. See Updating .","title":"From 3.0.x to 3.0.4"},{"location":"thehive/legacy/thehive3/migration-guide/#from-213x-to-300","text":"The schema of data has been changed in Cerana to integrate dashboard. At the first request, TheHive will ask you to migrate the data. A new index, called the_hive_12 by default, will be created then. See Updating .","title":"From 2.13.x to 3.0.0"},{"location":"thehive/legacy/thehive3/migration-guide/#from-2130-or-2131-to-2132","text":"At the first connection to TheHive 2.13.2, a migration of the database will be asked. This will create a new ElasticSearch index ( the_hive_11 by default). See Updating .","title":"From 2.13.0 or 2.13.1 to 2.13.2"},{"location":"thehive/legacy/thehive3/migration-guide/#from-212x-to-213x","text":"","title":"From 2.12.x to 2.13.x"},{"location":"thehive/legacy/thehive3/migration-guide/#configuration-updates","text":"play.crypto.secret is deprecated, use play.http.secret.key instead. auth.type is deprecated, use auth.provider instead. Basic authentication is disabled by default. We strongly recommand to update the clients that rely on the API to interact with TheHive to use the new API key authentication method . This feature has been added in this release. If you need to enable basic authentication, use auth.method.basic=true in application.conf Note that the TheHive4Py 1.3.0 Python library also adds API key authentication support.","title":"Configuration updates"},{"location":"thehive/legacy/thehive3/migration-guide/#alert-role","text":"A new role \"alert\" has been added. Only users with this role can create an alert. If you have tool that uses TheHive API to create alerts, you must give the ability to do it in user administration.","title":"Alert role"},{"location":"thehive/legacy/thehive3/migration-guide/#elasticsearch","text":"TheHive 2.13 uses ElasticSearch 5.x. Our tests have been done on ElasticSearch 5.5. So we recommend to use this specific version, even if TheHive should work perfectly with ElasticSearch 5.6 that doesn't introduce breaking changes.","title":"ElasticSearch"},{"location":"thehive/legacy/thehive3/migration-guide/#data-structure-migration","text":"Before upgrading ElasticSearch, backup all your indices . Then remove all indices except the last index of TheHive (most probably the_hive_10). You can list all indices with the following command: curl http://127.0.0.1:9200/_cat/indices ElasticSearch has changed the structure of its data directory (please refer to Path to data on disk ). The node name in the path where data are stored (DATA_DIR) must be removed. Stop ElasticSearch and execute the following lines to change the directory structure: echo -n 'Enter the path of ElasticSearch data: ' read DATA_DIR echo -n 'Enter the name of your cluster [hive]: ' read CLUSTER_NAME mv ${DATA_DIR}/${CLUSTER_NAME:=hive}/* ${DATA_DIR} rmdir ${DATA_DIR}/${CLUSTER_NAME}","title":"Data structure migration"},{"location":"thehive/legacy/thehive3/migration-guide/#system-requirements","text":"ElasticSearch 5.x requires at least 262144 memory map areas (vm.max_map_count). Run sysctl -w vm.max_map_count=262144. To make this setting persistent after a server restart, add vm.max_map_count = 262144 in /etc/sysctl.conf (or to /etc/sysctl.d/80-elasticsearch.conf )","title":"System requirements"},{"location":"thehive/legacy/thehive3/migration-guide/#configuration","text":"The configuration of ElasticSearch should contain the following settings: http.host: 127.0.0.1 transport.host: 127.0.0.1 cluster.name: hive script.inline: true thread_pool.index.queue_size: 100000 thread_pool.search.queue_size: 100000 thread_pool.bulk.queue_size: 100000 Adapt http.host and transport.host to your environment.","title":"Configuration"},{"location":"thehive/legacy/thehive3/migration-guide/#docker","text":"The default ElasticSearch image has been deprecated. It is recommended to use the docker image from Elastic.co . The new image doesn't use the same user ID so you need to change the owner of the data files. You can simply run chown -R 1000.1000 $DATA_DIR (DATA_DIR is the folder which contains ElasticSearch data). Then you can use the following script: docker run \\ --name elasticsearch \\ --hostname elasticsearch \\ --rm \\ --publish 127.0.0.1:9200:9200 \\ --publish 127.0.0.1:9300:9300 \\ --volume ***DATA_DIR***:/usr/share/elasticsearch/data \\ -e \"http.host=0.0.0.0\" \\ -e \"transport.host=0.0.0.0\" \\ -e \"xpack.security.enabled=false\" \\ -e \"cluster.name=hive\" \\ -e \"script.inline=true\" \\ -e \"thread_pool.index.queue_size=100000\" \\ -e \"thread_pool.search.queue_size=100000\" \\ -e \"thread_pool.bulk.queue_size=100000\" \\ docker.elastic.co/elasticsearch/elasticsearch:5.5.2 Note : TheHive doesn't support X-Pack. Don't enable it .","title":"Docker"},{"location":"thehive/legacy/thehive3/migration-guide/#warnings-you-can-safely-ignore-with-es-55","text":"ElasticSearch 5.5 will output the following warnings: - unexpected docvalues type NONE for field '_parent' (expected one of [SORTED, SORTED_SET]). Re-index with correct docvalues type. You can safely ignore this message. For more information see issues #25849 and #26341 - License [will expire] on [***]. If you have a new license, please update it. Ignore this warning as TheHive doesn't use Elasticsearch's commercial features. Note : ElasticSearch 5.6 fixes those warnings.","title":"Warnings You Can Safely Ignore with ES 5.5"},{"location":"thehive/legacy/thehive3/migration-guide/#from-211x-to-212x","text":"","title":"From 2.11.x to 2.12.x"},{"location":"thehive/legacy/thehive3/migration-guide/#database-migration","text":"At the first connection to TheHive 2.12, a migration of the database will be asked. This will create a new ElasticSearch index (the_hive_10). See Updating .","title":"Database migration"},{"location":"thehive/legacy/thehive3/migration-guide/#from-210x-to-211x","text":"","title":"From 2.10.x to 2.11.x"},{"location":"thehive/legacy/thehive3/migration-guide/#database-migration_1","text":"At the first connection to TheHive 2.11, a migration of the database will be asked. This will create a new ElastciSearch index (the_hive_9). See Updating .","title":"Database migration"},{"location":"thehive/legacy/thehive3/migration-guide/#misp-to-alert","text":"MISP synchronization is now done using alerting framework. MISP events are seen like other alert. You can use TheHive4py to create your own alert.","title":"MISP to alert"},{"location":"thehive/legacy/thehive3/migration-guide/#configuration-changes","text":"","title":"Configuration changes"},{"location":"thehive/legacy/thehive3/migration-guide/#misp-certificate-authority-deprecated","text":"Specifying certificate authority in MISP configuration using \"cert\" key is now deprecated. You must replace it by - before: misp { [...] cert = \"/path/to/truststore.jks\" } - after: misp { [...] ws.ssl.trustManager.stores = [ { type: \"JKS\" path: \"/path/to/truststore.jks\" } ] } ws key can be placed in MISP server section or in global MISP section. In the latter, ws configuration will be applied on all MISP instances.","title":"MISP certificate authority deprecated"},{"location":"thehive/legacy/thehive3/migration-guide/#cortex-and-misp-http-client-options","text":"HTTP client used by Cortex and MISP is more configurable. Proxy can be configured, with or without authentication. Refer to configuration for all possible options.","title":"Cortex and MISP HTTP client options"},{"location":"thehive/legacy/thehive3/migration-guide/#packages","text":"","title":"Packages"},{"location":"thehive/legacy/thehive3/migration-guide/#new-rpm-and-deb-packages","text":"RPM and DEB packages are now available. This makes the installation easier than using a binary package (ZIP). See the Installation Guide for reference.","title":"New RPM and DEB packages"},{"location":"thehive/legacy/thehive3/migration-guide/#docker_1","text":"All-in-One docker (containing TheHive and Cortex) is not provided any longer. New TheHive docker image doesn't contain ElasticSearch. We recommend to use docker-compose to link TheHive, ElasticSearch and Cortex dockers. For more information, see the Installation Guide for reference. TheHive configuration is located in /etc/thehive/application.conf for all packages. If you use docker package you must update its location (previously was /opt/docker/conf/application.conf ).","title":"Docker"},{"location":"thehive/legacy/thehive3/admin/admin-guide/","text":"Administrator's guide # 1. User management # Users can be managed through the Administration > Users page. Only administrators may access it. Each user is identified by their login, full name and role. Please note that you still need to create user accounts if you use LDAP or Active Directory authentication. This is necessary for TheHive to retrieve their role and authenticate them against the local database, LDAP and/or AD directories. There are 4 roles currently: - read : all non-sensitive data can be read. With this role, a user can't make any change. They can't add a case, task, log or observable. They also can't run analyzers; - write : create, remove and change data of any type. This role is for standard users. write role inherits read rights; - admin : this role is reserved for TheHive administrators. Users with this role can manage user accounts, metrics, create case templates and observable data types. admin inherits write rights; - alert : users with this role can only create alerts. Warning : Please note that user accounts cannot be removed once they have been created, otherwise audit logs will refer to an unknown user. However, unwanted or unused accounts can be locked. 2. Case template management # Some cases may share the same structure (tags, tasks, description, metrics). Templates are here to automatically add tasks, description, metrics and custom fields while creating a new case. A user can choose to create an empty case or based on a registered template. To create a template, as admin go into the administration menu, and open the \"Case templates\" item. In this screen, you can add, remove or change template. A template contains: * default severity * default tags * title prefix (can be changed by user at case creation) * default TLP * default default * task list (title and description) * metrics * custom fields Except for title prefix, task list and metrics, the user can change values defined in template. 3. Report template management # When TheHive is connected to a Cortex server, observables can be analyzed to get additional information on them. Cortex outputs reports in JSON format. In order to make reports more readable, you can configure report templates. Report templates convert JSON in to HTML using the AngularJS template engine. For each analyzer available in Cortex you can define two kinds of templates: short and long. A short report exposes synthetic information, shows in top of observable page. With short reports you can see a summary of all run analyzers. Long reports show detailed information only when the user selects the report. Raw data in JSON format is always available. Report templates can be configured in the Admin > Report templates menu. We offer report templates for default Cortex analyzers. A package with all report templates can be downloaded at https://download.thehive-project.org/report-templates.zip and can be injected using the Import templates button. 4. Metrics management # Metrics have been integrated to have relevant indicators about cases. Metrics are numerical values associated to cases (for example, the number of impacted users). Each metric has a name , a title and a description , defined by an administrator. When a metric is added to a case, it can't be removed and must be filled. Metrics are used to monitor business indicators, thanks to graphs. Metrics are defined globally. To create metrics, as admin go into the administration menu, and open the \"Case metrics\" item. Metrics are used to create statistics (\"Statistics\" item in the user profile menu). They can be filtered on time interval, and case with specific tags. For example you can show metrics of case with \"malspam\" tag on January 2016 : For graphs based on time, the user can choose metrics to show. They are aggregated on interval of time (by day, week, month of year) using a function (sum, min or max). Some metrics are predefined (in addition to those defined by administrator) like case handling duration (how much time the case had been open) and number of cases open or closed.","title":"Administrator's guide"},{"location":"thehive/legacy/thehive3/admin/admin-guide/#administrators-guide","text":"","title":"Administrator's guide"},{"location":"thehive/legacy/thehive3/admin/admin-guide/#1-user-management","text":"Users can be managed through the Administration > Users page. Only administrators may access it. Each user is identified by their login, full name and role. Please note that you still need to create user accounts if you use LDAP or Active Directory authentication. This is necessary for TheHive to retrieve their role and authenticate them against the local database, LDAP and/or AD directories. There are 4 roles currently: - read : all non-sensitive data can be read. With this role, a user can't make any change. They can't add a case, task, log or observable. They also can't run analyzers; - write : create, remove and change data of any type. This role is for standard users. write role inherits read rights; - admin : this role is reserved for TheHive administrators. Users with this role can manage user accounts, metrics, create case templates and observable data types. admin inherits write rights; - alert : users with this role can only create alerts. Warning : Please note that user accounts cannot be removed once they have been created, otherwise audit logs will refer to an unknown user. However, unwanted or unused accounts can be locked.","title":"1. User management"},{"location":"thehive/legacy/thehive3/admin/admin-guide/#2-case-template-management","text":"Some cases may share the same structure (tags, tasks, description, metrics). Templates are here to automatically add tasks, description, metrics and custom fields while creating a new case. A user can choose to create an empty case or based on a registered template. To create a template, as admin go into the administration menu, and open the \"Case templates\" item. In this screen, you can add, remove or change template. A template contains: * default severity * default tags * title prefix (can be changed by user at case creation) * default TLP * default default * task list (title and description) * metrics * custom fields Except for title prefix, task list and metrics, the user can change values defined in template.","title":"2. Case template management"},{"location":"thehive/legacy/thehive3/admin/admin-guide/#3-report-template-management","text":"When TheHive is connected to a Cortex server, observables can be analyzed to get additional information on them. Cortex outputs reports in JSON format. In order to make reports more readable, you can configure report templates. Report templates convert JSON in to HTML using the AngularJS template engine. For each analyzer available in Cortex you can define two kinds of templates: short and long. A short report exposes synthetic information, shows in top of observable page. With short reports you can see a summary of all run analyzers. Long reports show detailed information only when the user selects the report. Raw data in JSON format is always available. Report templates can be configured in the Admin > Report templates menu. We offer report templates for default Cortex analyzers. A package with all report templates can be downloaded at https://download.thehive-project.org/report-templates.zip and can be injected using the Import templates button.","title":"3. Report template management"},{"location":"thehive/legacy/thehive3/admin/admin-guide/#4-metrics-management","text":"Metrics have been integrated to have relevant indicators about cases. Metrics are numerical values associated to cases (for example, the number of impacted users). Each metric has a name , a title and a description , defined by an administrator. When a metric is added to a case, it can't be removed and must be filled. Metrics are used to monitor business indicators, thanks to graphs. Metrics are defined globally. To create metrics, as admin go into the administration menu, and open the \"Case metrics\" item. Metrics are used to create statistics (\"Statistics\" item in the user profile menu). They can be filtered on time interval, and case with specific tags. For example you can show metrics of case with \"malspam\" tag on January 2016 : For graphs based on time, the user can choose metrics to show. They are aggregated on interval of time (by day, week, month of year) using a function (sum, min or max). Some metrics are predefined (in addition to those defined by administrator) like case handling duration (how much time the case had been open) and number of cases open or closed.","title":"4. Metrics management"},{"location":"thehive/legacy/thehive3/admin/backup-restore/","text":"Backup and restore data # All persistent data is stored in an Elasticsearch database. The backup and restore procedures are the ones that are detailed in Elasticsearch documentation . Note : you may have to adapt your indices in the examples below. To find the right index, use the following command : curl 'localhost:9200/_cat/indices?v' You can also refer to the schema version page. To save all your data you only need to backup the last indice. For example, if the previous command gives you the following results, all your data belongs to the_hive_12 . health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open the_hive_11 HVVYDC68SrGAfSbcjVPZWg 5 1 43018 17 24.9mb 24.9mb yellow open the_hive_12 Cq4Gc4qkRPaTCqrorFgDRw 5 1 43226 0 25.3mb 25.3mb In the rest of this document, ensure to change to your own last index in order to backup or restore all your data. 1. Create a backup repository # First you must define a location in local filesystem (where Elasticsearch instance runs) where the backup will be written. This repository must be declared in the Elasticsearch configuration. Edit elasticsearch.yml file by adding: path.repo: [\"/absolute/path/to/backup/directory\"] Then, restart the Elasticsearch service. Note : Be careful if you run Elasticsearch in Docker, the directory must be mapped in host filesystem using --volume parameter (cf. Docker documentation ). 2. Register a snapshot repository # Create an Elasticsearch snapshot point named the_hive_backup with the following command (set the same path in the location setting as the one set in the configuration file): $ curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -H 'Content-Type: application/json' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/absolute/path/to/backup/directory\", \"compress\": true } }' The result of the command should look like this : {\"acknowledged\":true} Since, everything is fine to backup and restore data. 3. Backup your data # Create a backup named snapshot_1 of all your data by executing the following command : $ curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1?wait_for_completion=true&pretty' -H 'Content-Type: application/json' -d '{ \"indices\": \"<INDEX>\" }' This command terminates only when the backup is complete and the result of the command should look like this: { \"snapshots\": [{ \"snapshot\": \"snapshot_1\", \"uuid\": \"ZQ3kv5-FQoeN3NFIhfKgMg\", \"version_id\": 5060099, \"version\": \"5.6.0\", \"indices\": [\"the_hive_12\"], \"state\": \"SUCCESS\", \"start_time\": \"2018-01-29T14:41:51.580Z\", \"start_time_in_millis\": 1517236911580, \"end_time\": \"2018-01-29T14:42:05.216Z\", \"end_time_in_millis\": 1517236925216, \"duration_in_millis\": 13636, \"failures\": [], \"shards\": { \"total\": 41, \"failed\": 0, \"successful\": 41 } }] } Note : You can backup the last index of TheHive (you can list indices in your Elasticsearch cluster with curl -s http://localhost:9200/_cat/indices | cut -d ' ' -f3 ) or all indices with _all value. 4. Restore data # Restore will do the reverse actions : it reads the backup in your snapshot directory and loads indices into the Elasticsearch cluster. This operation is done with the following command : $ curl -XPOST 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1/_restore' -d ' { \"indices\": \"<INDEX>\" }' The result of the command should look like this : {\"accepted\":true} Note : be sure to restore data from the same version of Elasticsearch. 5. Moving data from one server to another # If you want to move your data from one server from another: - Create your backup on the origin server (steps 1 , 2 , 3 ) - copy your backup directory from the origin server to the destination server - On the destination server : - Register your backup repository in the Elasticsearch configuration (step 1 ) - Register your snapshot repository with the same snapshot name (step 2 ) - Restore your data (step 4 )","title":"Backup and restore data"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#backup-and-restore-data","text":"All persistent data is stored in an Elasticsearch database. The backup and restore procedures are the ones that are detailed in Elasticsearch documentation . Note : you may have to adapt your indices in the examples below. To find the right index, use the following command : curl 'localhost:9200/_cat/indices?v' You can also refer to the schema version page. To save all your data you only need to backup the last indice. For example, if the previous command gives you the following results, all your data belongs to the_hive_12 . health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open the_hive_11 HVVYDC68SrGAfSbcjVPZWg 5 1 43018 17 24.9mb 24.9mb yellow open the_hive_12 Cq4Gc4qkRPaTCqrorFgDRw 5 1 43226 0 25.3mb 25.3mb In the rest of this document, ensure to change to your own last index in order to backup or restore all your data.","title":"Backup and restore data"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#1-create-a-backup-repository","text":"First you must define a location in local filesystem (where Elasticsearch instance runs) where the backup will be written. This repository must be declared in the Elasticsearch configuration. Edit elasticsearch.yml file by adding: path.repo: [\"/absolute/path/to/backup/directory\"] Then, restart the Elasticsearch service. Note : Be careful if you run Elasticsearch in Docker, the directory must be mapped in host filesystem using --volume parameter (cf. Docker documentation ).","title":"1. Create a backup repository"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#2-register-a-snapshot-repository","text":"Create an Elasticsearch snapshot point named the_hive_backup with the following command (set the same path in the location setting as the one set in the configuration file): $ curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -H 'Content-Type: application/json' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/absolute/path/to/backup/directory\", \"compress\": true } }' The result of the command should look like this : {\"acknowledged\":true} Since, everything is fine to backup and restore data.","title":"2. Register a snapshot repository"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#3-backup-your-data","text":"Create a backup named snapshot_1 of all your data by executing the following command : $ curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1?wait_for_completion=true&pretty' -H 'Content-Type: application/json' -d '{ \"indices\": \"<INDEX>\" }' This command terminates only when the backup is complete and the result of the command should look like this: { \"snapshots\": [{ \"snapshot\": \"snapshot_1\", \"uuid\": \"ZQ3kv5-FQoeN3NFIhfKgMg\", \"version_id\": 5060099, \"version\": \"5.6.0\", \"indices\": [\"the_hive_12\"], \"state\": \"SUCCESS\", \"start_time\": \"2018-01-29T14:41:51.580Z\", \"start_time_in_millis\": 1517236911580, \"end_time\": \"2018-01-29T14:42:05.216Z\", \"end_time_in_millis\": 1517236925216, \"duration_in_millis\": 13636, \"failures\": [], \"shards\": { \"total\": 41, \"failed\": 0, \"successful\": 41 } }] } Note : You can backup the last index of TheHive (you can list indices in your Elasticsearch cluster with curl -s http://localhost:9200/_cat/indices | cut -d ' ' -f3 ) or all indices with _all value.","title":"3. Backup your data"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#4-restore-data","text":"Restore will do the reverse actions : it reads the backup in your snapshot directory and loads indices into the Elasticsearch cluster. This operation is done with the following command : $ curl -XPOST 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1/_restore' -d ' { \"indices\": \"<INDEX>\" }' The result of the command should look like this : {\"accepted\":true} Note : be sure to restore data from the same version of Elasticsearch.","title":"4. Restore data"},{"location":"thehive/legacy/thehive3/admin/backup-restore/#5-moving-data-from-one-server-to-another","text":"If you want to move your data from one server from another: - Create your backup on the origin server (steps 1 , 2 , 3 ) - copy your backup directory from the origin server to the destination server - On the destination server : - Register your backup repository in the Elasticsearch configuration (step 1 ) - Register your snapshot repository with the same snapshot name (step 2 ) - Restore your data (step 4 )","title":"5. Moving data from one server to another"},{"location":"thehive/legacy/thehive3/admin/certauth/","text":"Single Sign-On on TheHive with X.509 Certificates # Abstract # SSL managed by TheHive is known to have some stability problem. It is advise to not enable it in production and configure SSL on a reverse proxy, in front of TheHive. This make X509 certificate authentication non applicable. In order to do x509 authentication it is recommended to do it in the reverse proxy and then forward user identity to TheHive in a HTTP header. This feature has been added in version 3.2. WARNING This setup is valid only if nobody except the reverse proxy can connect to TheHive. Users must have to use the reverse proxy. Otherwise, an user would be able to choose his identity on TheHive. Setup a reverse proxy # If you use nginx, the site configuration file should look like: server { listen 443 ssl; server_name thehive.example.com; ssl on; ssl_certificate ssl/thehive_cert.pem; ssl_certificate_key ssl/thehive_key.pem; # Force client to have a certificate ssl_verify_client on; proxy_connect_timeout 600; proxy_send_timeout 600; proxy_read_timeout 600; send_timeout 600; client_max_body_size 2G; proxy_buffering off; client_header_buffer_size 8k; # Map certificate DN to user login stored in TheHive map $ssl_client_s_dn $thehive_user { default \"\"; /C=FR/O=TheHive-Project/CN=Thomas toom; /C=FR/O=TheHive-Project/CN=Georges bofh; }; # Redirect all request to local TheHive location / { add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; # Send the mapped user login to TheHive, in THEHIVE_USER HTTP header proxy_set_header THEHIVE_USER $thehive_user; proxy_pass http://127.0.0.1:9000/; proxy_http_version 1.1; } } Enable authentication delegation in TheHive # Setup TheHive to identify user by the configured HTTP header (THEHIVE_USER): auth { method.header = true header.name = THEHIVE_USER } # Listen only on localhost to prevent direct access to TheHive http.address=127.0.0.1","title":"Single Sign-On on TheHive with X.509 Certificates"},{"location":"thehive/legacy/thehive3/admin/certauth/#single-sign-on-on-thehive-with-x509-certificates","text":"","title":"Single Sign-On on TheHive with X.509 Certificates"},{"location":"thehive/legacy/thehive3/admin/certauth/#abstract","text":"SSL managed by TheHive is known to have some stability problem. It is advise to not enable it in production and configure SSL on a reverse proxy, in front of TheHive. This make X509 certificate authentication non applicable. In order to do x509 authentication it is recommended to do it in the reverse proxy and then forward user identity to TheHive in a HTTP header. This feature has been added in version 3.2. WARNING This setup is valid only if nobody except the reverse proxy can connect to TheHive. Users must have to use the reverse proxy. Otherwise, an user would be able to choose his identity on TheHive.","title":"Abstract"},{"location":"thehive/legacy/thehive3/admin/certauth/#setup-a-reverse-proxy","text":"If you use nginx, the site configuration file should look like: server { listen 443 ssl; server_name thehive.example.com; ssl on; ssl_certificate ssl/thehive_cert.pem; ssl_certificate_key ssl/thehive_key.pem; # Force client to have a certificate ssl_verify_client on; proxy_connect_timeout 600; proxy_send_timeout 600; proxy_read_timeout 600; send_timeout 600; client_max_body_size 2G; proxy_buffering off; client_header_buffer_size 8k; # Map certificate DN to user login stored in TheHive map $ssl_client_s_dn $thehive_user { default \"\"; /C=FR/O=TheHive-Project/CN=Thomas toom; /C=FR/O=TheHive-Project/CN=Georges bofh; }; # Redirect all request to local TheHive location / { add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; # Send the mapped user login to TheHive, in THEHIVE_USER HTTP header proxy_set_header THEHIVE_USER $thehive_user; proxy_pass http://127.0.0.1:9000/; proxy_http_version 1.1; } }","title":"Setup a reverse proxy"},{"location":"thehive/legacy/thehive3/admin/certauth/#enable-authentication-delegation-in-thehive","text":"Setup TheHive to identify user by the configured HTTP header (THEHIVE_USER): auth { method.header = true header.name = THEHIVE_USER } # Listen only on localhost to prevent direct access to TheHive http.address=127.0.0.1","title":"Enable authentication delegation in TheHive"},{"location":"thehive/legacy/thehive3/admin/cluster/","text":"Cluster Configuration # Starting from version 3.1.0, TheHive can scale horizontally very easily. You can dynamically add nodes to your cluster to increase the performance of the platform. TheHive API is stateless to the exclusion of the stream (or real-time flow). For this reason, the cluster nodes need to communicate with each other. The first node of the cluster has a specific role: it must initiate the cluster creation. Any additional node only needs to contact at least one node of the cluster to join it. This is done by configuring so-called seed nodes . The first node must have itself in the seed node list. The other nodes must have at least one entry corresponding to a node that has already joined the seed node list. Note : all cluster nodes must share the same secret ( play.http.secret.key in application.conf ). Configuration # Define node1 (for example with IP address 10.0.0.1 ) as the first node of the cluster. The configuration section in application.conf should look like the following: akka { remote { netty.tcp { hostname = \"10.0.0.1\" port = 2552 } } # seed node is itself as it is the first node of the cluster cluster.seed-nodes = [\"akka.tcp://application@10.0.0.1:2552\"] } Then add another node. Let's call it node2 and assume its IP address is 10.0.0.2 to our one-node cluster. You can see that it is referring to the first node in cluster.seed-nodes : akka { remote { netty.tcp { hostname = \"10.0.0.2\" port = 2552 } } # seed node list contains at least one active node cluster.seed-nodes = [\"akka.tcp://application@10.0.0.1:2552\"] } We recommend defining several seed nodes in the respective configuration files, except for the first one. For example: node configured seed nodes node1 node1 node2 node1, node3 node3 node2, node4 node4 node1, node2, node3 Load Balancing # In front of TheHive cluster, you can add a load balancer which distributes HTTP requests to cluster nodes. One client does not need to always use the same node as affinity is not required. Below is an non-optimized example of a haproxy configuration: # Global standard configuration, nothing specific for TheHive global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin stats timeout 30s user haproxy group haproxy daemon ca-base /etc/ssl/certs crt-base /etc/ssl/private ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3 defaults log global mode http option httplog option dontlognull timeout connect 500 timeout client 50000 # server timeout must be at least the stream.refresh parameter in application.conf timeout server 2m errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http # Listen on all interfaces, on port 9000/tcp frontend http-in bind *:9000 default_backend servers # Configure all cluster node backend servers balance roundrobin server node1 10.0.0.1:9000 check server node2 10.0.0.2:9000 check server node3 10.0.0.3:9000 check server node4 10.0.0.4:9000 check Troubleshooting # Should you encounter troubles with your setup, you can enable debug messages with the following configuration: akka { actor { debug { receive = on autoreceive = on lifecycle = on unhandled = on } } } Additional Information # TheHive Leverages Akka Cluster. You can refer to the Akka documentation for additional information.","title":"Cluster Configuration"},{"location":"thehive/legacy/thehive3/admin/cluster/#cluster-configuration","text":"Starting from version 3.1.0, TheHive can scale horizontally very easily. You can dynamically add nodes to your cluster to increase the performance of the platform. TheHive API is stateless to the exclusion of the stream (or real-time flow). For this reason, the cluster nodes need to communicate with each other. The first node of the cluster has a specific role: it must initiate the cluster creation. Any additional node only needs to contact at least one node of the cluster to join it. This is done by configuring so-called seed nodes . The first node must have itself in the seed node list. The other nodes must have at least one entry corresponding to a node that has already joined the seed node list. Note : all cluster nodes must share the same secret ( play.http.secret.key in application.conf ).","title":"Cluster Configuration"},{"location":"thehive/legacy/thehive3/admin/cluster/#configuration","text":"Define node1 (for example with IP address 10.0.0.1 ) as the first node of the cluster. The configuration section in application.conf should look like the following: akka { remote { netty.tcp { hostname = \"10.0.0.1\" port = 2552 } } # seed node is itself as it is the first node of the cluster cluster.seed-nodes = [\"akka.tcp://application@10.0.0.1:2552\"] } Then add another node. Let's call it node2 and assume its IP address is 10.0.0.2 to our one-node cluster. You can see that it is referring to the first node in cluster.seed-nodes : akka { remote { netty.tcp { hostname = \"10.0.0.2\" port = 2552 } } # seed node list contains at least one active node cluster.seed-nodes = [\"akka.tcp://application@10.0.0.1:2552\"] } We recommend defining several seed nodes in the respective configuration files, except for the first one. For example: node configured seed nodes node1 node1 node2 node1, node3 node3 node2, node4 node4 node1, node2, node3","title":"Configuration"},{"location":"thehive/legacy/thehive3/admin/cluster/#load-balancing","text":"In front of TheHive cluster, you can add a load balancer which distributes HTTP requests to cluster nodes. One client does not need to always use the same node as affinity is not required. Below is an non-optimized example of a haproxy configuration: # Global standard configuration, nothing specific for TheHive global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin stats timeout 30s user haproxy group haproxy daemon ca-base /etc/ssl/certs crt-base /etc/ssl/private ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3 defaults log global mode http option httplog option dontlognull timeout connect 500 timeout client 50000 # server timeout must be at least the stream.refresh parameter in application.conf timeout server 2m errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http # Listen on all interfaces, on port 9000/tcp frontend http-in bind *:9000 default_backend servers # Configure all cluster node backend servers balance roundrobin server node1 10.0.0.1:9000 check server node2 10.0.0.2:9000 check server node3 10.0.0.3:9000 check server node4 10.0.0.4:9000 check","title":"Load Balancing"},{"location":"thehive/legacy/thehive3/admin/cluster/#troubleshooting","text":"Should you encounter troubles with your setup, you can enable debug messages with the following configuration: akka { actor { debug { receive = on autoreceive = on lifecycle = on unhandled = on } } }","title":"Troubleshooting"},{"location":"thehive/legacy/thehive3/admin/cluster/#additional-information","text":"TheHive Leverages Akka Cluster. You can refer to the Akka documentation for additional information.","title":"Additional Information"},{"location":"thehive/legacy/thehive3/admin/configuration/","text":"Configuration Guide # The configuration file of TheHive is /etc/thehive/application.conf by default. This file uses the HOCON format . All configuration parameters should go in this file. You can have a look at the default settings . Table of Contents # 1. Database 2. Datastore 3. Authentication 3.1 LDAP/AD 3.2 OAuth2/OpenID Connect 4. Streaming (a.k.a The Flow) 5. Entity size limit 6. Cortex 7. MISP 7.1 Configuration 7.2 Associate a Case Template to Alerts corresponding to MISP events 7.3 Event Filters 7.4 MISP Purpose 8. HTTP Client Configuration 9. Monitoring and Performance Metrics (deprecated) 10. HTTPS 10.1 HTTPS using a reverse proxy 10.2 HTTPS without reverse proxy 10.3 Strengthen security 1. Database # TheHive uses the Elasticsearch search engine to store all persistent data. Elasticsearch is not part of TheHive package. It must be installed and configured as a standalone instance which can be located on the same machine. For more information on how to set up Elasticsearch, please refer to Elasticsearch installation guide . Three settings are required to connect to Elasticsearch: * the base name of the index * the name of the cluster * the address(es) and port(s) of the Elasticsearch instance The Defaults settings are: # Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch instance address. uri = \"http://127.0.0.1:9200/\" # Scroll keepalive keepalive = 1m # Size of the page for scroll pagesize = 50 # Number of shards nbshards = 5 # Number of replicas nbreplicas = 1 # Arbitrary settings settings { # Maximum number of nested fields mapping.nested_fields.limit = 100 } ## Authentication configuration #search.username = \"\" #search.password = \"\" ## SSL configuration #search.keyStore { # path = \"/path/to/keystore\" # type = \"JKS\" # or PKCS12 # password = \"keystore-password\" #} #search.trustStore { # path = \"/path/to/trustStore\" # type = \"JKS\" # or PKCS12 # password = \"trustStore-password\" #} } If you use a different configuration, modify the parameters accordingly in the application.conf file. If multiple Elasticsearch nodes are used as a cluster, you should add addresses of the master nodes in the url like this: search { uri = http://node1:9200,node2:9200/ ... TheHive uses the http port of Elasticsearch (9200/tcp by default). TheHive versions index schema (mapping) in Elasticsearch. Version numbers are appended to the index base name (the 8th version of the schema uses the index the_hive_8 if search.index = the_hive ). When too many documents are requested to TheHive, it uses the scroll feature: the results are retrieved through pagination. You can specify the size of the page ( search.pagesize ) and how long pages are kept in Elasticsearch (( search.keepalive ) before purging. 2. Datastore # TheHive stores attachments as Elasticsearch documents. They are split in chunks and each chunk sent to Elasticsearch is identified by the hash of the entire attachment and the associated chunk number. The chunk size ( datastore.chunksize ) can be changed but any change will only affect new attachments. Existing ones won't be changed. An attachment is identified by its hash. The algorithm used is configurable ( datastore.hash.main ) but must not be changed after the first attachment insertion. Otherwise, previous files cannot be retrieved. Extra hash algorithms can be configured using datastore.hash.extra . These hashes are not used to identify the attachment but are shown in the user interface (the hash associated to the main algorithm is also shown). If you change extra algorithms, you should inform TheHive and ask it to recompute all hashes. Please note that the associated API call is currently disabled in Buckfast (v 2.10). It will be reinstated in the next release. Observables can contain malicious data. When you try to download an attachment from an observable (typically a file), it is automatically zipped and the resulting ZIP file is password-protected. The default password is malware but it can be changed with the datastore.attachment.password setting. Default values are: # Datastore datastore { name = data # Size of stored data chunks chunksize = 50k hash { # Main hash algorithm /!\\ Don't change this value main = \"SHA-256\" # Additional hash algorithms (used in attachments) extra = [\"SHA-1\", \"MD5\"] } attachment.password = \"malware\" } 3. Authentication # TheHive supports local, LDAP, Active Directory (AD) or OAuth2/OpenID Connect for authentication. By default, it relies on local credentials stored in Elasticsearch. Authentication methods are stored in the auth.provider parameter, which is multi-valued. When a user logs in, each authentication method is tried in order until one succeeds. If no authentication method works, an error is returned and the user cannot log in. The default values within the configuration file are: auth { # \"provider\" parameter contains authentication provider. It can be multi-valued (useful for migration) # available auth types are: # services.LocalAuthSrv : passwords are stored in user entity (in Elasticsearch). No configuration is required. # ad : use ActiveDirectory to authenticate users. Configuration is under \"auth.ad\" key # ldap : use LDAP to authenticate users. Configuration is under \"auth.ldap\" key # oauth2 : use OAuth/OIDC to authenticate users. Configuration is under \"auth.oauth2\" and \"auth.sso\" keys provider = [local] # By default, basic authentication is disabled. You can enable it by setting \"method.basic\" to true. #method.basic = true ad { # The Windows domain name in DNS format. This parameter is required if you do not use # 'serverNames' below. #domainFQDN = \"mydomain.local\" # Optionally you can specify the host names of the domain controllers instead of using 'domainFQDN # above. If this parameter is not set, TheHive uses 'domainFQDN'. #serverNames = [ad1.mydomain.local, ad2.mydomain.local] # The Windows domain name using short format. This parameter is required. #domainName = \"MYDOMAIN\" # If 'true', use SSL to connect to the domain controller. #useSSL = true } ldap { # The LDAP server name or address. The port can be specified using the 'host:port' # syntax. This parameter is required if you don't use 'serverNames' below. #serverName = \"ldap.mydomain.local:389\" # If you have multiple LDAP servers, use the multi-valued setting 'serverNames' instead. #serverNames = [ldap1.mydomain.local, ldap2.mydomain.local] # Account to use to bind to the LDAP server. This parameter is required. #bindDN = \"cn=thehive,ou=services,dc=mydomain,dc=local\" # Password of the binding account. This parameter is required. #bindPW = \"***secret*password***\" # Base DN to search users. This parameter is required. #baseDN = \"ou=users,dc=mydomain,dc=local\" # Filter to search user in the directory server. Please note that {0} is replaced # by the actual user name. This parameter is required. #filter = \"(cn={0})\" # If 'true', use SSL to connect to the LDAP directory server. #useSSL = true } oauth2 { # URL of the authorization server #clientId = \"client-id\" #clientSecret = \"client-secret\" #redirectUri = \"https://my-thehive-instance.example/api/ssoLogin\" #responseType = \"code\" #grantType = \"authorization_code\" # URL from where to get the access token #authorizationUrl = \"https://auth-site.com/OAuth/Authorize\" #tokenUrl = \"https://auth-site.com/OAuth/Token\" # The endpoint from which to obtain user details using the OAuth token, after successful login #userUrl = \"https://auth-site.com/api/User\" #scope = [\"openid profile\"] } # Single-Sign On sso { # Autocreate user in database? #autocreate = false # Autoupdate its profile and roles? #autoupdate = false # Autologin user using SSO? #autologin = false # Attributes mappings #attributes { # login = \"sub\" # name = \"name\" # groups = \"groups\" # #roles = \"roles\" #} # Name of mapping class from user resource to backend user ('simple' or 'group') #mapper = group # Default roles for users with no groups mapped (\"read\", \"write\", \"admin\") #defaultRoles = [] #groups { # # URL to retrieve groups (leave empty if you are using OIDC) # #url = \"https://auth-site.com/api/Groups\" # # Group mappings, you can have multiple roles for each group: they are merged # mappings { # admin-profile-name = [\"admin\"] # editor-profile-name = [\"write\"] # reader-profile-name = [\"read\"] # } #} } } # Maximum time between two requests without requesting authentication session { warning = 5m inactivity = 1h } 3.1. LDAP/AD # To enable authentication using AD or LDAP, edit the application.conf file and supply the values for your environment. Then you need to create an account on TheHive for each AD or LDAP user in Administration > Users page (which can only be accessed by an administrator). This is required as TheHive needs to look up the role associated with the user and that role is stored locally by TheHive. Obviously, you don't need to supply a password as TheHive will check the credentials against the remote directory. In order to use SSL on LDAP or AD, TheHive must be able to validate remote certificates. To that end, the Java truststore must contain certificate authorities used to generate the AD and/or LDAP certificates. The Default JVM truststore contains the main official authorities but LDAP and AD certificates are probably not issued by them. Use keytool to create the truststore: keytool -import -file /path/to/your/ca.cert -alias InternalCA -keystore /path/to/your/truststore.jks Then add -Djavax.net.ssl.trustStore=/path/to/your/truststore.jks parameter when you start TheHive or put it in the JAVA_OPTS environment variable before starting TheHive. 3.2. OAuth2/OpenID Connect # To enable authentication using OAuth2/OpenID Connect, edit the application.conf file and supply the values of auth.oauth2 according to your environment. In addition, you need to supply: auth.sso.attributes.login : name of the attribute containing the OAuth2 user's login in retreived user info (mandatory) auth.sso.attributes.name : name of the attribute containing the OAuth2 user's name in retreived user info (mandatory) auth.sso.attributes.groups : name of the attribute containing the OAuth2 user's groups (mandatory using groups mappings) auth.sso.attributes.roles : name of the attribute containing the OAuth2 user's roles in retreived user info (mandatory using simple mapping) Important notes # Authenticate the user using an external OAuth2 authenticator server. The configuration is: clientId (string) client ID in the OAuth2 server. clientSecret (string) client secret in the OAuth2 server. redirectUri (string) the url of TheHive AOuth2 page (.../api/ssoLogin). responseType (string) type of the response. Currently only \"code\" is accepted. grantType (string) type of the grant. Currently only \"authorization_code\" is accepted. authorizationUrl (string) the url of the OAuth2 server. authorizationHeader (string) prefix of the authorization header to get user info: Bearer, token, ... tokenUrl (string) the token url of the OAuth2 server. userUrl (string) the url to get user information in OAuth2 server. scope (list of string) list of scope. Example # auth { provider = [local, oauth2] [..] sso { autocreate: false autoupdate: false mapper: \"simple\" attributes { login: \"login\" name: \"name\" roles: \"role\" } defaultRoles: [\"read\", \"write\"] defaultOrganization: \"demo\" } oauth2 { name: oauth2 clientId: \"Client_ID\" clientSecret: \"Client_ID\" redirectUri: \"http://localhost:9000/api/ssoLogin\" responseType: code grantType: \"authorization_code\" authorizationUrl: \"https://github.com/login/oauth/authorize\" authorizationHeader: \"token\" tokenUrl: \"https://github.com/login/oauth/access_token\" userUrl: \"https://api.github.com/user\" scope: [\"user\"] } [..] } 3.2.1. Roles mappings # You can choose a roles mapping with the auth.sso.mapper parameter. The available options are simple and group : Using simple mapping, we assume that the user info retrieved from auth.oauth2.userUrl contains the roles associated to the OAuth2 user. They can be: read , write or admin . Using groups mappings, we assume the retrieved user info contains groups that have to be associated to internal roles. In that case, you have to define mapppings in auth.sso.groups.mappings . If a user has multiple groups, mapped roles are merged. If you need to retreive groups from another endpoint that the one used for user info, you can provide it in auth.sso.groups.url . The retrieved groups can be a valid JSON array or a string listing them: { \"sub\": \"userid1\", \"name\": \"User name 1\", \"groups\": [\"admin-profile-name\", \"reader-profile-name\"] } OR { \"sub\": \"userid2\", \"name\": \"User name 2\", \"groups\": \"[admin-profile-name, reader-profile-name, \\\"another profile\\\", 'a last group']\" } OR { \"sub\": \"userid3\", \"name\": \"User name 3\", \"groups\": \"the-only-group-of-the-user\" } Finally, you can setup default roles associated with user with no roles/groups retrieved, using the auth.sso.defaultRoles parameter. 3.2.2. User autocreation, autoupdate and autologin # The main advantage of OAuth2/OpenID Connect authentication is you won't need to create an account on TheHive for each OAuth2 user if you set the config parameter auth.sso.autocreate to true . However, by default, OAuth2 users won't be updated on SSO login unless you set auth.sso.autoupdate to true . If you set this last parameter, roles and name will be fetched from retrieved user info and will be updated in local database on each login of the user. With auth.sso.autologin set to true , each user connecting to TheHive will automatically be redirected to auth.oauth2.authorizationUrl . The only way to authenticate in TheHive using a local user will be either: Connecting to TheHive using https://my-hive-instance.com/index.html#!/login?code=BAD_CODE . You will get an Authentication Failure but will then be able to authenticate. Connecting to TheHive using a real OAuth2 account, then disconnect. On disconnection, you won't be redirected to authorization URL. 3.2.3. Debugging # To debug the OAuth2 feature, you can uncomment the following lines in /etc/thehive/logback.xml : <!-- Uncomment the next lines to log debug information for OAuth/OIDC login --> <logger name=\"org.elastic4play.services.auth\" level=\"DEBUG\" /> <logger name=\"services.OAuth2Srv\" level=\"DEBUG\" /> <logger name=\"services.mappers\" level=\"DEBUG\" /> 4. Streaming (a.k.a The Flow) # The user interface is automatically updated when data is changed in the back-end. To do this, the back-end sends events to all the connected front-ends. The mechanism used to notify the front-end is called long polling and its settings are: refresh : when there is no notification, close the connection after this duration (the default is 1 minute). cache : before polling a session must be created, in order to make sure no event is lost between two polls. If there is no poll during the cache setting, the session is destroyed (the default is 15 minutes). nextItemMaxWait , globalMaxWait : when an event occurs, it is not immediately sent to the front-ends. The back-end waits nextItemMaxWait and up to globalMaxWait in case another event can be included in the notification. This mechanism saves many HTTP requests. Default values are: # Streaming stream.longpolling { # Maximum time a stream request waits for new element refresh = 1m # Lifetime of the stream session without request cache = 15m nextItemMaxWait = 500ms globalMaxWait = 1s } 5. Entity size limit # The Play framework used by TheHive sets the HTTP body size limit to 100KB by default for textual content (json, xml, text, form data) and 10MB for file uploads. This could be too small in most cases so you may want to change it with the following settings in the application.conf file: # Max textual content length play.http.parser.maxMemoryBuffer=1M # Max file size play.http.parser.maxDiskBuffer=1G Note : if you are using a NGINX reverse proxy in front of TheHive, be aware that it doesn't distinguish between text data and a file upload. So, you should also set the client_max_body_size parameter in your NGINX server configuration to the highest value among the two: file upload and text size defined in TheHive application.conf file. 6. Cortex # TheHive can use one or several Cortex analysis engines to get additional information on observables. When configured, analyzers available in Cortex become usable on TheHive. First you must enable CortexConnector , choose an identifier then specify the URL for each Cortex server: ## Enable the Cortex module play.modules.enabled += connectors.cortex.CortexConnector cortex { \"CORTEX-SERVER-ID\" { # URL of the Cortex server url = \"http://CORTEX_SERVER:CORTEX_PORT\" # Key of the Cortex user, mandatory for Cortex 2 key = \"API key\" } # HTTP client configuration, more details in section 8 # ws { # proxy {} # ssl {} # } # Check job update time interval refreshDelay = 1 minute # Maximum number of successive errors before give up maxRetryOnError = 3 # Check remote Cortex status time interval statusCheckInterval = 1 minute } If you connect TheHive with Cortex 2, you must create a user in Cortex with the read, analyze roles, set an API key and add this key in the Cortex server definition in TheHive application.conf . For Cortex 1, authentication is not required, the key is not used. To create a user with the read, analyze role in Cortex 2, you must have at least one organization configured then you can connect to the Cortex 2 Web UI using a orgAdmin account for that organization to create the user and generate their API key. Please refer to the Cortex Quick Start Guide for more information. Cortex analyzes observables and outputs reports in JSON format. TheHive shows the report as-is by default. In order to make reports more readable, we provide report templates which are in a separate package and must be installed manually: - download the report template package from https://dl.bintray.com/thehive-project/binary/report-templates.zip - log in TheHive using an administrator account - go to Admin > Report templates menu - click on Import templates button and select the downloaded package HTTP client used by Cortex connector use global configuration (in play.ws ) but can be overridden in Cortex section and in each Cortex server configuration. Refer to section 8 for more detail on how to configure HTTP client. 7. MISP # TheHive has the ability to connect to one or several MISP instances in order to import and export events. Hence TheHive is able to: receive events as they are added or updated from multiple MISP instances. These events will appear within the Alerts pane. export cases as MISP events to one or several MISP instances. The exported cases will not be published automatically though as they need to be reviewed prior to publishing. We strongly advise you to review the categories and types of attributes at least, before publishing the corresponding MISP events. Note : Please note that only and all the observables marked as IOCs will be used to create the MISP event. Any other observable will not be shared. This is not configurable. Within the configuration file, you can register your MISP server(s) under the misp configuration keyword. Each server shall be identified using an arbitrary name, its url , the corresponding authentication key and optional tags to add each observable created from a MISP event. Any registered server will be used to import events as alerts. It can also be used to export cases to as MISP events, if the account used by TheHive on the MISP instance has sufficient rights. This means that TheHive can import events from configured MISP servers and export cases to the same configured MISP servers. Having different configuration for sources and destination servers is expected in a future version. Important Notes # TheHive requires MISP 2.4.73 or better . Make sure that your are using a compatible version of MISP before reporting problems. MISP 2.4.72 and below do not work correctly with TheHive. 7.1 Configuration # To sync with a MISP server and retrieve events or export cases, edit the application.conf file and adjust the example shown below to your setup: ## Enable the MISP module (import and export) play.modules.enabled += connectors.misp.MispConnector misp { \"MISP-SERVER-ID\" { # URL of the MISP instance. url = \"<The_URL_of_the_MISP_Server_goes_here>\" # Authentication key. key = \"<the_auth_key_goes_here>\" # Name of the case template created in TheHive that shall be used to import # MISP events as cases by default. caseTemplate = \"<Template_Name_goes_here>\" # Tags to add to each observable imported from an event available on # this instance. tags = [\"misp-server-id\"] # Truststore to use to validate the X.509 certificate of the MISP # instance if the default truststore is not sufficient. #ws.ssl.trustManager.stores = [ #{ # type: \"JKS\" # path: \"/path/to/truststore.jks\" #} #] # HTTP client configuration, more details in section 8 # ws { # proxy {} # ssl {} # } # filters: max-attributes = 1000 max-size = 1 MiB max-age = 7 days exclusion { organisation = [\"bad organisation\", \"other orga\"] tags = [\"tag1\", \"tag2\"] } whitelist.tags = [\"whitelist-tag1\", \"whitelist-tag2\"] # MISP purpose defines if this instance can be used to import events (ImportOnly), export cases (ExportOnly) or both (ImportAndExport) # Default is ImportAndExport purpose = ImportAndExport } # Check remote TheHive status time interval statusCheckInterval = 1 minute # Interval between consecutive MISP event imports in hours (h) or # minutes (m). interval = 1h } The HTTP client used by the MISP connector uses a global configuration (in play.ws ) but it can be overridden within the MISP section of the configuation file and/or in the configuration section of each MISP server (in misp.MISP-SERVER-ID.ws ). Refer to section 8 for more details on how to configure the HTTP client. 7.2 Associate a Case Template to Alerts corresponding to MISP events # As stated in the subsection above, TheHive is able to automatically import MISP events (they will appear as alerts within the Alerts pane) and create cases out of them. This operation leverages the template engine. Thus you'll need to create a case template prior to importing MISP events. First, create a case template. Let's call it MISP-EVENT . Then update TheHive's configuration to add a 'caseTemplate' parameter as shown in the example below: misp { \"MISP-SERVER-ID\" { # URL of the MISP server url = \"<The_URL_of_the_MISP_Server_goes_here>\" # authentication key key = \"<the_auth_key_goes_here>\" # tags that must be automatically added to the case corresponding to the imported event tags = [\"misp\"] # case template caseTemplate = \"MISP-EVENT\" } Once the configuration file has been edited, restart TheHive. Every new import of a MISP event will generate a case using to the MISP-EVENT template by default. The template can be overridden though during the event import. MISP events will only be imported by TheHive if they have at least one attribute and were published. 7.3 Event Filters # When you first connect TheHive to a MISP instance, you can be overwhelmed by the number of alerts that will be generated, particularly if the MISP instance contains a lot of events. Indeed, every event, even those that date back to the beginning of the Internet, will generate an alert. To avoid alert fatigue, and starting from TheHive 3.0.4 (Cerana 0.4), you can exclude MISP events using different filters: the maximum number of attributes (max-attributes) the maximum size of the event's JSON message (max-size) the maximum age of the last publication (max-age) the organisation is black-listed (exclusion.organisation) one of the tags is black-listed (exclusion.tags) doesn't contain one of the whitelist tag (whitelist.tags) Please note that MISP event filters can be adapted to the configuration associated to each MISP server TheHive is connected with. As regards the max-age filter, it applies to the publication date of MISP events and not to the creation date. In the example below, the following MISP events won't generate alerts in TheHive: events that have more than 1000 attributes events which JSON message size is greater than 1MB events that have been published more than one week from the current date events that have been created by bad organisation or other orga events that contain tag1 or tag2 # filters: max-attributes = 1000 max-size = 1 MiB max-age = 7 days exclusion { organisation = [\"bad organisation\", \"other orga\"] tags = [\"tag1\", \"tag2\"] } Of course, you can omit some of the filters or all of them. 7.4 MISP Purpose # TheHive can interact with MISP in two ways: import a MISP event to create a case in TheHive and export a TheHive case to create a MISP event. By default, any MISP instance that is added to TheHive's configuration will be used for importing events and exporting cases ( ImportAndExport ). If you want to use MISP in only one way, you can set its purpose in the configuration as ImportOnly or ExportOnly . Starting from TheHive 3.3, when exporting a case to a MISP instance, you can export all its tags to the freshly created MISP event. This behaviour is not enabled by default. If you want to enable it you must set the exportCaseTags variable to true as shown below: misp { \"local\" { url = \"http://127.0.0.1\" key = \"<the_auth_key_goes_here>\" exportCaseTags = true [...] # additional parameters go here } } 8. HTTP Client Configuration # HTTP client can be configured by adding ws key in sections that needs to connect to remote HTTP service. The key can contain configuration items defined in play WS configuration : ws.followRedirects : Configures the client to follow 301 and 302 redirects (default is true). ws.useragent : To configure the User-Agent header field. ws.compressionEnabled : Set it to true to use gzip/deflater encoding (default is false). Timeouts # There are 3 different timeouts in WS. Reaching a timeout causes the WS request to interrupt. - ws.timeout.connection : The maximum time to wait when connecting to the remote host (default is 120 seconds). - ws.timeout.idle : The maximum time the request can stay idle (connection is established but waiting for more data) (default is 120 seconds). - ws.timeout.request : The total time you accept a request to take (it will be interrupted even if the remote host is still sending data) (default is 120 seconds). Proxy # Proxy can be used. By default, the proxy configured in JVM is used but one can configured specific configurations for each HTTP client. - ws.useProxyProperties : To use the JVM system\u2019s HTTP proxy settings (http.proxyHost, http.proxyPort) (default is true). This setting is ignored if ws.proxy settings is present. - ws.proxy.host : The hostname of the proxy server. - ws.proxy.post : The port of the proxy server. - ws.proxy.protocol : The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified. - ws.proxy.user : The username of the credentials for the proxy server. - ws.proxy.password : The password for the credentials for the proxy server. - ws.proxy.ntlmDomain : The password for the credentials for the proxy server. - ws.proxy.encoding : The realm's charset. - ws.proxy.nonProxyHosts : The list of hosts on which proxy must not be used. SSL # SSL of HTTP client can be completely configured in application.conf file. Certificate manager # Certificate manager is used to store client certificates and certificate authorities. keyManager indicates which certificate HTTP client can use to authenticate itself on remote host (when certificate based authentication is used) ws.ssl.keyManager { stores = [ { type: \"pkcs12\" // JKS or PEM path: \"mycert.p12\" password: \"password1\" } ] } Certificate authorities are configured using trustManager key. It is used to establish a secure connection with remote host. Server certificate must be signed by a trusted certificate authority. ws.ssl.trustManager { stores = [ { type: \"JKS\" // JKS or PEM path: \"keystore.jks\" password: \"password1\" } ] } Debugging # To debug the key manager / trust manager, set the following flags: ws.ssl.debug = { ssl = true trustmanager = true keymanager = true sslctx = true handshake = true verbose = true data = true certpath = true } Protocols # If you want to define a different default protocol, you can set it specifically in the client: ws.ssl.protocol = \"TLSv1.2\" If you want to define the list of enabled protocols, you can do so explicitly: ws.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"] Ciphers # Cipher suites can be configured using ws.ssl.enabledCipherSuites : ws.ssl.enabledCipherSuites = [ \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", ] 9. Monitoring and Performance Metrics (deprecated) # Performance metrics (response time, call rate to Elasticsearch and HTTP request, throughput, memory used...) can be collected if enabled in configuration. Enable it by editing the application.conf file, and add: # Register module for dependency injection play.modules.enabled += connectors.metrics.MetricsModule metrics.enabled = true These metrics can optionally be sent to an external database (graphite, ganglia or influxdb) in order to monitor the health of the platform. This feature is disabled by default. metrics { name = default enabled = true rateUnit = SECONDS durationUnit = SECONDS showSamples = false jvm = true logback = true graphite { enabled = false host = \"127.0.0.1\" port = 2003 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS period = 10s } ganglia { enabled = false host = \"127.0.0.1\" port = 8649 mode = UNICAST ttl = 1 version = 3.1 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS tmax = 60 dmax = 0 period = 10s } influx { enabled = false url = \"http://127.0.0.1:8086\" user = root password = root database = thehive retention = default consistency = ALL #tags = { # tag1 = value1 # tag2 = value2 #} period = 10s } } 10. HTTPS # You can enable HTTPS on TheHive application or add a reverse proxy in front of TheHive. The latter solution is recommended. 10.1 HTTPS using a reverse proxy # You can choose any reverse proxy to add SSL on TheHive. Below an example of NGINX configuration: server { listen 443 ssl; server_name thehive.example.com; ssl_certificate ssl/thehive_cert.pem; ssl_certificate_key ssl/thehive_key.pem; proxy_connect_timeout 600; proxy_send_timeout 600; proxy_read_timeout 600; send_timeout 600; client_max_body_size 2G; proxy_buffering off; client_header_buffer_size 8k; location / { add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; proxy_pass http://127.0.0.1:9000/; proxy_http_version 1.1; proxy_set_header Connection \"\"; # cf. https://github.com/akka/akka/issues/19542 } } 10.2 HTTPS without reverse proxy # This is not supported. 10.3 Strengthen security # When SSL is enable (with reverse proxy or not), you can configure cookie to be \"secure\" (usable only with HTTPS protocol). This is done by adding play.http.session.secure=true in the application.conf file. You can also enable HSTS . This header must be configured on the SSL termination component. For NGINX, use add_header directive, as show above.","title":"Configuration Guide"},{"location":"thehive/legacy/thehive3/admin/configuration/#configuration-guide","text":"The configuration file of TheHive is /etc/thehive/application.conf by default. This file uses the HOCON format . All configuration parameters should go in this file. You can have a look at the default settings .","title":"Configuration Guide"},{"location":"thehive/legacy/thehive3/admin/configuration/#table-of-contents","text":"1. Database 2. Datastore 3. Authentication 3.1 LDAP/AD 3.2 OAuth2/OpenID Connect 4. Streaming (a.k.a The Flow) 5. Entity size limit 6. Cortex 7. MISP 7.1 Configuration 7.2 Associate a Case Template to Alerts corresponding to MISP events 7.3 Event Filters 7.4 MISP Purpose 8. HTTP Client Configuration 9. Monitoring and Performance Metrics (deprecated) 10. HTTPS 10.1 HTTPS using a reverse proxy 10.2 HTTPS without reverse proxy 10.3 Strengthen security","title":"Table of Contents"},{"location":"thehive/legacy/thehive3/admin/configuration/#1-database","text":"TheHive uses the Elasticsearch search engine to store all persistent data. Elasticsearch is not part of TheHive package. It must be installed and configured as a standalone instance which can be located on the same machine. For more information on how to set up Elasticsearch, please refer to Elasticsearch installation guide . Three settings are required to connect to Elasticsearch: * the base name of the index * the name of the cluster * the address(es) and port(s) of the Elasticsearch instance The Defaults settings are: # Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch instance address. uri = \"http://127.0.0.1:9200/\" # Scroll keepalive keepalive = 1m # Size of the page for scroll pagesize = 50 # Number of shards nbshards = 5 # Number of replicas nbreplicas = 1 # Arbitrary settings settings { # Maximum number of nested fields mapping.nested_fields.limit = 100 } ## Authentication configuration #search.username = \"\" #search.password = \"\" ## SSL configuration #search.keyStore { # path = \"/path/to/keystore\" # type = \"JKS\" # or PKCS12 # password = \"keystore-password\" #} #search.trustStore { # path = \"/path/to/trustStore\" # type = \"JKS\" # or PKCS12 # password = \"trustStore-password\" #} } If you use a different configuration, modify the parameters accordingly in the application.conf file. If multiple Elasticsearch nodes are used as a cluster, you should add addresses of the master nodes in the url like this: search { uri = http://node1:9200,node2:9200/ ... TheHive uses the http port of Elasticsearch (9200/tcp by default). TheHive versions index schema (mapping) in Elasticsearch. Version numbers are appended to the index base name (the 8th version of the schema uses the index the_hive_8 if search.index = the_hive ). When too many documents are requested to TheHive, it uses the scroll feature: the results are retrieved through pagination. You can specify the size of the page ( search.pagesize ) and how long pages are kept in Elasticsearch (( search.keepalive ) before purging.","title":"1. Database"},{"location":"thehive/legacy/thehive3/admin/configuration/#2-datastore","text":"TheHive stores attachments as Elasticsearch documents. They are split in chunks and each chunk sent to Elasticsearch is identified by the hash of the entire attachment and the associated chunk number. The chunk size ( datastore.chunksize ) can be changed but any change will only affect new attachments. Existing ones won't be changed. An attachment is identified by its hash. The algorithm used is configurable ( datastore.hash.main ) but must not be changed after the first attachment insertion. Otherwise, previous files cannot be retrieved. Extra hash algorithms can be configured using datastore.hash.extra . These hashes are not used to identify the attachment but are shown in the user interface (the hash associated to the main algorithm is also shown). If you change extra algorithms, you should inform TheHive and ask it to recompute all hashes. Please note that the associated API call is currently disabled in Buckfast (v 2.10). It will be reinstated in the next release. Observables can contain malicious data. When you try to download an attachment from an observable (typically a file), it is automatically zipped and the resulting ZIP file is password-protected. The default password is malware but it can be changed with the datastore.attachment.password setting. Default values are: # Datastore datastore { name = data # Size of stored data chunks chunksize = 50k hash { # Main hash algorithm /!\\ Don't change this value main = \"SHA-256\" # Additional hash algorithms (used in attachments) extra = [\"SHA-1\", \"MD5\"] } attachment.password = \"malware\" }","title":"2. Datastore"},{"location":"thehive/legacy/thehive3/admin/configuration/#3-authentication","text":"TheHive supports local, LDAP, Active Directory (AD) or OAuth2/OpenID Connect for authentication. By default, it relies on local credentials stored in Elasticsearch. Authentication methods are stored in the auth.provider parameter, which is multi-valued. When a user logs in, each authentication method is tried in order until one succeeds. If no authentication method works, an error is returned and the user cannot log in. The default values within the configuration file are: auth { # \"provider\" parameter contains authentication provider. It can be multi-valued (useful for migration) # available auth types are: # services.LocalAuthSrv : passwords are stored in user entity (in Elasticsearch). No configuration is required. # ad : use ActiveDirectory to authenticate users. Configuration is under \"auth.ad\" key # ldap : use LDAP to authenticate users. Configuration is under \"auth.ldap\" key # oauth2 : use OAuth/OIDC to authenticate users. Configuration is under \"auth.oauth2\" and \"auth.sso\" keys provider = [local] # By default, basic authentication is disabled. You can enable it by setting \"method.basic\" to true. #method.basic = true ad { # The Windows domain name in DNS format. This parameter is required if you do not use # 'serverNames' below. #domainFQDN = \"mydomain.local\" # Optionally you can specify the host names of the domain controllers instead of using 'domainFQDN # above. If this parameter is not set, TheHive uses 'domainFQDN'. #serverNames = [ad1.mydomain.local, ad2.mydomain.local] # The Windows domain name using short format. This parameter is required. #domainName = \"MYDOMAIN\" # If 'true', use SSL to connect to the domain controller. #useSSL = true } ldap { # The LDAP server name or address. The port can be specified using the 'host:port' # syntax. This parameter is required if you don't use 'serverNames' below. #serverName = \"ldap.mydomain.local:389\" # If you have multiple LDAP servers, use the multi-valued setting 'serverNames' instead. #serverNames = [ldap1.mydomain.local, ldap2.mydomain.local] # Account to use to bind to the LDAP server. This parameter is required. #bindDN = \"cn=thehive,ou=services,dc=mydomain,dc=local\" # Password of the binding account. This parameter is required. #bindPW = \"***secret*password***\" # Base DN to search users. This parameter is required. #baseDN = \"ou=users,dc=mydomain,dc=local\" # Filter to search user in the directory server. Please note that {0} is replaced # by the actual user name. This parameter is required. #filter = \"(cn={0})\" # If 'true', use SSL to connect to the LDAP directory server. #useSSL = true } oauth2 { # URL of the authorization server #clientId = \"client-id\" #clientSecret = \"client-secret\" #redirectUri = \"https://my-thehive-instance.example/api/ssoLogin\" #responseType = \"code\" #grantType = \"authorization_code\" # URL from where to get the access token #authorizationUrl = \"https://auth-site.com/OAuth/Authorize\" #tokenUrl = \"https://auth-site.com/OAuth/Token\" # The endpoint from which to obtain user details using the OAuth token, after successful login #userUrl = \"https://auth-site.com/api/User\" #scope = [\"openid profile\"] } # Single-Sign On sso { # Autocreate user in database? #autocreate = false # Autoupdate its profile and roles? #autoupdate = false # Autologin user using SSO? #autologin = false # Attributes mappings #attributes { # login = \"sub\" # name = \"name\" # groups = \"groups\" # #roles = \"roles\" #} # Name of mapping class from user resource to backend user ('simple' or 'group') #mapper = group # Default roles for users with no groups mapped (\"read\", \"write\", \"admin\") #defaultRoles = [] #groups { # # URL to retrieve groups (leave empty if you are using OIDC) # #url = \"https://auth-site.com/api/Groups\" # # Group mappings, you can have multiple roles for each group: they are merged # mappings { # admin-profile-name = [\"admin\"] # editor-profile-name = [\"write\"] # reader-profile-name = [\"read\"] # } #} } } # Maximum time between two requests without requesting authentication session { warning = 5m inactivity = 1h }","title":"3. Authentication"},{"location":"thehive/legacy/thehive3/admin/configuration/#31-ldapad","text":"To enable authentication using AD or LDAP, edit the application.conf file and supply the values for your environment. Then you need to create an account on TheHive for each AD or LDAP user in Administration > Users page (which can only be accessed by an administrator). This is required as TheHive needs to look up the role associated with the user and that role is stored locally by TheHive. Obviously, you don't need to supply a password as TheHive will check the credentials against the remote directory. In order to use SSL on LDAP or AD, TheHive must be able to validate remote certificates. To that end, the Java truststore must contain certificate authorities used to generate the AD and/or LDAP certificates. The Default JVM truststore contains the main official authorities but LDAP and AD certificates are probably not issued by them. Use keytool to create the truststore: keytool -import -file /path/to/your/ca.cert -alias InternalCA -keystore /path/to/your/truststore.jks Then add -Djavax.net.ssl.trustStore=/path/to/your/truststore.jks parameter when you start TheHive or put it in the JAVA_OPTS environment variable before starting TheHive.","title":"3.1. LDAP/AD"},{"location":"thehive/legacy/thehive3/admin/configuration/#32-oauth2openid-connect","text":"To enable authentication using OAuth2/OpenID Connect, edit the application.conf file and supply the values of auth.oauth2 according to your environment. In addition, you need to supply: auth.sso.attributes.login : name of the attribute containing the OAuth2 user's login in retreived user info (mandatory) auth.sso.attributes.name : name of the attribute containing the OAuth2 user's name in retreived user info (mandatory) auth.sso.attributes.groups : name of the attribute containing the OAuth2 user's groups (mandatory using groups mappings) auth.sso.attributes.roles : name of the attribute containing the OAuth2 user's roles in retreived user info (mandatory using simple mapping)","title":"3.2. OAuth2/OpenID Connect"},{"location":"thehive/legacy/thehive3/admin/configuration/#important-notes","text":"Authenticate the user using an external OAuth2 authenticator server. The configuration is: clientId (string) client ID in the OAuth2 server. clientSecret (string) client secret in the OAuth2 server. redirectUri (string) the url of TheHive AOuth2 page (.../api/ssoLogin). responseType (string) type of the response. Currently only \"code\" is accepted. grantType (string) type of the grant. Currently only \"authorization_code\" is accepted. authorizationUrl (string) the url of the OAuth2 server. authorizationHeader (string) prefix of the authorization header to get user info: Bearer, token, ... tokenUrl (string) the token url of the OAuth2 server. userUrl (string) the url to get user information in OAuth2 server. scope (list of string) list of scope.","title":"Important notes"},{"location":"thehive/legacy/thehive3/admin/configuration/#example","text":"auth { provider = [local, oauth2] [..] sso { autocreate: false autoupdate: false mapper: \"simple\" attributes { login: \"login\" name: \"name\" roles: \"role\" } defaultRoles: [\"read\", \"write\"] defaultOrganization: \"demo\" } oauth2 { name: oauth2 clientId: \"Client_ID\" clientSecret: \"Client_ID\" redirectUri: \"http://localhost:9000/api/ssoLogin\" responseType: code grantType: \"authorization_code\" authorizationUrl: \"https://github.com/login/oauth/authorize\" authorizationHeader: \"token\" tokenUrl: \"https://github.com/login/oauth/access_token\" userUrl: \"https://api.github.com/user\" scope: [\"user\"] } [..] }","title":"Example"},{"location":"thehive/legacy/thehive3/admin/configuration/#321-roles-mappings","text":"You can choose a roles mapping with the auth.sso.mapper parameter. The available options are simple and group : Using simple mapping, we assume that the user info retrieved from auth.oauth2.userUrl contains the roles associated to the OAuth2 user. They can be: read , write or admin . Using groups mappings, we assume the retrieved user info contains groups that have to be associated to internal roles. In that case, you have to define mapppings in auth.sso.groups.mappings . If a user has multiple groups, mapped roles are merged. If you need to retreive groups from another endpoint that the one used for user info, you can provide it in auth.sso.groups.url . The retrieved groups can be a valid JSON array or a string listing them: { \"sub\": \"userid1\", \"name\": \"User name 1\", \"groups\": [\"admin-profile-name\", \"reader-profile-name\"] } OR { \"sub\": \"userid2\", \"name\": \"User name 2\", \"groups\": \"[admin-profile-name, reader-profile-name, \\\"another profile\\\", 'a last group']\" } OR { \"sub\": \"userid3\", \"name\": \"User name 3\", \"groups\": \"the-only-group-of-the-user\" } Finally, you can setup default roles associated with user with no roles/groups retrieved, using the auth.sso.defaultRoles parameter.","title":"3.2.1. Roles mappings"},{"location":"thehive/legacy/thehive3/admin/configuration/#322-user-autocreation-autoupdate-and-autologin","text":"The main advantage of OAuth2/OpenID Connect authentication is you won't need to create an account on TheHive for each OAuth2 user if you set the config parameter auth.sso.autocreate to true . However, by default, OAuth2 users won't be updated on SSO login unless you set auth.sso.autoupdate to true . If you set this last parameter, roles and name will be fetched from retrieved user info and will be updated in local database on each login of the user. With auth.sso.autologin set to true , each user connecting to TheHive will automatically be redirected to auth.oauth2.authorizationUrl . The only way to authenticate in TheHive using a local user will be either: Connecting to TheHive using https://my-hive-instance.com/index.html#!/login?code=BAD_CODE . You will get an Authentication Failure but will then be able to authenticate. Connecting to TheHive using a real OAuth2 account, then disconnect. On disconnection, you won't be redirected to authorization URL.","title":"3.2.2. User autocreation, autoupdate and autologin"},{"location":"thehive/legacy/thehive3/admin/configuration/#323-debugging","text":"To debug the OAuth2 feature, you can uncomment the following lines in /etc/thehive/logback.xml : <!-- Uncomment the next lines to log debug information for OAuth/OIDC login --> <logger name=\"org.elastic4play.services.auth\" level=\"DEBUG\" /> <logger name=\"services.OAuth2Srv\" level=\"DEBUG\" /> <logger name=\"services.mappers\" level=\"DEBUG\" />","title":"3.2.3. Debugging"},{"location":"thehive/legacy/thehive3/admin/configuration/#4-streaming-aka-the-flow","text":"The user interface is automatically updated when data is changed in the back-end. To do this, the back-end sends events to all the connected front-ends. The mechanism used to notify the front-end is called long polling and its settings are: refresh : when there is no notification, close the connection after this duration (the default is 1 minute). cache : before polling a session must be created, in order to make sure no event is lost between two polls. If there is no poll during the cache setting, the session is destroyed (the default is 15 minutes). nextItemMaxWait , globalMaxWait : when an event occurs, it is not immediately sent to the front-ends. The back-end waits nextItemMaxWait and up to globalMaxWait in case another event can be included in the notification. This mechanism saves many HTTP requests. Default values are: # Streaming stream.longpolling { # Maximum time a stream request waits for new element refresh = 1m # Lifetime of the stream session without request cache = 15m nextItemMaxWait = 500ms globalMaxWait = 1s }","title":"4. Streaming (a.k.a The Flow)"},{"location":"thehive/legacy/thehive3/admin/configuration/#5-entity-size-limit","text":"The Play framework used by TheHive sets the HTTP body size limit to 100KB by default for textual content (json, xml, text, form data) and 10MB for file uploads. This could be too small in most cases so you may want to change it with the following settings in the application.conf file: # Max textual content length play.http.parser.maxMemoryBuffer=1M # Max file size play.http.parser.maxDiskBuffer=1G Note : if you are using a NGINX reverse proxy in front of TheHive, be aware that it doesn't distinguish between text data and a file upload. So, you should also set the client_max_body_size parameter in your NGINX server configuration to the highest value among the two: file upload and text size defined in TheHive application.conf file.","title":"5. Entity size limit"},{"location":"thehive/legacy/thehive3/admin/configuration/#6-cortex","text":"TheHive can use one or several Cortex analysis engines to get additional information on observables. When configured, analyzers available in Cortex become usable on TheHive. First you must enable CortexConnector , choose an identifier then specify the URL for each Cortex server: ## Enable the Cortex module play.modules.enabled += connectors.cortex.CortexConnector cortex { \"CORTEX-SERVER-ID\" { # URL of the Cortex server url = \"http://CORTEX_SERVER:CORTEX_PORT\" # Key of the Cortex user, mandatory for Cortex 2 key = \"API key\" } # HTTP client configuration, more details in section 8 # ws { # proxy {} # ssl {} # } # Check job update time interval refreshDelay = 1 minute # Maximum number of successive errors before give up maxRetryOnError = 3 # Check remote Cortex status time interval statusCheckInterval = 1 minute } If you connect TheHive with Cortex 2, you must create a user in Cortex with the read, analyze roles, set an API key and add this key in the Cortex server definition in TheHive application.conf . For Cortex 1, authentication is not required, the key is not used. To create a user with the read, analyze role in Cortex 2, you must have at least one organization configured then you can connect to the Cortex 2 Web UI using a orgAdmin account for that organization to create the user and generate their API key. Please refer to the Cortex Quick Start Guide for more information. Cortex analyzes observables and outputs reports in JSON format. TheHive shows the report as-is by default. In order to make reports more readable, we provide report templates which are in a separate package and must be installed manually: - download the report template package from https://dl.bintray.com/thehive-project/binary/report-templates.zip - log in TheHive using an administrator account - go to Admin > Report templates menu - click on Import templates button and select the downloaded package HTTP client used by Cortex connector use global configuration (in play.ws ) but can be overridden in Cortex section and in each Cortex server configuration. Refer to section 8 for more detail on how to configure HTTP client.","title":"6. Cortex"},{"location":"thehive/legacy/thehive3/admin/configuration/#7-misp","text":"TheHive has the ability to connect to one or several MISP instances in order to import and export events. Hence TheHive is able to: receive events as they are added or updated from multiple MISP instances. These events will appear within the Alerts pane. export cases as MISP events to one or several MISP instances. The exported cases will not be published automatically though as they need to be reviewed prior to publishing. We strongly advise you to review the categories and types of attributes at least, before publishing the corresponding MISP events. Note : Please note that only and all the observables marked as IOCs will be used to create the MISP event. Any other observable will not be shared. This is not configurable. Within the configuration file, you can register your MISP server(s) under the misp configuration keyword. Each server shall be identified using an arbitrary name, its url , the corresponding authentication key and optional tags to add each observable created from a MISP event. Any registered server will be used to import events as alerts. It can also be used to export cases to as MISP events, if the account used by TheHive on the MISP instance has sufficient rights. This means that TheHive can import events from configured MISP servers and export cases to the same configured MISP servers. Having different configuration for sources and destination servers is expected in a future version.","title":"7. MISP"},{"location":"thehive/legacy/thehive3/admin/configuration/#important-notes_1","text":"TheHive requires MISP 2.4.73 or better . Make sure that your are using a compatible version of MISP before reporting problems. MISP 2.4.72 and below do not work correctly with TheHive.","title":"Important Notes"},{"location":"thehive/legacy/thehive3/admin/configuration/#71-configuration","text":"To sync with a MISP server and retrieve events or export cases, edit the application.conf file and adjust the example shown below to your setup: ## Enable the MISP module (import and export) play.modules.enabled += connectors.misp.MispConnector misp { \"MISP-SERVER-ID\" { # URL of the MISP instance. url = \"<The_URL_of_the_MISP_Server_goes_here>\" # Authentication key. key = \"<the_auth_key_goes_here>\" # Name of the case template created in TheHive that shall be used to import # MISP events as cases by default. caseTemplate = \"<Template_Name_goes_here>\" # Tags to add to each observable imported from an event available on # this instance. tags = [\"misp-server-id\"] # Truststore to use to validate the X.509 certificate of the MISP # instance if the default truststore is not sufficient. #ws.ssl.trustManager.stores = [ #{ # type: \"JKS\" # path: \"/path/to/truststore.jks\" #} #] # HTTP client configuration, more details in section 8 # ws { # proxy {} # ssl {} # } # filters: max-attributes = 1000 max-size = 1 MiB max-age = 7 days exclusion { organisation = [\"bad organisation\", \"other orga\"] tags = [\"tag1\", \"tag2\"] } whitelist.tags = [\"whitelist-tag1\", \"whitelist-tag2\"] # MISP purpose defines if this instance can be used to import events (ImportOnly), export cases (ExportOnly) or both (ImportAndExport) # Default is ImportAndExport purpose = ImportAndExport } # Check remote TheHive status time interval statusCheckInterval = 1 minute # Interval between consecutive MISP event imports in hours (h) or # minutes (m). interval = 1h } The HTTP client used by the MISP connector uses a global configuration (in play.ws ) but it can be overridden within the MISP section of the configuation file and/or in the configuration section of each MISP server (in misp.MISP-SERVER-ID.ws ). Refer to section 8 for more details on how to configure the HTTP client.","title":"7.1 Configuration"},{"location":"thehive/legacy/thehive3/admin/configuration/#72-associate-a-case-template-to-alerts-corresponding-to-misp-events","text":"As stated in the subsection above, TheHive is able to automatically import MISP events (they will appear as alerts within the Alerts pane) and create cases out of them. This operation leverages the template engine. Thus you'll need to create a case template prior to importing MISP events. First, create a case template. Let's call it MISP-EVENT . Then update TheHive's configuration to add a 'caseTemplate' parameter as shown in the example below: misp { \"MISP-SERVER-ID\" { # URL of the MISP server url = \"<The_URL_of_the_MISP_Server_goes_here>\" # authentication key key = \"<the_auth_key_goes_here>\" # tags that must be automatically added to the case corresponding to the imported event tags = [\"misp\"] # case template caseTemplate = \"MISP-EVENT\" } Once the configuration file has been edited, restart TheHive. Every new import of a MISP event will generate a case using to the MISP-EVENT template by default. The template can be overridden though during the event import. MISP events will only be imported by TheHive if they have at least one attribute and were published.","title":"7.2 Associate a Case Template to Alerts corresponding to MISP events"},{"location":"thehive/legacy/thehive3/admin/configuration/#73-event-filters","text":"When you first connect TheHive to a MISP instance, you can be overwhelmed by the number of alerts that will be generated, particularly if the MISP instance contains a lot of events. Indeed, every event, even those that date back to the beginning of the Internet, will generate an alert. To avoid alert fatigue, and starting from TheHive 3.0.4 (Cerana 0.4), you can exclude MISP events using different filters: the maximum number of attributes (max-attributes) the maximum size of the event's JSON message (max-size) the maximum age of the last publication (max-age) the organisation is black-listed (exclusion.organisation) one of the tags is black-listed (exclusion.tags) doesn't contain one of the whitelist tag (whitelist.tags) Please note that MISP event filters can be adapted to the configuration associated to each MISP server TheHive is connected with. As regards the max-age filter, it applies to the publication date of MISP events and not to the creation date. In the example below, the following MISP events won't generate alerts in TheHive: events that have more than 1000 attributes events which JSON message size is greater than 1MB events that have been published more than one week from the current date events that have been created by bad organisation or other orga events that contain tag1 or tag2 # filters: max-attributes = 1000 max-size = 1 MiB max-age = 7 days exclusion { organisation = [\"bad organisation\", \"other orga\"] tags = [\"tag1\", \"tag2\"] } Of course, you can omit some of the filters or all of them.","title":"7.3 Event Filters"},{"location":"thehive/legacy/thehive3/admin/configuration/#74-misp-purpose","text":"TheHive can interact with MISP in two ways: import a MISP event to create a case in TheHive and export a TheHive case to create a MISP event. By default, any MISP instance that is added to TheHive's configuration will be used for importing events and exporting cases ( ImportAndExport ). If you want to use MISP in only one way, you can set its purpose in the configuration as ImportOnly or ExportOnly . Starting from TheHive 3.3, when exporting a case to a MISP instance, you can export all its tags to the freshly created MISP event. This behaviour is not enabled by default. If you want to enable it you must set the exportCaseTags variable to true as shown below: misp { \"local\" { url = \"http://127.0.0.1\" key = \"<the_auth_key_goes_here>\" exportCaseTags = true [...] # additional parameters go here } }","title":"7.4 MISP Purpose"},{"location":"thehive/legacy/thehive3/admin/configuration/#8-http-client-configuration","text":"HTTP client can be configured by adding ws key in sections that needs to connect to remote HTTP service. The key can contain configuration items defined in play WS configuration : ws.followRedirects : Configures the client to follow 301 and 302 redirects (default is true). ws.useragent : To configure the User-Agent header field. ws.compressionEnabled : Set it to true to use gzip/deflater encoding (default is false).","title":"8. HTTP Client Configuration"},{"location":"thehive/legacy/thehive3/admin/configuration/#timeouts","text":"There are 3 different timeouts in WS. Reaching a timeout causes the WS request to interrupt. - ws.timeout.connection : The maximum time to wait when connecting to the remote host (default is 120 seconds). - ws.timeout.idle : The maximum time the request can stay idle (connection is established but waiting for more data) (default is 120 seconds). - ws.timeout.request : The total time you accept a request to take (it will be interrupted even if the remote host is still sending data) (default is 120 seconds).","title":"Timeouts"},{"location":"thehive/legacy/thehive3/admin/configuration/#proxy","text":"Proxy can be used. By default, the proxy configured in JVM is used but one can configured specific configurations for each HTTP client. - ws.useProxyProperties : To use the JVM system\u2019s HTTP proxy settings (http.proxyHost, http.proxyPort) (default is true). This setting is ignored if ws.proxy settings is present. - ws.proxy.host : The hostname of the proxy server. - ws.proxy.post : The port of the proxy server. - ws.proxy.protocol : The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified. - ws.proxy.user : The username of the credentials for the proxy server. - ws.proxy.password : The password for the credentials for the proxy server. - ws.proxy.ntlmDomain : The password for the credentials for the proxy server. - ws.proxy.encoding : The realm's charset. - ws.proxy.nonProxyHosts : The list of hosts on which proxy must not be used.","title":"Proxy"},{"location":"thehive/legacy/thehive3/admin/configuration/#ssl","text":"SSL of HTTP client can be completely configured in application.conf file.","title":"SSL"},{"location":"thehive/legacy/thehive3/admin/configuration/#certificate-manager","text":"Certificate manager is used to store client certificates and certificate authorities. keyManager indicates which certificate HTTP client can use to authenticate itself on remote host (when certificate based authentication is used) ws.ssl.keyManager { stores = [ { type: \"pkcs12\" // JKS or PEM path: \"mycert.p12\" password: \"password1\" } ] } Certificate authorities are configured using trustManager key. It is used to establish a secure connection with remote host. Server certificate must be signed by a trusted certificate authority. ws.ssl.trustManager { stores = [ { type: \"JKS\" // JKS or PEM path: \"keystore.jks\" password: \"password1\" } ] }","title":"Certificate manager"},{"location":"thehive/legacy/thehive3/admin/configuration/#debugging","text":"To debug the key manager / trust manager, set the following flags: ws.ssl.debug = { ssl = true trustmanager = true keymanager = true sslctx = true handshake = true verbose = true data = true certpath = true }","title":"Debugging"},{"location":"thehive/legacy/thehive3/admin/configuration/#protocols","text":"If you want to define a different default protocol, you can set it specifically in the client: ws.ssl.protocol = \"TLSv1.2\" If you want to define the list of enabled protocols, you can do so explicitly: ws.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"]","title":"Protocols"},{"location":"thehive/legacy/thehive3/admin/configuration/#ciphers","text":"Cipher suites can be configured using ws.ssl.enabledCipherSuites : ws.ssl.enabledCipherSuites = [ \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", ]","title":"Ciphers"},{"location":"thehive/legacy/thehive3/admin/configuration/#9-monitoring-and-performance-metrics-deprecated","text":"Performance metrics (response time, call rate to Elasticsearch and HTTP request, throughput, memory used...) can be collected if enabled in configuration. Enable it by editing the application.conf file, and add: # Register module for dependency injection play.modules.enabled += connectors.metrics.MetricsModule metrics.enabled = true These metrics can optionally be sent to an external database (graphite, ganglia or influxdb) in order to monitor the health of the platform. This feature is disabled by default. metrics { name = default enabled = true rateUnit = SECONDS durationUnit = SECONDS showSamples = false jvm = true logback = true graphite { enabled = false host = \"127.0.0.1\" port = 2003 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS period = 10s } ganglia { enabled = false host = \"127.0.0.1\" port = 8649 mode = UNICAST ttl = 1 version = 3.1 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS tmax = 60 dmax = 0 period = 10s } influx { enabled = false url = \"http://127.0.0.1:8086\" user = root password = root database = thehive retention = default consistency = ALL #tags = { # tag1 = value1 # tag2 = value2 #} period = 10s } }","title":"9. Monitoring and Performance Metrics (deprecated)"},{"location":"thehive/legacy/thehive3/admin/configuration/#10-https","text":"You can enable HTTPS on TheHive application or add a reverse proxy in front of TheHive. The latter solution is recommended.","title":"10. HTTPS"},{"location":"thehive/legacy/thehive3/admin/configuration/#101-https-using-a-reverse-proxy","text":"You can choose any reverse proxy to add SSL on TheHive. Below an example of NGINX configuration: server { listen 443 ssl; server_name thehive.example.com; ssl_certificate ssl/thehive_cert.pem; ssl_certificate_key ssl/thehive_key.pem; proxy_connect_timeout 600; proxy_send_timeout 600; proxy_read_timeout 600; send_timeout 600; client_max_body_size 2G; proxy_buffering off; client_header_buffer_size 8k; location / { add_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\"; proxy_pass http://127.0.0.1:9000/; proxy_http_version 1.1; proxy_set_header Connection \"\"; # cf. https://github.com/akka/akka/issues/19542 } }","title":"10.1 HTTPS using a reverse proxy"},{"location":"thehive/legacy/thehive3/admin/configuration/#102-https-without-reverse-proxy","text":"This is not supported.","title":"10.2 HTTPS without reverse proxy"},{"location":"thehive/legacy/thehive3/admin/configuration/#103-strengthen-security","text":"When SSL is enable (with reverse proxy or not), you can configure cookie to be \"secure\" (usable only with HTTPS protocol). This is done by adding play.http.session.secure=true in the application.conf file. You can also enable HSTS . This header must be configured on the SSL termination component. For NGINX, use add_header directive, as show above.","title":"10.3 Strengthen security"},{"location":"thehive/legacy/thehive3/admin/default-configuration/","text":"You can find the default configuration settings of TheHive below: # maximum number of similar cases maxSimilarCases = 100 # ElasticSearch search { # Name of the index index = the_hive # Name of the ElasticSearch cluster cluster = hive # Address of the ElasticSearch instance host = [\"127.0.0.1:9300\"] # Scroll keepalive keepalive = 1m # Size of the page for scroll pagesize = 50 # Arbitrary settings settings { # Maximum number of nested fields mapping.nested_fields.limit = 50 } } # Datastore datastore { # Size of stored data chunks chunksize = 50k hash { # Main hash algorithm /!\\ Don't change this value main = \"SHA-256\" # Additional hash algorithms (used in attachments) extra = [\"SHA-1\", \"MD5\"] } attachment.password = \"malware\" } auth { # \"provider\" parameter contains authentication provider. It can be multi-valued (useful for migration) # available auth types are: # local : passwords are stored in user entity (in ElasticSearch). No configuration are required. # ad : use ActiveDirectory to authenticate users. Configuration is under \"auth.ad\" key # ldap : use LDAP to authenticate users. Configuration is under \"auth.ldap\" key provider = [local] ad { # The name of the Microsoft Windows domaine using the DNS format. This parameter is required. #domainFQDN = \"mydomain.local\" # Optionally you can specify the host names of the domain controllers. If not set, TheHive uses \"domainFQDN\". #serverNames = [ad1.mydomain.local, ad2.mydomain.local] # The Microsoft Windows domain name using the short format. This parameter is required. #domainName = \"MYDOMAIN\" # Use SSL to connect to the domain controller(s). #useSSL = true } ldap { # LDAP server name or address. Port can be specified (host:port). This parameter is required. #serverName = \"ldap.mydomain.local:389\" # If you have multiple ldap servers, use the multi-valued settings. #serverNames = [ldap1.mydomain.local, ldap2.mydomain.local] # Use SSL to connect to directory server #useSSL = true # Account to use to bind on LDAP server. This parameter is required. #bindDN = \"cn=thehive,ou=services,dc=mydomain,dc=local\" # Password of the binding account. This parameter is required. #bindPW = \"***secret*password***\" # Base DN to search users. This parameter is required. #baseDN = \"ou=users,dc=mydomain,dc=local\" # Filter to search user {0} is replaced by user name. This parameter is required. #filter = \"(cn={0})\" } } # Maximum time between two requests without requesting authentication session { warning = 5m inactivity = 1h } # Streaming stream.longpolling { # Maximum time a stream request waits for new element refresh = 1m # Lifetime of the stream session without request cache = 15m nextItemMaxWait = 500ms globalMaxWait = 1s } # Cortex configuration ######## cortex { #\"CORTEX-SERVER-ID\" { # # URL of MISP server # url = \"\" # #HTTP client configuration, more details in section 8 # ws { # ws.useProxyProperties = true # proxy { # # The hostname of the proxy server. # #host = \"\" # # The port of the proxy server. # #post = 0 # # The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified. # #protocol = \"http\" # # The username of the credentials for the proxy server. # #user = \"\" # # The password for the credentials for the proxy server. # #password = \"\" # # The password for the credentials for the proxy server. # #ntlmDomain = \"\" # # The realm's charset. # #encoding = \"\" # # The list of host on which proxy must not be used. # #nonProxyHosts = \"\" # } # ssl { # keyManager { # used for client certificate authentication # stores = [{ # type: \"pkcs12\" // JKS or PEM # path: \"mycert.p12\" # password: \"password1\" # }] # } # # Add certificate authorities to trust remote certificate # trustManager { # stores = [{ # type: \"JKS\" // JKS or PEM # path: \"keystore.jks\" # password: \"password1\" # }] # } # debug = { # ssl = false # trustmanager = false # keymanager = false # sslctx = false # handshake = false # verbose = false # data = false # certpath = false # } # # # default SSL protocol # #protocol = \"TLSv1.2\" # # # list of enabled SSL protocols # #ws.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"] # # # SSL Cipher suite # #enabledCipherSuites = [ # # \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", # # \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", # # \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", # # \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", # #] # } # } #} } # MISP configuration ######## misp { #\"MISP-SERVER-ID\" { # # URL of MISP server # url = \"\" # # authentication key # key = \"\" # #tags to be added to imported artifact # tags = [\"misp\"] # # # filters: # # the maximum number of attributes (max-attributes) # #max-attributes = 1000 # # the maximum size of the event json message # #max-size = 1 MiB # # the age of the last publication # #max-age = 7 days # exclusion { # # the organisation is black-listed # #organisation = [\"bad organisation\", \"other orga\"] # # one of the tags is black-listed # #tags = [\"tag1\", \"tag2\"] # } # # ws { # ws.useProxyProperties = true # proxy { # # The hostname of the proxy server. # #host = \"\" # # The port of the proxy server. # #post = 0 # # The protocol of the proxy server. Use \"http\" or \"https\". Defaults to \"http\" if not specified. # #protocol = \"http\" # # The username of the credentials for the proxy server. # #user = \"\" # # The password for the credentials for the proxy server. # #password = \"\" # # The password for the credentials for the proxy server. # #ntlmDomain = \"\" # # The realm's charset. # #encoding = \"\" # # The list of host on which proxy must not be used. # #nonProxyHosts = \"\" # } # # ssl { # keyManager { # used for client certificate authentication # stores = [{ # type: \"pkcs12\" // JKS or PEM # path: \"mycert.p12\" # password: \"password1\" # }] # } # # Add certificate authorities to trust remote certificate # trustManager { # stores = [{ # type: \"JKS\" // JKS or PEM # path: \"keystore.jks\" # password: \"password1\" # }] # } # debug = { # ssl = false # trustmanager = false # keymanager = false # sslctx = false # handshake = false # verbose = false # data = false # certpath = false # } # # # default SSL protocol # #protocol = \"TLSv1.2\" # # # list of enabled SSL protocols # #ws.ssl.enabledProtocols = [\"TLSv1.2\", \"TLSv1.1\", \"TLSv1\"] # # # SSL Cipher suite # #enabledCipherSuites = [ # # \"TLS_DHE_RSA_WITH_AES_128_GCM_SHA256\", # # \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", # # \"TLS_DHE_RSA_WITH_AES_256_GCM_SHA384\", # # \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", # #] # } # } #} # Interval between two MISP event import interval = 1h } # Metrics configuration ######## metrics { name = default enabled = false rateUnit = SECONDS durationUnit = SECONDS jvm = true logback = true graphite { enabled = false host = \"127.0.0.1\" port = 2003 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS period = 10s } ganglia { enabled = false host = \"127.0.0.1\" port = 8649 mode = UNICAST ttl = 1 version = 3.1 prefix = thehive rateUnit = SECONDS durationUnit = MILLISECONDS tmax = 60 dmax = 0 period = 10s } influx { enabled = false url = \"http://127.0.0.1:8086\" user = root password = root database = thehive retention = default consistency = ALL #tags = { # tag1 = value1 # tag2 = value2 #} period = 10s } }","title":"Default configuration"},{"location":"thehive/legacy/thehive3/admin/schema_version/","text":"Schema version # The data of TheHive is stored in an ElasticSearch index. The name of the index is suffixed by the revision of the schema. When the schema of TheHive database changes, a new one is created and the version is incremented. By default, index base name is \"the_hive\" but can be configured ( index.index in application.conf). The following table show for each version of TheHive the default name of the index: TheHive version Index name 2.9.1 the_hive_7 2.9.2 the_hive_7 2.10.0 the_hive_8 2.10.1 the_hive_8 2.10.2 the_hive_8 2.11.0 the_hive_9 2.11.1 the_hive_9 2.11.2 the_hive_9 2.11.3 the_hive_9 2.12.0 the_hive_10 2.12.1 the_hive_10 2.13.0 the_hive_10 2.13.1 the_hive_10 2.13.2 the_hive_11 3.0.0 the_hive_12 3.0.1 the_hive_12 3.0.2 the_hive_12 3.0.3 the_hive_12 3.0.4 the_hive_13","title":"Schema version"},{"location":"thehive/legacy/thehive3/admin/schema_version/#schema-version","text":"The data of TheHive is stored in an ElasticSearch index. The name of the index is suffixed by the revision of the schema. When the schema of TheHive database changes, a new one is created and the version is incremented. By default, index base name is \"the_hive\" but can be configured ( index.index in application.conf). The following table show for each version of TheHive the default name of the index: TheHive version Index name 2.9.1 the_hive_7 2.9.2 the_hive_7 2.10.0 the_hive_8 2.10.1 the_hive_8 2.10.2 the_hive_8 2.11.0 the_hive_9 2.11.1 the_hive_9 2.11.2 the_hive_9 2.11.3 the_hive_9 2.12.0 the_hive_10 2.12.1 the_hive_10 2.13.0 the_hive_10 2.13.1 the_hive_10 2.13.2 the_hive_11 3.0.0 the_hive_12 3.0.1 the_hive_12 3.0.2 the_hive_12 3.0.3 the_hive_12 3.0.4 the_hive_13","title":"Schema version"},{"location":"thehive/legacy/thehive3/admin/updating/","text":"Update TheHive # TheHive is simple to update. You only need to replace your current package files by new ones. If the schema of the data changes between the two versions, the first request to the application asks the user to start a data migration. In this case, authentication is not required. This process creates a new index in ElasticSearch (suffixed by the version of the schema) and copies all the data on it (before adapting its format). It is always possible to rollback to the previous version but all modifications done on the new version will be lost.","title":"Update TheHive"},{"location":"thehive/legacy/thehive3/admin/updating/#update-thehive","text":"TheHive is simple to update. You only need to replace your current package files by new ones. If the schema of the data changes between the two versions, the first request to the application asks the user to start a data migration. In this case, authentication is not required. This process creates a new index in ElasticSearch (suffixed by the version of the schema) and copies all the data on it (before adapting its format). It is always possible to rollback to the previous version but all modifications done on the new version will be lost.","title":"Update TheHive"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/","text":"This document is related to upgrading TheHive and Elasticsearch on Ubuntu 16.04 . Upgrade TheHive to version 3.4 # Perform a backup of data # curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/absolute/path/to/backup/directory\", \"compress\": true } }' For example: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/opt/backup\", \"compress\": true } }' Next: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1?wait_for_completion=true&pretty' -d '{ \"indices\": \"<INDEX>\" }' For example: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/the_hive_152019060701_1?wait_for_completion=true&pretty' -d '{ \"indices\": \"the_hive_15\" }' Output example: { \"snapshot\" : { \"snapshot\" : \"the_hive_152019060701_1\" , \"uuid\" : \"ZKhBL2BHTAS2g71Xby2OgQ\" , \"version_id\" : 5061699 , \"version\" : \"5.6.16\" , \"indices\" : [ \"the_hive_15\" ], \"state\" : \"SUCCESS\" , \"start_time\" : \"2019-06-07T13:07:38.844Z\" , \"start_time_in_millis\" : 1559912858844 , \"end_time\" : \"2019-06-07T13:07:40.640Z\" , \"end_time_in_millis\" : 1559912860640 , \"duration_in_millis\" : 1796 , \"failures\" : [ ], \"shards\" : { \"total\" : 5 , \"failed\" : 0 , \"successful\" : 5 } } } You can find more information about backup and restore in the dedicated documentation . Stop TheHive service # service thehive stop Update TheHive configuration # Current /etc/thehive/application.conf # Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch cluster name. cluster = hive # ElasticSearch instance address. #host = [\"127.0.0.1:9300\"] [..] } New /etc/thehive/application.conf : Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch instance address. uri = \"http://127.0.0.1:9200\" [] } cluster { name = hive } Upgrade TheHive # apt upgrade thehive Restart TheHive service # Ensure everything is working. Upgrade Elasticsearch from version 5.x to 6.x # This is greatly inspired by the official documentation : https://www.elastic.co/guide/en/elasticsearch/reference/6.0/rolling-upgrades.html with additional info we had to set up to make everything work. Upgrading from earlier 5.x versions requires a full cluster restart . Disable shard allocation # curl -X PUT \"localhost:9200/_cluster/settings\" -H 'Content-Type: application/json' -d ' { \"persistent\": { \"cluster.routing.allocation.enable\": \"none\" } } ' Output: {\"acknowledged\":true,\"persistent\":{\"cluster\":{\"routing\":{\"allocation\":{\"enable\":\"none\"}}}},\"transient\":{}} curl -X POST \"localhost:9200/_flush/synced\" Output : {\"_shards\":{\"total\":60,\"successful\":30,\"failed\":0},\"cortex_4\":{\"total\":10,\"successful\":5,\"failed\":0},\"cortex_2\":{\"total\":10,\"successful\":5,\"failed\":0},\"cortex_3\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_13\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_15\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_14\":{\"total\":10,\"successful\":5,\"failed\":0}} shut down a single node # sudo -i service elasticsearch stop Upgrade the node you shut down # wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - sudo apt-get install apt-transport-https echo \"deb https://artifacts.elastic.co/packages/6.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list sudo apt-get update && sudo apt-get install elasticsearch Upgrade plugins # /usr/share/elasticsearch/bin/elasticsearch-plugin list ## for all plugin: /usr/share/elasticsearch/bin/elasticsearch-plugin install $plugin Update Elasticsearch configuration # Add path.logs and path.data in `/etc/elasticsearch/elasticsearch.yml: http.host : 127.0.0.1 transport.host : 127.0.0.1 cluster.name : hive thread_pool.index.queue_size : 100000 thread_pool.search.queue_size : 100000 thread_pool.bulk.queue_size : 100000 path.repo : [ \"/opt/backup\" ] path.logs : \"/var/log/elasticsearch\" path.data : \"/var/lib/elasticsearch\" Set $JAVA_HOME in /etc/default/elasticsearch for example: [..] JAVA_HOME=/usr/lib/jvm/java-8-oracle/ [..] On Ubuntu 16.04 we had to set read persmissions manually to this file: chmod o+r /etc/default/elasticsearch Restart the node # sudo update-rc.d elasticsearch defaults 95 10 sudo -i service elasticsearch start Check that elasticsearch is running # curl -X GET \"localhost:9200/\" curl -X GET \"localhost:9200/_cat/nodes\" Reenable shard allocation # Once the node has joined the cluster: curl -X PUT \"localhost:9200/_cluster/settings\" -H 'Content-Type: application/json' -d ' { \"persistent\": { \"cluster.routing.allocation.enable\": null } } ' Output: {\"acknowledged\":true,\"persistent\":{},\"transient\":{}} Wait for the node to recover # Before upgrading the next node, wait for the cluster to finish shard allocation. You can check progress by submitting a _cat/health request: curl -X GET \"localhost:9200/_cat/health?v\" Resources # https://www.elastic.co/guide/en/elasticsearch/reference/6.0/rolling-upgrades.html https://www.elastic.co/guide/en/elasticsearch/reference/6.0/restart-upgrade.html https://www.elastic.co/guide/en/elasticsearch/reference/6.0/deb.html","title":"Upgrade to thehive 3 4 and es 6 x"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#upgrade-thehive-to-version-34","text":"","title":"Upgrade TheHive to version 3.4"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#perform-a-backup-of-data","text":"curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/absolute/path/to/backup/directory\", \"compress\": true } }' For example: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup' -d '{ \"type\": \"fs\", \"settings\": { \"location\": \"/opt/backup\", \"compress\": true } }' Next: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/snapshot_1?wait_for_completion=true&pretty' -d '{ \"indices\": \"<INDEX>\" }' For example: curl -XPUT 'http://localhost:9200/_snapshot/the_hive_backup/the_hive_152019060701_1?wait_for_completion=true&pretty' -d '{ \"indices\": \"the_hive_15\" }' Output example: { \"snapshot\" : { \"snapshot\" : \"the_hive_152019060701_1\" , \"uuid\" : \"ZKhBL2BHTAS2g71Xby2OgQ\" , \"version_id\" : 5061699 , \"version\" : \"5.6.16\" , \"indices\" : [ \"the_hive_15\" ], \"state\" : \"SUCCESS\" , \"start_time\" : \"2019-06-07T13:07:38.844Z\" , \"start_time_in_millis\" : 1559912858844 , \"end_time\" : \"2019-06-07T13:07:40.640Z\" , \"end_time_in_millis\" : 1559912860640 , \"duration_in_millis\" : 1796 , \"failures\" : [ ], \"shards\" : { \"total\" : 5 , \"failed\" : 0 , \"successful\" : 5 } } } You can find more information about backup and restore in the dedicated documentation .","title":"Perform a backup of data"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#stop-thehive-service","text":"service thehive stop","title":"Stop TheHive service"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#update-thehive-configuration","text":"Current /etc/thehive/application.conf # Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch cluster name. cluster = hive # ElasticSearch instance address. #host = [\"127.0.0.1:9300\"] [..] } New /etc/thehive/application.conf : Elasticsearch search { ## Basic configuration # Index name. index = the_hive # ElasticSearch instance address. uri = \"http://127.0.0.1:9200\" [] } cluster { name = hive }","title":"Update TheHive configuration"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#upgrade-thehive","text":"apt upgrade thehive","title":"Upgrade TheHive"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#restart-thehive-service","text":"Ensure everything is working.","title":"Restart TheHive service"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#upgrade-elasticsearch-from-version-5x-to-6x","text":"This is greatly inspired by the official documentation : https://www.elastic.co/guide/en/elasticsearch/reference/6.0/rolling-upgrades.html with additional info we had to set up to make everything work. Upgrading from earlier 5.x versions requires a full cluster restart .","title":"Upgrade Elasticsearch from version 5.x to 6.x"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#disable-shard-allocation","text":"curl -X PUT \"localhost:9200/_cluster/settings\" -H 'Content-Type: application/json' -d ' { \"persistent\": { \"cluster.routing.allocation.enable\": \"none\" } } ' Output: {\"acknowledged\":true,\"persistent\":{\"cluster\":{\"routing\":{\"allocation\":{\"enable\":\"none\"}}}},\"transient\":{}} curl -X POST \"localhost:9200/_flush/synced\" Output : {\"_shards\":{\"total\":60,\"successful\":30,\"failed\":0},\"cortex_4\":{\"total\":10,\"successful\":5,\"failed\":0},\"cortex_2\":{\"total\":10,\"successful\":5,\"failed\":0},\"cortex_3\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_13\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_15\":{\"total\":10,\"successful\":5,\"failed\":0},\"the_hive_14\":{\"total\":10,\"successful\":5,\"failed\":0}}","title":"Disable shard allocation"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#shut-down-a-single-node","text":"sudo -i service elasticsearch stop","title":"shut down a single node"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#upgrade-the-node-you-shut-down","text":"wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - sudo apt-get install apt-transport-https echo \"deb https://artifacts.elastic.co/packages/6.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-6.x.list sudo apt-get update && sudo apt-get install elasticsearch","title":"Upgrade the node you shut down"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#upgrade-plugins","text":"/usr/share/elasticsearch/bin/elasticsearch-plugin list ## for all plugin: /usr/share/elasticsearch/bin/elasticsearch-plugin install $plugin","title":"Upgrade plugins"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#update-elasticsearch-configuration","text":"Add path.logs and path.data in `/etc/elasticsearch/elasticsearch.yml: http.host : 127.0.0.1 transport.host : 127.0.0.1 cluster.name : hive thread_pool.index.queue_size : 100000 thread_pool.search.queue_size : 100000 thread_pool.bulk.queue_size : 100000 path.repo : [ \"/opt/backup\" ] path.logs : \"/var/log/elasticsearch\" path.data : \"/var/lib/elasticsearch\" Set $JAVA_HOME in /etc/default/elasticsearch for example: [..] JAVA_HOME=/usr/lib/jvm/java-8-oracle/ [..] On Ubuntu 16.04 we had to set read persmissions manually to this file: chmod o+r /etc/default/elasticsearch","title":"Update Elasticsearch configuration"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#restart-the-node","text":"sudo update-rc.d elasticsearch defaults 95 10 sudo -i service elasticsearch start","title":"Restart the node"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#check-that-elasticsearch-is-running","text":"curl -X GET \"localhost:9200/\" curl -X GET \"localhost:9200/_cat/nodes\"","title":"Check that elasticsearch is running"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#reenable-shard-allocation","text":"Once the node has joined the cluster: curl -X PUT \"localhost:9200/_cluster/settings\" -H 'Content-Type: application/json' -d ' { \"persistent\": { \"cluster.routing.allocation.enable\": null } } ' Output: {\"acknowledged\":true,\"persistent\":{},\"transient\":{}}","title":"Reenable shard allocation"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#wait-for-the-node-to-recover","text":"Before upgrading the next node, wait for the cluster to finish shard allocation. You can check progress by submitting a _cat/health request: curl -X GET \"localhost:9200/_cat/health?v\"","title":"Wait for the node to recover"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_4_and_es_6_x/#resources","text":"https://www.elastic.co/guide/en/elasticsearch/reference/6.0/rolling-upgrades.html https://www.elastic.co/guide/en/elasticsearch/reference/6.0/restart-upgrade.html https://www.elastic.co/guide/en/elasticsearch/reference/6.0/deb.html","title":"Resources"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/","text":"Migration from Elasticsearch 6.8.2 to ES 7.x # \u26a0\ufe0f IMPORTANT NOTE This migration process is intended for single node of Elasticsearch database The current version of this document is provided for testing purpose ONLY! This guide has been written and tested to migrate data from ES 6.8.2 to ES 7.8.1, and TheHive 3.4.2 to TheHive 3.5.0-RC1 only! This guide starts with Elasticsearch version 6.8.2 up and running, indexes and data. To test this guide, we recommend using a backup of you production server. (see Backup and Restore page for more information) This guide is illustrated with TheHive index. The process is identical for Cortex, you just have to adjust index names. Prerequisite # The software jq is required to manipulate JSON and create new indexes. More information at https://stedolan.github.io/jq/ . Identify if your index should be reindexed # You can easily identify if indexes should be reindexed or not. On the index named the_hive_15 run the following command: curl -s http://127.0.0.1:9200/the_hive_15?human | jq '.the_hive_15.settings.index.version.created' if the output is similar to \"5xxxxxx\" then reindexing is required, you should follow this guide. If it is \"6xxxxxx\" then the index can be read by Elasticsearch 7.8.x. Upgrade Elasticsearch, and TheHive-3.5.0. Migration guide # Current status # Current context is: - Elasticsearch 6.8.2 - TheHive 3.4.2 All up and running. Start by identifying indices on you Elasticsearch instance. curl http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb The index name is the_hive_15 . Record this somewhere. Stop services # Before starting updating the database, lets stop applications: sudo service thehive stop Create a new index # The First operation lies in creating a new index named new_the_hive_15 with settings from current index the_hive_15 (ensure to keep index version, needed for future upgrade). curl -XPUT 'http://localhost:9200/new_the_hive_15' \\ -H 'Content-Type: application/json' \\ -d \" $( curl http://localhost:9200/the_hive_15 | \\ jq '.the_hive_15 | del(.settings.index.provided_name, .settings.index.creation_date, .settings.index.uuid, .settings.index.version, .settings.index.mapping.single_type, .mappings.doc._all)' ) \" Check the new index is well created: curl -XGET http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open new_the_hive_15 A2KLoZPpSXygutlfy_RNCQ 5 1 0 0 1.1kb 1.1kb green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb 33.2mb Proceed to Reindex # Next operation lies in running the reindex command in the newly created index: curl -XPOST -H 'Content-Type: application/json' http://localhost:9200/_reindex -d '{ \"conflicts\": \"proceed\", \"source\": { \"index\": \"the_hive_15\" }, \"dest\": { \"index\": \"new_the_hive_15\" } }' After a moment, you should get a similar output: { \"took\" : 5119 , \"timed_out\" : false , \"total\" : 5889 , \"updated\" : 0 , \"created\" : 5889 , \"deleted\" : 0 , \"batches\" : 6 , \"version_conflicts\" : 0 , \"noops\" : 0 , \"retries\" : { \"bulk\" : 0 , \"search\" : 0 }, \"throttled_millis\" : 0 , \"requests_per_second\" : -1.0 , \"throttled_until_millis\" : 0 , \"failures\" : [] } Ensure new index has been created # Run the following command, and ensure the new index is like the current one (size can vary): curl -XGET http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_the_hive_15 GV-3Y8QjTjWw0F-p2sjW6Q 5 0 30977 0 26mb 26mb green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb 33.2mb Delete old indices # This is the thrilling part. Now the new index new_the_hive_15 is created and similar the_hive_15, older indexes should be completely deleted from the database. To delete index named the_hive_15 , run the following command: curl -XDELETE http://localhost:9200/the_hive_15 Run the same command for older indexes if exist (the_hive_14, the_hive_13....). Elasticsearch 7.x cannot run with index created with Elasticsearch 5.x. Create an alias # Before stopping Elasticsearch service, let\u2019s create an alias to keep index names in the future. curl -XPOST -H 'Content-Type: application/json' 'http://localhost:9200/_aliases' -d '{ \"actions\": [ { \"add\": { \"index\": \"new_the_hive_15\", \"alias\": \"the_hive_15\" } } ] }' Doing so will allow TheHive 3.5.0 to find the index without updating the configuration file. Check the alias has been well created by running the following command curl -XGET http://localhost:9200/_alias?pretty The output should look like: { \"new_the_hive_15\" : { \"aliases\" : { \"the_hive_15\" : { } } } } Stop Elasticsearch version 6.8.2 # sudo service elasticsearch stop Update Elasticsearch # Update the configuration of Elastisearch. Configuration file should look like this: [..] http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive script.allowed_types: inline thread_pool.search.queue_size: 100000 thread_pool.write.queue_size: 10000 Now, upgrade Elasticsearch to version 7.x following the documentation for your Operating System, and ensure the service start successfully. Install or update to TheHive 3.5.0 # DEB package # If using Debian based Linux operating system, configure it to follow our beta repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then install it by running: sudo apt install thehive or sudo apt install thehive = 3 .5.0-1 RPM # Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [thehive-project] enabled=1 priority=1 name=TheHive-Project RPM repository baseurl=http://rpm.thehive-project.org/release/noarch gpgcheck=1 Then install it by running: sudo yum install thehive or sudo yum install thehive-3.5.0-1 Install binaries # cd /opt wget https://download.thehive-project.org/thehive-3.5.0-1.zip unzip thehive-3.5.0-1.zip ln -s thehive-3.5.0-1 thehive Docker images # Docker images are also provided on Dockerhub. docker pull thehiveproject/thehive:3.5.0-1 Update Database # Connect to TheHive, the maintenance page should ask to update. Once updated, ensure a new index named the_hive_16 has been created. curl -XGET http://localhost:9200/_cat/indices \\? v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_the_hive_15 GV-3Y8QjTjWw0F-p2sjW6Q 5 0 30977 0 26mb 26mb yellow open the_hive_16 Nz0vCKqhRK2xkx1t_WF-0g 5 1 30977 0 26.1mb 26.1mb","title":"Migration from Elasticsearch 6.8.2 to ES 7.x"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#migration-from-elasticsearch-682-to-es-7x","text":"\u26a0\ufe0f IMPORTANT NOTE This migration process is intended for single node of Elasticsearch database The current version of this document is provided for testing purpose ONLY! This guide has been written and tested to migrate data from ES 6.8.2 to ES 7.8.1, and TheHive 3.4.2 to TheHive 3.5.0-RC1 only! This guide starts with Elasticsearch version 6.8.2 up and running, indexes and data. To test this guide, we recommend using a backup of you production server. (see Backup and Restore page for more information) This guide is illustrated with TheHive index. The process is identical for Cortex, you just have to adjust index names.","title":"Migration from Elasticsearch 6.8.2 to ES 7.x"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#prerequisite","text":"The software jq is required to manipulate JSON and create new indexes. More information at https://stedolan.github.io/jq/ .","title":"Prerequisite"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#identify-if-your-index-should-be-reindexed","text":"You can easily identify if indexes should be reindexed or not. On the index named the_hive_15 run the following command: curl -s http://127.0.0.1:9200/the_hive_15?human | jq '.the_hive_15.settings.index.version.created' if the output is similar to \"5xxxxxx\" then reindexing is required, you should follow this guide. If it is \"6xxxxxx\" then the index can be read by Elasticsearch 7.8.x. Upgrade Elasticsearch, and TheHive-3.5.0.","title":"Identify if your index should be reindexed"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#migration-guide","text":"","title":"Migration guide"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#current-status","text":"Current context is: - Elasticsearch 6.8.2 - TheHive 3.4.2 All up and running. Start by identifying indices on you Elasticsearch instance. curl http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb The index name is the_hive_15 . Record this somewhere.","title":"Current status"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#stop-services","text":"Before starting updating the database, lets stop applications: sudo service thehive stop","title":"Stop services"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#create-a-new-index","text":"The First operation lies in creating a new index named new_the_hive_15 with settings from current index the_hive_15 (ensure to keep index version, needed for future upgrade). curl -XPUT 'http://localhost:9200/new_the_hive_15' \\ -H 'Content-Type: application/json' \\ -d \" $( curl http://localhost:9200/the_hive_15 | \\ jq '.the_hive_15 | del(.settings.index.provided_name, .settings.index.creation_date, .settings.index.uuid, .settings.index.version, .settings.index.mapping.single_type, .mappings.doc._all)' ) \" Check the new index is well created: curl -XGET http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open new_the_hive_15 A2KLoZPpSXygutlfy_RNCQ 5 1 0 0 1.1kb 1.1kb green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb 33.2mb","title":"Create a new index"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#proceed-to-reindex","text":"Next operation lies in running the reindex command in the newly created index: curl -XPOST -H 'Content-Type: application/json' http://localhost:9200/_reindex -d '{ \"conflicts\": \"proceed\", \"source\": { \"index\": \"the_hive_15\" }, \"dest\": { \"index\": \"new_the_hive_15\" } }' After a moment, you should get a similar output: { \"took\" : 5119 , \"timed_out\" : false , \"total\" : 5889 , \"updated\" : 0 , \"created\" : 5889 , \"deleted\" : 0 , \"batches\" : 6 , \"version_conflicts\" : 0 , \"noops\" : 0 , \"retries\" : { \"bulk\" : 0 , \"search\" : 0 }, \"throttled_millis\" : 0 , \"requests_per_second\" : -1.0 , \"throttled_until_millis\" : 0 , \"failures\" : [] }","title":"Proceed to Reindex"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#ensure-new-index-has-been-created","text":"Run the following command, and ensure the new index is like the current one (size can vary): curl -XGET http://localhost:9200/_cat/indices\\?v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_the_hive_15 GV-3Y8QjTjWw0F-p2sjW6Q 5 0 30977 0 26mb 26mb green open the_hive_15 Oap-I61ySgyv6EAI1ZUTFQ 5 0 30977 36 33.2mb 33.2mb","title":"Ensure new index has been created"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#delete-old-indices","text":"This is the thrilling part. Now the new index new_the_hive_15 is created and similar the_hive_15, older indexes should be completely deleted from the database. To delete index named the_hive_15 , run the following command: curl -XDELETE http://localhost:9200/the_hive_15 Run the same command for older indexes if exist (the_hive_14, the_hive_13....). Elasticsearch 7.x cannot run with index created with Elasticsearch 5.x.","title":"Delete old indices"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#create-an-alias","text":"Before stopping Elasticsearch service, let\u2019s create an alias to keep index names in the future. curl -XPOST -H 'Content-Type: application/json' 'http://localhost:9200/_aliases' -d '{ \"actions\": [ { \"add\": { \"index\": \"new_the_hive_15\", \"alias\": \"the_hive_15\" } } ] }' Doing so will allow TheHive 3.5.0 to find the index without updating the configuration file. Check the alias has been well created by running the following command curl -XGET http://localhost:9200/_alias?pretty The output should look like: { \"new_the_hive_15\" : { \"aliases\" : { \"the_hive_15\" : { } } } }","title":"Create an alias"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#stop-elasticsearch-version-682","text":"sudo service elasticsearch stop","title":"Stop Elasticsearch version 6.8.2"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#update-elasticsearch","text":"Update the configuration of Elastisearch. Configuration file should look like this: [..] http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive script.allowed_types: inline thread_pool.search.queue_size: 100000 thread_pool.write.queue_size: 10000 Now, upgrade Elasticsearch to version 7.x following the documentation for your Operating System, and ensure the service start successfully.","title":"Update Elasticsearch"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#install-or-update-to-thehive-350","text":"","title":"Install or update to TheHive 3.5.0"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#deb-package","text":"If using Debian based Linux operating system, configure it to follow our beta repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then install it by running: sudo apt install thehive or sudo apt install thehive = 3 .5.0-1","title":"DEB package"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#rpm","text":"Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [thehive-project] enabled=1 priority=1 name=TheHive-Project RPM repository baseurl=http://rpm.thehive-project.org/release/noarch gpgcheck=1 Then install it by running: sudo yum install thehive or sudo yum install thehive-3.5.0-1","title":"RPM"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#install-binaries","text":"cd /opt wget https://download.thehive-project.org/thehive-3.5.0-1.zip unzip thehive-3.5.0-1.zip ln -s thehive-3.5.0-1 thehive","title":"Install binaries"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#docker-images","text":"Docker images are also provided on Dockerhub. docker pull thehiveproject/thehive:3.5.0-1","title":"Docker images"},{"location":"thehive/legacy/thehive3/admin/upgrade_to_thehive_3_5_and_es_7_x/#update-database","text":"Connect to TheHive, the maintenance page should ask to update. Once updated, ensure a new index named the_hive_16 has been created. curl -XGET http://localhost:9200/_cat/indices \\? v The output should look like this: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open new_the_hive_15 GV-3Y8QjTjWw0F-p2sjW6Q 5 0 30977 0 26mb 26mb yellow open the_hive_16 Nz0vCKqhRK2xkx1t_WF-0g 5 1 30977 0 26.1mb 26.1mb","title":"Update Database"},{"location":"thehive/legacy/thehive3/admin/webhooks/","text":"WebHooks # Starting from version 2.13, TheHive supports webhooks . When enabled, TheHive will send each action that has been performed on it (add case, update case, add task, ...), in real time, to an HTTP endpoint. You can then create a program or application on the HTTP endpoint to react on specific events. Configuration Data Sent to the HTTP Endpoint Sample Webhook Server Application Dependencies Python Script Run Configuration # Webhooks are configured using the webhook key in the configuration file ( /etc/thehive/application.conf by default). A minimal configuration contains an arbitrary name and an URL. The URL corresponds to the HTTP endpoint: webhooks { myLocalWebHook { url = \"http://my_HTTP_endpoint/webhook\" } } Proxy and SSL configuration can be added in the same manner as for MISP or Cortex: webhooks { securedWebHook { url = \"https://my_HTTP_endpoint/webhook\" ws { ssl.trustManager { stores = [ { type: \"JKS\" // JKS or PEM path: \"keystore.jks\" password: \"password1\" } ] } proxy { host: \"10.1.0.1\" port: 3128 } } } } Data Sent to the HTTP Endpoint # For each action performed on it, TheHive sends an audit trail entry in JSON format to the HTTP endpoint. Here is an example corresponding to the creation of a case: { \"operation\": \"Creation\", # Creation, Update or Delete \"objectType\": \"case\", # Type of object \"objectId\": \"AV6FZsTj0KeanEfOQfd_\", # Object ID \"startDate\": 1505476659427, # When the operation has been done \"requestId\": \"13b17ff13d1cfc56:2b7b048b:15e84f42c33:-8000:426\", # HTTP request ID which has done the operation \"details\": { # Attributes used for creation of update \"customFields\": {}, \"metrics\": {}, \"description\": \"Example of case creation\", \"flag\": false, \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"caseId\": 1445, \"severity\": 2, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [] }, \"base\": true, # Internal information used to determine the main operation when there are several operations for the same request \"rootId\": \"AV6FZsTj0KeanEfOQfd_\", # ID of the root parent of the object (internal use) \"object\": { # The object after the operation \"customFields\": {}, \"metrics\": {}, \"createdBy\": \"me\", \"description\": \"Example of case creation\", \"flag\": false, \"user\": \"me\", \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"createdAt\": 1505476658289, \"caseId\": 1445, \"severity\": 2, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [], \"id\": \"AV6FZsTj0KeanEfOQfd_\", \"_type\": \"case\" } } For an update, the data will look like: { \"operation\": \"Update\", \"details\": { \"severity\": 3 }, \"objectType\": \"case\", \"objectId\": \"AV6FZsTj0KeanEfOQfd_\", \"base\": true, \"startDate\": 1505477372601, \"rootId\": \"AV6FZsTj0KeanEfOQfd_\", \"requestId\": \"13b17ff13d1cfc56:2b7b048b:15e84f42c33:-8000:446\", \"object\": { \"customFields\": {}, \"metrics\": {}, \"createdBy\": \"me\", \"description\": \"Example of case creation\", \"flag\": false, \"user\": \"me\", \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"createdAt\": 1505476658289, \"caseId\": 1445, \"severity\": 3, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [], \"updatedBy\": \"me\", \"updatedAt\": 1505477372246, \"id\": \"AV6FZsTj0KeanEfOQfd_\", \"_type\": \"case\" } } Sample Webhook Server Application # The following application is a sample intended to help you get started with webhooks. It is very basic as it listens to a local port and displays the contents of the received POST JSON data. Dependencies # Install dependencies: sudo pip install flask Python Script # Create a simple Python script (e.g. webhooktest.py ): from flask import Flask, request import json app = Flask(__name__) @app.route('/',methods=['POST']) def foo(): data = json.loads(request.data) print(json.dumps(data, indent=4)) return \"OK\" if __name__ == '__main__': app.run() Run # Run the server: python webhooktest.py","title":"WebHooks"},{"location":"thehive/legacy/thehive3/admin/webhooks/#webhooks","text":"Starting from version 2.13, TheHive supports webhooks . When enabled, TheHive will send each action that has been performed on it (add case, update case, add task, ...), in real time, to an HTTP endpoint. You can then create a program or application on the HTTP endpoint to react on specific events. Configuration Data Sent to the HTTP Endpoint Sample Webhook Server Application Dependencies Python Script Run","title":"WebHooks"},{"location":"thehive/legacy/thehive3/admin/webhooks/#configuration","text":"Webhooks are configured using the webhook key in the configuration file ( /etc/thehive/application.conf by default). A minimal configuration contains an arbitrary name and an URL. The URL corresponds to the HTTP endpoint: webhooks { myLocalWebHook { url = \"http://my_HTTP_endpoint/webhook\" } } Proxy and SSL configuration can be added in the same manner as for MISP or Cortex: webhooks { securedWebHook { url = \"https://my_HTTP_endpoint/webhook\" ws { ssl.trustManager { stores = [ { type: \"JKS\" // JKS or PEM path: \"keystore.jks\" password: \"password1\" } ] } proxy { host: \"10.1.0.1\" port: 3128 } } } }","title":"Configuration"},{"location":"thehive/legacy/thehive3/admin/webhooks/#data-sent-to-the-http-endpoint","text":"For each action performed on it, TheHive sends an audit trail entry in JSON format to the HTTP endpoint. Here is an example corresponding to the creation of a case: { \"operation\": \"Creation\", # Creation, Update or Delete \"objectType\": \"case\", # Type of object \"objectId\": \"AV6FZsTj0KeanEfOQfd_\", # Object ID \"startDate\": 1505476659427, # When the operation has been done \"requestId\": \"13b17ff13d1cfc56:2b7b048b:15e84f42c33:-8000:426\", # HTTP request ID which has done the operation \"details\": { # Attributes used for creation of update \"customFields\": {}, \"metrics\": {}, \"description\": \"Example of case creation\", \"flag\": false, \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"caseId\": 1445, \"severity\": 2, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [] }, \"base\": true, # Internal information used to determine the main operation when there are several operations for the same request \"rootId\": \"AV6FZsTj0KeanEfOQfd_\", # ID of the root parent of the object (internal use) \"object\": { # The object after the operation \"customFields\": {}, \"metrics\": {}, \"createdBy\": \"me\", \"description\": \"Example of case creation\", \"flag\": false, \"user\": \"me\", \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"createdAt\": 1505476658289, \"caseId\": 1445, \"severity\": 2, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [], \"id\": \"AV6FZsTj0KeanEfOQfd_\", \"_type\": \"case\" } } For an update, the data will look like: { \"operation\": \"Update\", \"details\": { \"severity\": 3 }, \"objectType\": \"case\", \"objectId\": \"AV6FZsTj0KeanEfOQfd_\", \"base\": true, \"startDate\": 1505477372601, \"rootId\": \"AV6FZsTj0KeanEfOQfd_\", \"requestId\": \"13b17ff13d1cfc56:2b7b048b:15e84f42c33:-8000:446\", \"object\": { \"customFields\": {}, \"metrics\": {}, \"createdBy\": \"me\", \"description\": \"Example of case creation\", \"flag\": false, \"user\": \"me\", \"title\": \"Test case for webhook\", \"status\": \"Open\", \"owner\": \"me\", \"createdAt\": 1505476658289, \"caseId\": 1445, \"severity\": 3, \"tlp\": 2, \"startDate\": 1505476620000, \"tags\": [], \"updatedBy\": \"me\", \"updatedAt\": 1505477372246, \"id\": \"AV6FZsTj0KeanEfOQfd_\", \"_type\": \"case\" } }","title":"Data Sent to the HTTP Endpoint"},{"location":"thehive/legacy/thehive3/admin/webhooks/#sample-webhook-server-application","text":"The following application is a sample intended to help you get started with webhooks. It is very basic as it listens to a local port and displays the contents of the received POST JSON data.","title":"Sample Webhook Server Application"},{"location":"thehive/legacy/thehive3/admin/webhooks/#dependencies","text":"Install dependencies: sudo pip install flask","title":"Dependencies"},{"location":"thehive/legacy/thehive3/admin/webhooks/#python-script","text":"Create a simple Python script (e.g. webhooktest.py ): from flask import Flask, request import json app = Flask(__name__) @app.route('/',methods=['POST']) def foo(): data = json.loads(request.data) print(json.dumps(data, indent=4)) return \"OK\" if __name__ == '__main__': app.run()","title":"Python Script"},{"location":"thehive/legacy/thehive3/admin/webhooks/#run","text":"Run the server: python webhooktest.py","title":"Run"},{"location":"thehive/legacy/thehive3/api/","text":"TheHive API # TheHive exposes REST APIs through JSON over HTTP. HTTP request format Authentication Model Alert Case Observable Task Log User Connectors","title":"TheHive API"},{"location":"thehive/legacy/thehive3/api/#thehive-api","text":"TheHive exposes REST APIs through JSON over HTTP. HTTP request format Authentication Model Alert Case Observable Task Log User Connectors","title":"TheHive API"},{"location":"thehive/legacy/thehive3/api/alert/","text":"Alert # Model definition # Required attributes: - title (text) : title of the alert - description (text) : description of the alert - severity (number) : severity of the alert (1: low; 2: medium; 3: high) default=2 - date (date) : date and time when the alert was raised default=now - tags (multi-string) : case tags default=empty - tlp (number) : TLP ( 0 : white ; 1 : green ; 2: amber ; 3: red ) default=2 - status (AlertStatus) : status of the alert ( New , Updated , Ignored , Imported ) default=New - type (string) : type of the alert (read only) - source (string) : source of the alert (read only) - sourceRef (string) : source reference of the alert (read only) - artifacts (multi-artifact) : artifact of the alert. It is a array of JSON object containing artifact attributes default=empty - follow (boolean) : if true, the alert becomes active when updated default=true Optional attributes: - caseTemplate (string) : case template to use when a case is created from this alert. If the alert specifies a non-existent case template or doesn't supply one, TheHive will import the alert into a case using a case template that has the exact same name as the alert's type if it exists. For example, if you raise an alert with a type value of splunk and you do not provide the caseTemplate attribute or supply a non-existent one (for example splink ), TheHive will import the alert using the case template called splunk if it exists. Otherwise, the alert will be imported using an empty case (i.e. from scratch). Attributes generated by the backend: - lastSyncDate (date) : date of the last synchronization - case (string) : id of the case, if created Alert ID is computed from type , source and sourceRef . Alert Manipulation # Alert methods # HTTP Method URI Action GET /api/alert List alerts POST /api/alert/_search Find alerts PATCH /api/alert/_bulk Update alerts in bulk POST /api/alert/_stats Compute stats on alerts POST /api/alert Create an alert GET /api/alert/:alertId Get an alert PATCH /api/alert/:alertId Update an alert DELETE /api/alert/:alertId Delete an alert POST /api/alert/:alertId/markAsRead Mark an alert as read POST /api/alert/:alertId/markAsUnread Mark an alert as unread POST /api/alert/:alertId/createCase Create a case from an alert POST /api/alert/:alertId/follow Follow an alert POST /api/alert/:alertId/unfollow Unfollow an alert POST /api/alert/:alertId/merge/:caseId Merge an alert in a case POST /api/alert/merge/_bulk Merge several alerts in one case Get an alert # An alert's details can be retrieve using the url: GET /api/alert/:alertId The alert ID is obtained by List alerts or Find alerts API. If the parameter similarity is set to \"1\" or \"true\", this API returns information on cases which have similar observables. With this feature, output will contain the similarCases attribute which list case details with: - artifactCount: number of observables in the original case - iocCount: number of observables marked as IOC in original case - similarArtifactCount: number of observables which are in alert and in case - similarIocCount: number of IOCs which are in alert and in case warning IOCs are observables Examples # Get alert without similarity data: curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810 It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\" } Get alert with similarity data: curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810?similarity=1 It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\", \"similarCases\": [ { \"_id\": \"AVwwrym-Rw5vhyJUfdJW\", \"artifactCount\": 5, \"endDate\": null, \"id\": \"AVwwrym-Rw5vhyJUfdJW\", \"iocCount\": 1, \"resolutionStatus\": null, \"severity\": 1, \"similarArtifactCount\": 2, \"similarIocCount\": 1, \"startDate\": 1495465039000, \"status\": \"Open\", \"tags\": [ \"src:MISP\" ], \"caseId\": 1405, \"title\": \"TEST TheHive\", \"tlp\": 2 } ] } Create an alert # An alert can be created using the following url: POST /api/alert Required case attributes (cf. models) must be provided. If an alert with the same tuple type , source and sourceRef already exists, TheHive will refuse to create it. This call returns attributes of the created alert. Examples # Creation of a simple alert: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert -d '{ \"title\": \"New Alert\", \"description\": \"N/A\", \"type\": \"external\", \"source\": \"instance1\", \"sourceRef\": \"alert-ref\" }' It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\" } Creation of another alert: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert -d '{ \"title\": \"Other alert\", \"description\": \"alert description\", \"type\": \"external\", \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"severity\": 3, \"tlp\": 3, \"artifacts\": [ { \"dataType\": \"ip\", \"data\": \"127.0.0.1\", \"message\": \"localhost\" }, { \"dataType\": \"domain\", \"data\": \"thehive-project.org\", \"tags\": [\"home\", \"TheHive\"] }, { \"dataType\": \"file\", \"data\": \"logo.svg;image/svg+xml;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\", \"message\": \"logo\" } ], \"caseTemplate\": \"external-alert\" }' Merge an alert # An alert can be merge in a case using the URL: POST /api/alert/:alertId/merge/:caseId Each observable of the alert will be added to the case if it doesn't exist in the case. The description of the alert will be appended to the case's description. The HTTP response contains the updated case. Example # Merge the alert ce2c00f17132359cb3c50dfbb1901810 in case AVXeF-pZmeHK_2HEYj2z : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810/merge/AVXeF-pZmeHK_2HEYj2z The call returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script ### Merged with alert #10 my alert title This is my alert description\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_type\":\"case\" } Bulk merge alert # This API merge several alerts with one case: POST /api/alert/merge/_bulk The observable of each alert listed in alertIds field will be imported into the case (identified by caseId field). The description of the case is not modified. The HTTP response contains the case. Example # Merge the alerts ce2c00f17132359cb3c50dfbb1901810 and a97148693200f731cfa5237ff2edf67b in case AVXeF-pZmeHK_2HEYj2z : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert/merge/_bulk -d '{ \"caseId\": \"AVXeF-pZmeHK_2HEYj2z\", \"alertIds\": [\"ce2c00f17132359cb3c50dfbb1901810\", \"a97148693200f731cfa5237ff2edf67b\"] }' The call returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_type\":\"case\" }","title":"Alert"},{"location":"thehive/legacy/thehive3/api/alert/#alert","text":"","title":"Alert"},{"location":"thehive/legacy/thehive3/api/alert/#model-definition","text":"Required attributes: - title (text) : title of the alert - description (text) : description of the alert - severity (number) : severity of the alert (1: low; 2: medium; 3: high) default=2 - date (date) : date and time when the alert was raised default=now - tags (multi-string) : case tags default=empty - tlp (number) : TLP ( 0 : white ; 1 : green ; 2: amber ; 3: red ) default=2 - status (AlertStatus) : status of the alert ( New , Updated , Ignored , Imported ) default=New - type (string) : type of the alert (read only) - source (string) : source of the alert (read only) - sourceRef (string) : source reference of the alert (read only) - artifacts (multi-artifact) : artifact of the alert. It is a array of JSON object containing artifact attributes default=empty - follow (boolean) : if true, the alert becomes active when updated default=true Optional attributes: - caseTemplate (string) : case template to use when a case is created from this alert. If the alert specifies a non-existent case template or doesn't supply one, TheHive will import the alert into a case using a case template that has the exact same name as the alert's type if it exists. For example, if you raise an alert with a type value of splunk and you do not provide the caseTemplate attribute or supply a non-existent one (for example splink ), TheHive will import the alert using the case template called splunk if it exists. Otherwise, the alert will be imported using an empty case (i.e. from scratch). Attributes generated by the backend: - lastSyncDate (date) : date of the last synchronization - case (string) : id of the case, if created Alert ID is computed from type , source and sourceRef .","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/alert/#alert-manipulation","text":"","title":"Alert Manipulation"},{"location":"thehive/legacy/thehive3/api/alert/#alert-methods","text":"HTTP Method URI Action GET /api/alert List alerts POST /api/alert/_search Find alerts PATCH /api/alert/_bulk Update alerts in bulk POST /api/alert/_stats Compute stats on alerts POST /api/alert Create an alert GET /api/alert/:alertId Get an alert PATCH /api/alert/:alertId Update an alert DELETE /api/alert/:alertId Delete an alert POST /api/alert/:alertId/markAsRead Mark an alert as read POST /api/alert/:alertId/markAsUnread Mark an alert as unread POST /api/alert/:alertId/createCase Create a case from an alert POST /api/alert/:alertId/follow Follow an alert POST /api/alert/:alertId/unfollow Unfollow an alert POST /api/alert/:alertId/merge/:caseId Merge an alert in a case POST /api/alert/merge/_bulk Merge several alerts in one case","title":"Alert methods"},{"location":"thehive/legacy/thehive3/api/alert/#get-an-alert","text":"An alert's details can be retrieve using the url: GET /api/alert/:alertId The alert ID is obtained by List alerts or Find alerts API. If the parameter similarity is set to \"1\" or \"true\", this API returns information on cases which have similar observables. With this feature, output will contain the similarCases attribute which list case details with: - artifactCount: number of observables in the original case - iocCount: number of observables marked as IOC in original case - similarArtifactCount: number of observables which are in alert and in case - similarIocCount: number of IOCs which are in alert and in case warning IOCs are observables","title":"Get an alert"},{"location":"thehive/legacy/thehive3/api/alert/#examples","text":"Get alert without similarity data: curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810 It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\" } Get alert with similarity data: curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810?similarity=1 It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\", \"similarCases\": [ { \"_id\": \"AVwwrym-Rw5vhyJUfdJW\", \"artifactCount\": 5, \"endDate\": null, \"id\": \"AVwwrym-Rw5vhyJUfdJW\", \"iocCount\": 1, \"resolutionStatus\": null, \"severity\": 1, \"similarArtifactCount\": 2, \"similarIocCount\": 1, \"startDate\": 1495465039000, \"status\": \"Open\", \"tags\": [ \"src:MISP\" ], \"caseId\": 1405, \"title\": \"TEST TheHive\", \"tlp\": 2 } ] }","title":"Examples"},{"location":"thehive/legacy/thehive3/api/alert/#create-an-alert","text":"An alert can be created using the following url: POST /api/alert Required case attributes (cf. models) must be provided. If an alert with the same tuple type , source and sourceRef already exists, TheHive will refuse to create it. This call returns attributes of the created alert.","title":"Create an alert"},{"location":"thehive/legacy/thehive3/api/alert/#examples_1","text":"Creation of a simple alert: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert -d '{ \"title\": \"New Alert\", \"description\": \"N/A\", \"type\": \"external\", \"source\": \"instance1\", \"sourceRef\": \"alert-ref\" }' It returns: { \"_id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"_type\": \"alert\", \"artifacts\": [], \"createdAt\": 1495012062014, \"createdBy\": \"myuser\", \"date\": 1495012062016, \"description\": \"N/A\", \"follow\": true, \"id\": \"ce2c00f17132359cb3c50dfbb1901810\", \"lastSyncDate\": 1495012062016, \"severity\": 2, \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"status\": \"New\", \"title\": \"New Alert\", \"tlp\": 2, \"type\": \"external\", \"user\": \"myuser\" } Creation of another alert: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert -d '{ \"title\": \"Other alert\", \"description\": \"alert description\", \"type\": \"external\", \"source\": \"instance1\", \"sourceRef\": \"alert-ref\", \"severity\": 3, \"tlp\": 3, \"artifacts\": [ { \"dataType\": \"ip\", \"data\": \"127.0.0.1\", \"message\": \"localhost\" }, { \"dataType\": \"domain\", \"data\": \"thehive-project.org\", \"tags\": [\"home\", \"TheHive\"] }, { \"dataType\": \"file\", \"data\": \"logo.svg;image/svg+xml;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\", \"message\": \"logo\" } ], \"caseTemplate\": \"external-alert\" }'","title":"Examples"},{"location":"thehive/legacy/thehive3/api/alert/#merge-an-alert","text":"An alert can be merge in a case using the URL: POST /api/alert/:alertId/merge/:caseId Each observable of the alert will be added to the case if it doesn't exist in the case. The description of the alert will be appended to the case's description. The HTTP response contains the updated case.","title":"Merge an alert"},{"location":"thehive/legacy/thehive3/api/alert/#example","text":"Merge the alert ce2c00f17132359cb3c50dfbb1901810 in case AVXeF-pZmeHK_2HEYj2z : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/alert/ce2c00f17132359cb3c50dfbb1901810/merge/AVXeF-pZmeHK_2HEYj2z The call returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script ### Merged with alert #10 my alert title This is my alert description\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_type\":\"case\" }","title":"Example"},{"location":"thehive/legacy/thehive3/api/alert/#bulk-merge-alert","text":"This API merge several alerts with one case: POST /api/alert/merge/_bulk The observable of each alert listed in alertIds field will be imported into the case (identified by caseId field). The description of the case is not modified. The HTTP response contains the case.","title":"Bulk merge alert"},{"location":"thehive/legacy/thehive3/api/alert/#example_1","text":"Merge the alerts ce2c00f17132359cb3c50dfbb1901810 and a97148693200f731cfa5237ff2edf67b in case AVXeF-pZmeHK_2HEYj2z : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/alert/merge/_bulk -d '{ \"caseId\": \"AVXeF-pZmeHK_2HEYj2z\", \"alertIds\": [\"ce2c00f17132359cb3c50dfbb1901810\", \"a97148693200f731cfa5237ff2edf67b\"] }' The call returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_id\": \"AVXeF-pZmeHK_2HEYj2z\", \"_type\":\"case\" }","title":"Example"},{"location":"thehive/legacy/thehive3/api/artifact/","text":"Observable # Model definition # Required attributes: data (string) : content of the observable (read only). An observable can't contain data and attachment attributes attachment (attachment) : observable file content (read-only). An observable can't contain data and attachment attributes dataType (enumeration) : type of the observable (read only) message (text) : description of the observable in the context of the case startDate (date) : date of the observable creation default=now tlp (number) : TLP ( 0 : white ; 1 : green ; 2 : amber ; 3 : red ) default=2 ioc (boolean) : indicates if the observable is an IOC default=false status (artifactStatus) : status of the observable ( Ok or Deleted ) default=Ok Optional attributes: - tags (multi-string) : observable tags Observable manipulation # Observable methods # HTTP Method URI Action POST /api/case/artifact/_search Find observables POST /api/case/artifact/_stats Compute stats on observables POST /api/case/:caseId/artifact Create an observable GET /api/case/artifact/:artifactId Get an observable DELETE /api/case/artifact/:artifactId Remove an observable PATCH /api/case/artifact/:artifactId Update an observable GET /api/case/artifact/:artifactId/similar Get list of similar observables PATCH /api/case/artifact/_bulk Update observables in bulk List Observables of a Case # Complete observable list of a case can be retrieved by performing a search: POST /api/case/artifact/_search Parameters: - query : { \"_parent\": { \"_type\": \"case\", \"_query\": { \"_id\": \"<<caseId>>\" } } } - range : all \\<\\<caseId>> must be replaced by case id (not the case number !)","title":"Observable"},{"location":"thehive/legacy/thehive3/api/artifact/#observable","text":"","title":"Observable"},{"location":"thehive/legacy/thehive3/api/artifact/#model-definition","text":"Required attributes: data (string) : content of the observable (read only). An observable can't contain data and attachment attributes attachment (attachment) : observable file content (read-only). An observable can't contain data and attachment attributes dataType (enumeration) : type of the observable (read only) message (text) : description of the observable in the context of the case startDate (date) : date of the observable creation default=now tlp (number) : TLP ( 0 : white ; 1 : green ; 2 : amber ; 3 : red ) default=2 ioc (boolean) : indicates if the observable is an IOC default=false status (artifactStatus) : status of the observable ( Ok or Deleted ) default=Ok Optional attributes: - tags (multi-string) : observable tags","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/artifact/#observable-manipulation","text":"","title":"Observable manipulation"},{"location":"thehive/legacy/thehive3/api/artifact/#observable-methods","text":"HTTP Method URI Action POST /api/case/artifact/_search Find observables POST /api/case/artifact/_stats Compute stats on observables POST /api/case/:caseId/artifact Create an observable GET /api/case/artifact/:artifactId Get an observable DELETE /api/case/artifact/:artifactId Remove an observable PATCH /api/case/artifact/:artifactId Update an observable GET /api/case/artifact/:artifactId/similar Get list of similar observables PATCH /api/case/artifact/_bulk Update observables in bulk","title":"Observable methods"},{"location":"thehive/legacy/thehive3/api/artifact/#list-observables-of-a-case","text":"Complete observable list of a case can be retrieved by performing a search: POST /api/case/artifact/_search Parameters: - query : { \"_parent\": { \"_type\": \"case\", \"_query\": { \"_id\": \"<<caseId>>\" } } } - range : all \\<\\<caseId>> must be replaced by case id (not the case number !)","title":"List Observables of a Case"},{"location":"thehive/legacy/thehive3/api/authentication/","text":"Authentication # Most API calls require authentication. Credentials can be provided using a session cookie, an API key or directly using HTTP basic authentication (when enabled). Session cookie is suitable for browser authentication, not for a dedicated tool. The easiest solution if you want to write a tool that leverages TheHive's API is to use API key authentication. API keys can be generated using the Web interface of the product, under the user admin area. For example, to list cases, use the following curl command: # Using API key curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/case TheHive also supports basic authentication (disabled by default). You can enable it by adding auth.method.basic=true in the configuration file. # Using basic authentication curl -u mylogin:mypassword http://127.0.0.1:9000/api/case","title":"Authentication"},{"location":"thehive/legacy/thehive3/api/authentication/#authentication","text":"Most API calls require authentication. Credentials can be provided using a session cookie, an API key or directly using HTTP basic authentication (when enabled). Session cookie is suitable for browser authentication, not for a dedicated tool. The easiest solution if you want to write a tool that leverages TheHive's API is to use API key authentication. API keys can be generated using the Web interface of the product, under the user admin area. For example, to list cases, use the following curl command: # Using API key curl -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/case TheHive also supports basic authentication (disabled by default). You can enable it by adding auth.method.basic=true in the configuration file. # Using basic authentication curl -u mylogin:mypassword http://127.0.0.1:9000/api/case","title":"Authentication"},{"location":"thehive/legacy/thehive3/api/case/","text":"Case # Model definition # Required attributes: - title (text) : title of the case - description (text) : description of the case - severity (number) : severity of the case (1: low; 2: medium; 3: high) default=2 - startDate (date) : date and time of the begin of the case default=now - owner (string) : user to whom the case has been assigned default=use who create the case - flag (boolean) : flag of the case default=false - tlp (number) : TLP ( 0 : white ; 1 : green ; 2: amber ; 3: red ) default=2 - tags (multi-string) : case tags default=empty Optional attributes: - resolutionStatus (caseResolutionStatus) : resolution status of the case ( Indeterminate , FalsePositive , TruePositive , Other or Duplicated ) - impactStatus (caseImpactStatus) : impact status of the case ( NoImpact , WithImpact or NotApplicable ) - summary (text) : summary of the case, to be provided when closing a case - endDate (date) : resolution date - metrics (metrics) : list of metrics Attributes generated by the backend: - status (caseStatus) : status of the case ( Open , Resolved or Deleted ) default=Open - caseId (number) : Id of the case (auto-generated) - mergeInto (string) : ID of the case created by the merge - mergeFrom (multi-string) : IDs of the cases that were merged Case Manipulation # Case methods # HTTP Method URI Action GET /api/case List cases POST /api/case/_search Find cases PATCH /api/case/_bulk Update cases in bulk POST /api/case/_stats Compute stats on cases POST /api/case Create a case GET /api/case/:caseId Get a case PATCH /api/case/:caseId Update a case DELETE /api/case/:caseId Remove a case GET /api/case/:caseId/links Get list of cases linked to this case POST /api/case/:caseId1/_merge/:caseId2 Merge two cases Create a Case # A case can be created using the following url : POST /api/case Required case attributes (cf. models) must be provided. This call returns attributes of the created case. Examples # Creation of a simple case: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My first case\", \"description\": \"This case has been created by my custom script\" }' It returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVqqdpY2yQ6w1DNC8aDh\", \"_id\": \"AVqqdpY2yQ6w1DNC8aDh\", \"_type\":\"case\" } Creation of another case: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My second case\", \"description\": \"This case has been created by my custom script, its severity is high, tlp is red and it contains tags\", \"severity\": 3, \"tlp\": 3, \"tags\": [\"automatic\", \"creation\"] }' Creating a case with Tasks & Customfields: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My first case\", \"description\": \"This case has been created by my custom script\" \"tasks\": [{ \"title\": \"mytask\", \"description\": \"description of my task\" }], \"customFields\": { \"cvss\": { \"number\": 9, }, \"businessImpact\": { \"string\": \"HIGH\" } } }' For the customFields object, the attribute names should correspond to the ExternalReference (cvss and businessImpact in the example above) not to the name of custom fields.","title":"Case"},{"location":"thehive/legacy/thehive3/api/case/#case","text":"","title":"Case"},{"location":"thehive/legacy/thehive3/api/case/#model-definition","text":"Required attributes: - title (text) : title of the case - description (text) : description of the case - severity (number) : severity of the case (1: low; 2: medium; 3: high) default=2 - startDate (date) : date and time of the begin of the case default=now - owner (string) : user to whom the case has been assigned default=use who create the case - flag (boolean) : flag of the case default=false - tlp (number) : TLP ( 0 : white ; 1 : green ; 2: amber ; 3: red ) default=2 - tags (multi-string) : case tags default=empty Optional attributes: - resolutionStatus (caseResolutionStatus) : resolution status of the case ( Indeterminate , FalsePositive , TruePositive , Other or Duplicated ) - impactStatus (caseImpactStatus) : impact status of the case ( NoImpact , WithImpact or NotApplicable ) - summary (text) : summary of the case, to be provided when closing a case - endDate (date) : resolution date - metrics (metrics) : list of metrics Attributes generated by the backend: - status (caseStatus) : status of the case ( Open , Resolved or Deleted ) default=Open - caseId (number) : Id of the case (auto-generated) - mergeInto (string) : ID of the case created by the merge - mergeFrom (multi-string) : IDs of the cases that were merged","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/case/#case-manipulation","text":"","title":"Case Manipulation"},{"location":"thehive/legacy/thehive3/api/case/#case-methods","text":"HTTP Method URI Action GET /api/case List cases POST /api/case/_search Find cases PATCH /api/case/_bulk Update cases in bulk POST /api/case/_stats Compute stats on cases POST /api/case Create a case GET /api/case/:caseId Get a case PATCH /api/case/:caseId Update a case DELETE /api/case/:caseId Remove a case GET /api/case/:caseId/links Get list of cases linked to this case POST /api/case/:caseId1/_merge/:caseId2 Merge two cases","title":"Case methods"},{"location":"thehive/legacy/thehive3/api/case/#create-a-case","text":"A case can be created using the following url : POST /api/case Required case attributes (cf. models) must be provided. This call returns attributes of the created case.","title":"Create a Case"},{"location":"thehive/legacy/thehive3/api/case/#examples","text":"Creation of a simple case: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My first case\", \"description\": \"This case has been created by my custom script\" }' It returns: { \"severity\": 3, \"createdBy\": \"myuser\", \"createdAt\": 1488918582777, \"caseId\": 1, \"title\": \"My first case\", \"startDate\": 1488918582836, \"owner\": \"myuser\", \"status\": \"Open\", \"description\": \"This case has been created by my custom script\", \"user\": \"myuser\", \"tlp\": 2, \"flag\": false, \"id\": \"AVqqdpY2yQ6w1DNC8aDh\", \"_id\": \"AVqqdpY2yQ6w1DNC8aDh\", \"_type\":\"case\" } Creation of another case: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My second case\", \"description\": \"This case has been created by my custom script, its severity is high, tlp is red and it contains tags\", \"severity\": 3, \"tlp\": 3, \"tags\": [\"automatic\", \"creation\"] }' Creating a case with Tasks & Customfields: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case -d '{ \"title\": \"My first case\", \"description\": \"This case has been created by my custom script\" \"tasks\": [{ \"title\": \"mytask\", \"description\": \"description of my task\" }], \"customFields\": { \"cvss\": { \"number\": 9, }, \"businessImpact\": { \"string\": \"HIGH\" } } }' For the customFields object, the attribute names should correspond to the ExternalReference (cvss and businessImpact in the example above) not to the name of custom fields.","title":"Examples"},{"location":"thehive/legacy/thehive3/api/log/","text":"Log # Model definition # Required attributes: - message (text) : content of the Log - startDate (date) : date of the log submission default=now - status (logStatus) : status of the log ( Ok or Deleted ) default=Ok Optional attributes: - attachment (attachment) : file attached to the log Log manipulation # Log methods # HTTP Method URI Action GET /api/case/task/:taskId/log Get logs of the task POST /api/case/task/:taskId/log/_search Find logs in specified task POST /api/case/task/log/_search Find logs POST /api/case/task/:taskId/log Create a log PATCH /api/case/task/log/:logId Update a log DELETE /api/case/task/log/:logId Remove a log GET /api/case/task/log/:logId Get a log Create a log # The URL used to create a task is: POST /api/case/task/<<taskId>>/log \\<\\<taskId>> must be replaced by task id Required log attributes (cf. models) must be provided. This call returns attributes of the created log. Examples # Creation of a simple log in task AVqqeXc9yQ6w1DNC8aDj : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/task/AVqqeXc9yQ6w1DNC8aDj/log -d '{ \"message\": \"Some message\" }' It returns: { \"startDate\": 1488919949497, \"createdBy\": \"admin\", \"createdAt\": 1488919949495, \"user\": \"myuser\", \"message\":\"Some message\", \"status\": \"Ok\", \"id\": \"AVqqi3C-yQ6w1DNC8aDq\", \"_id\": \"AVqqi3C-yQ6w1DNC8aDq\", \"_type\":\"case_task_log\" } If log contains an attachment, the request must be in multipart format: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/case/task/AVqqeXc9yQ6w1DNC8aDj/log -F '_json={\"message\": \"Screenshot of fake site\"};type=application/json' -F 'attachment=@screenshot1.png;type=image/png' It returns: { \"createdBy\": \"myuser\", \"message\": \"Screenshot of fake site\", \"createdAt\": 1488920587391, \"startDate\": 1488920587394, \"user\": \"myuser\", \"status\": \"Ok\", \"attachment\": { \"name\": \"screenshot1.png\", \"hashes\": [ \"086541e99743c6752f5fd4931e256e6e8d5fc7afe47488fb9e0530c390d0ca65\", \"8b81e038ae0809488f20b5ec7dc91e488ef601e2\", \"c5883708f42a00c3ab1fba5bbb65786c\" ], \"size\": 15296, \"contentType\": \"image/png\", \"id\": \"086541e99743c6752f5fd4931e256e6e8d5fc7afe47488fb9e0530c390d0ca65\" }, \"id\": \"AVqqlSy0yQ6w1DNC8aDx\", \"_id\": \"AVqqlSy0yQ6w1DNC8aDx\", \"_type\": \"case_task_log\" }","title":"Log"},{"location":"thehive/legacy/thehive3/api/log/#log","text":"","title":"Log"},{"location":"thehive/legacy/thehive3/api/log/#model-definition","text":"Required attributes: - message (text) : content of the Log - startDate (date) : date of the log submission default=now - status (logStatus) : status of the log ( Ok or Deleted ) default=Ok Optional attributes: - attachment (attachment) : file attached to the log","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/log/#log-manipulation","text":"","title":"Log manipulation"},{"location":"thehive/legacy/thehive3/api/log/#log-methods","text":"HTTP Method URI Action GET /api/case/task/:taskId/log Get logs of the task POST /api/case/task/:taskId/log/_search Find logs in specified task POST /api/case/task/log/_search Find logs POST /api/case/task/:taskId/log Create a log PATCH /api/case/task/log/:logId Update a log DELETE /api/case/task/log/:logId Remove a log GET /api/case/task/log/:logId Get a log","title":"Log methods"},{"location":"thehive/legacy/thehive3/api/log/#create-a-log","text":"The URL used to create a task is: POST /api/case/task/<<taskId>>/log \\<\\<taskId>> must be replaced by task id Required log attributes (cf. models) must be provided. This call returns attributes of the created log.","title":"Create a log"},{"location":"thehive/legacy/thehive3/api/log/#examples","text":"Creation of a simple log in task AVqqeXc9yQ6w1DNC8aDj : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/task/AVqqeXc9yQ6w1DNC8aDj/log -d '{ \"message\": \"Some message\" }' It returns: { \"startDate\": 1488919949497, \"createdBy\": \"admin\", \"createdAt\": 1488919949495, \"user\": \"myuser\", \"message\":\"Some message\", \"status\": \"Ok\", \"id\": \"AVqqi3C-yQ6w1DNC8aDq\", \"_id\": \"AVqqi3C-yQ6w1DNC8aDq\", \"_type\":\"case_task_log\" } If log contains an attachment, the request must be in multipart format: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' http://127.0.0.1:9000/api/case/task/AVqqeXc9yQ6w1DNC8aDj/log -F '_json={\"message\": \"Screenshot of fake site\"};type=application/json' -F 'attachment=@screenshot1.png;type=image/png' It returns: { \"createdBy\": \"myuser\", \"message\": \"Screenshot of fake site\", \"createdAt\": 1488920587391, \"startDate\": 1488920587394, \"user\": \"myuser\", \"status\": \"Ok\", \"attachment\": { \"name\": \"screenshot1.png\", \"hashes\": [ \"086541e99743c6752f5fd4931e256e6e8d5fc7afe47488fb9e0530c390d0ca65\", \"8b81e038ae0809488f20b5ec7dc91e488ef601e2\", \"c5883708f42a00c3ab1fba5bbb65786c\" ], \"size\": 15296, \"contentType\": \"image/png\", \"id\": \"086541e99743c6752f5fd4931e256e6e8d5fc7afe47488fb9e0530c390d0ca65\" }, \"id\": \"AVqqlSy0yQ6w1DNC8aDx\", \"_id\": \"AVqqlSy0yQ6w1DNC8aDx\", \"_type\": \"case_task_log\" }","title":"Examples"},{"location":"thehive/legacy/thehive3/api/model/","text":"TheHive Model Definition # Field Types # string : textual data (example \"malware\"). text : textual data. The difference between string and text is in the way content can be searched. string is searchable as-is whereas text , words (token) are searchable, not the whole content (example \"Ten users have received this ransomware\"). date : date and time using timestamps with milliseconds format. boolean : true or false number : numeric value metrics : JSON object that contains only numbers Field can be prefixed with multi- in order to indicate that multiple values can be provided. Common Attributes # All entities share the following attributes: - createdBy (text) : login of the user who created the entity - createdAt (date) : date and time of the creation - updatedBy (text) : login of the user who last updated the entity - upadtedAt (date) : date and time of the last update - user (text) : same value as createdBy (this field is deprecated) These attributes are handled by the back-end and can't be directly updated.","title":"TheHive Model Definition"},{"location":"thehive/legacy/thehive3/api/model/#thehive-model-definition","text":"","title":"TheHive Model Definition"},{"location":"thehive/legacy/thehive3/api/model/#field-types","text":"string : textual data (example \"malware\"). text : textual data. The difference between string and text is in the way content can be searched. string is searchable as-is whereas text , words (token) are searchable, not the whole content (example \"Ten users have received this ransomware\"). date : date and time using timestamps with milliseconds format. boolean : true or false number : numeric value metrics : JSON object that contains only numbers Field can be prefixed with multi- in order to indicate that multiple values can be provided.","title":"Field Types"},{"location":"thehive/legacy/thehive3/api/model/#common-attributes","text":"All entities share the following attributes: - createdBy (text) : login of the user who created the entity - createdAt (date) : date and time of the creation - updatedBy (text) : login of the user who last updated the entity - upadtedAt (date) : date and time of the last update - user (text) : same value as createdBy (this field is deprecated) These attributes are handled by the back-end and can't be directly updated.","title":"Common Attributes"},{"location":"thehive/legacy/thehive3/api/request/","text":"Request formats # TheHive accepts several parameter formats within a HTTP request. They can be used indifferently. Input data can be: - a query string - URL-encoded form - multi-part - JSON Hence, the requests below are equivalent. Query String # curl -XPOST 'http://127.0.0.1:9000/api/login?user=me&password=secret' URL-encoded Form # curl -XPOST 'http://127.0.0.1:9000/api/login' -d user=me -d password=secret JSON # curl -XPOST http://127.0.0.1:9000/api/login -H 'Content-Type: application/json' -d '{ \"user\": \"me\", \"password\": \"secret\" }' Multi-part # curl -XPOST http://127.0.0.1:9000/api/login -F '_json=<-;type=application/json' << _EOF_ { \"user\": \"me\", \"password\": \"secret\" } _EOF_ ResponseFormat # TheHive outputs JSON data.","title":"Request"},{"location":"thehive/legacy/thehive3/api/request/#request-formats","text":"TheHive accepts several parameter formats within a HTTP request. They can be used indifferently. Input data can be: - a query string - URL-encoded form - multi-part - JSON Hence, the requests below are equivalent.","title":"Request formats"},{"location":"thehive/legacy/thehive3/api/request/#query-string","text":"curl -XPOST 'http://127.0.0.1:9000/api/login?user=me&password=secret'","title":"Query String"},{"location":"thehive/legacy/thehive3/api/request/#url-encoded-form","text":"curl -XPOST 'http://127.0.0.1:9000/api/login' -d user=me -d password=secret","title":"URL-encoded Form"},{"location":"thehive/legacy/thehive3/api/request/#json","text":"curl -XPOST http://127.0.0.1:9000/api/login -H 'Content-Type: application/json' -d '{ \"user\": \"me\", \"password\": \"secret\" }'","title":"JSON"},{"location":"thehive/legacy/thehive3/api/request/#multi-part","text":"curl -XPOST http://127.0.0.1:9000/api/login -F '_json=<-;type=application/json' << _EOF_ { \"user\": \"me\", \"password\": \"secret\" } _EOF_","title":"Multi-part"},{"location":"thehive/legacy/thehive3/api/request/#responseformat","text":"TheHive outputs JSON data.","title":"ResponseFormat"},{"location":"thehive/legacy/thehive3/api/task/","text":"Task # Model definition # Required attributes: - title (text) : title of the task - status (taskStatus) : status of the task ( Waiting , InProgress , Completed or Cancel ) default=Waiting - flag (boolean) : flag of the task default=false Optional attributes: - owner (string) : user who owns the task. This is automatically set to current user when status is set to InProgress - description (text) : task details - startDate (date) : date of the beginning of the task. This is automatically set when status is set to Open - endDate (date) : date of the end of the task. This is automatically set when status is set to Completed Task manipulation # Task methods # HTTP Method URI Action POST /api/case/:caseId/task/_search Find tasks in a case (deprecated) POST /api/case/task/_search Find tasks POST /api/case/task/_stats Compute stats on tasks GET /api/case/task/:taskId Get a task PATCH /api/case/task/:taskId Update a task POST /api/case/:caseId/task Create a task Create a task # The URL used to create a task is: POST /api/case/<<caseId>>/task \\<\\<caseId>> must be replaced by case id (not the case number !) Required task attributes (cf. models) must be provided. This call returns attributes of the created task. Examples # Creation of a simple task in case AVqqdpY2yQ6w1DNC8aDh : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/AVqqdpY2yQ6w1DNC8aDh/task -d '{ \"title\": \"Do something\" }' It returns: { \"createdAt\": 1488918771513, \"status\": \"Waiting\", \"createdBy\": \"myuser\", \"title\": \"Do something\", \"order\": 0, \"user\": \"myuser\", \"flag\": false, \"id\":\"AVqqeXc9yQ6w1DNC8aDj\", \"_id\":\"AVqqeXc9yQ6w1DNC8aDj\", \"_type\":\"case_task\" } Creation of another task: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/AVqqdpY2yQ6w1DNC8aDh/task -d '{ \"title\": \"Analyze the malware\", \"description\": \"The malware XXX is analyzed using sandbox ...\", \"owner\": \"Joe\", \"status\": \"InProgress\" }'","title":"Task"},{"location":"thehive/legacy/thehive3/api/task/#task","text":"","title":"Task"},{"location":"thehive/legacy/thehive3/api/task/#model-definition","text":"Required attributes: - title (text) : title of the task - status (taskStatus) : status of the task ( Waiting , InProgress , Completed or Cancel ) default=Waiting - flag (boolean) : flag of the task default=false Optional attributes: - owner (string) : user who owns the task. This is automatically set to current user when status is set to InProgress - description (text) : task details - startDate (date) : date of the beginning of the task. This is automatically set when status is set to Open - endDate (date) : date of the end of the task. This is automatically set when status is set to Completed","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/task/#task-manipulation","text":"","title":"Task manipulation"},{"location":"thehive/legacy/thehive3/api/task/#task-methods","text":"HTTP Method URI Action POST /api/case/:caseId/task/_search Find tasks in a case (deprecated) POST /api/case/task/_search Find tasks POST /api/case/task/_stats Compute stats on tasks GET /api/case/task/:taskId Get a task PATCH /api/case/task/:taskId Update a task POST /api/case/:caseId/task Create a task","title":"Task methods"},{"location":"thehive/legacy/thehive3/api/task/#create-a-task","text":"The URL used to create a task is: POST /api/case/<<caseId>>/task \\<\\<caseId>> must be replaced by case id (not the case number !) Required task attributes (cf. models) must be provided. This call returns attributes of the created task.","title":"Create a task"},{"location":"thehive/legacy/thehive3/api/task/#examples","text":"Creation of a simple task in case AVqqdpY2yQ6w1DNC8aDh : curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/AVqqdpY2yQ6w1DNC8aDh/task -d '{ \"title\": \"Do something\" }' It returns: { \"createdAt\": 1488918771513, \"status\": \"Waiting\", \"createdBy\": \"myuser\", \"title\": \"Do something\", \"order\": 0, \"user\": \"myuser\", \"flag\": false, \"id\":\"AVqqeXc9yQ6w1DNC8aDj\", \"_id\":\"AVqqeXc9yQ6w1DNC8aDj\", \"_type\":\"case_task\" } Creation of another task: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/case/AVqqdpY2yQ6w1DNC8aDh/task -d '{ \"title\": \"Analyze the malware\", \"description\": \"The malware XXX is analyzed using sandbox ...\", \"owner\": \"Joe\", \"status\": \"InProgress\" }'","title":"Examples"},{"location":"thehive/legacy/thehive3/api/user/","text":"User # Model definition # Required attributes: - login / id (string) : login of the user - userName (text) : Full name of the user - roles (multi-userRole) : Array containing roles of the user ( read , write or admin ) - status (userStatus) : Ok or Locked default=Ok - preference (string) : JSON object containing user preference default={} Optional attributes: - avatar (string) : avatar of user. It is an image encoded in base 64 - password (string) : user password if local authentication is used Attributes generated by the backend: - key (uuid) : API key to authenticate this user (deprecated) User Manipulation # User methods # HTTP Method URI Action GET /api/logout Logout POST /api/login User login GET /api/user/current Get current user POST /api/user/_search Find user POST /api/user Create a user GET /api/user/:userId Get a user DELETE /api/user/:userId Delete a user PATCH /api/user/:userId Update user details POST /api/user/:userId/password/set Set password POST /api/user/:userId/password/change Change password with-key (boolean) Create a User # A user can be created using the following URL: POST /api/user Required case attributes (cf. models) must be provided. This call returns attributes of the created user. This call is authenticated and requires admin role. Examples # Creation of a user: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/user -d '{ \"login\": \"georges\", \"name\": \"Georges Abitbol\", \"roles\": [\"read\", \"write\"], \"password\": \"La classe\" }' It returns: { \"createdBy\": \"myuser\", \"name\":\"Georges Abitbol\", \"roles\": [\"read\", \"write\" ], \"_id\": \"georges\", \"user\": \"myuser\", \"createdAt\": 1496561862924, \"status\": \"Ok\", \"id\": \"georges\", \"_type\": \"user\", \"has-key\":false } If external authentication is used (LDAP or AD) password field must not be provided.","title":"User"},{"location":"thehive/legacy/thehive3/api/user/#user","text":"","title":"User"},{"location":"thehive/legacy/thehive3/api/user/#model-definition","text":"Required attributes: - login / id (string) : login of the user - userName (text) : Full name of the user - roles (multi-userRole) : Array containing roles of the user ( read , write or admin ) - status (userStatus) : Ok or Locked default=Ok - preference (string) : JSON object containing user preference default={} Optional attributes: - avatar (string) : avatar of user. It is an image encoded in base 64 - password (string) : user password if local authentication is used Attributes generated by the backend: - key (uuid) : API key to authenticate this user (deprecated)","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/user/#user-manipulation","text":"","title":"User Manipulation"},{"location":"thehive/legacy/thehive3/api/user/#user-methods","text":"HTTP Method URI Action GET /api/logout Logout POST /api/login User login GET /api/user/current Get current user POST /api/user/_search Find user POST /api/user Create a user GET /api/user/:userId Get a user DELETE /api/user/:userId Delete a user PATCH /api/user/:userId Update user details POST /api/user/:userId/password/set Set password POST /api/user/:userId/password/change Change password with-key (boolean)","title":"User methods"},{"location":"thehive/legacy/thehive3/api/user/#create-a-user","text":"A user can be created using the following URL: POST /api/user Required case attributes (cf. models) must be provided. This call returns attributes of the created user. This call is authenticated and requires admin role.","title":"Create a User"},{"location":"thehive/legacy/thehive3/api/user/#examples","text":"Creation of a user: curl -XPOST -H 'Authorization: Bearer ***API*KEY***' -H 'Content-Type: application/json' http://127.0.0.1:9000/api/user -d '{ \"login\": \"georges\", \"name\": \"Georges Abitbol\", \"roles\": [\"read\", \"write\"], \"password\": \"La classe\" }' It returns: { \"createdBy\": \"myuser\", \"name\":\"Georges Abitbol\", \"roles\": [\"read\", \"write\" ], \"_id\": \"georges\", \"user\": \"myuser\", \"createdAt\": 1496561862924, \"status\": \"Ok\", \"id\": \"georges\", \"_type\": \"user\", \"has-key\":false } If external authentication is used (LDAP or AD) password field must not be provided.","title":"Examples"},{"location":"thehive/legacy/thehive3/api/connectors/","text":"Connectors API # TheHive offers an API to manipulate its various connectors Cortex MISP Metrics","title":"Connectors API"},{"location":"thehive/legacy/thehive3/api/connectors/#connectors-api","text":"TheHive offers an API to manipulate its various connectors Cortex MISP Metrics","title":"Connectors API"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/","text":"Cortex manipulation through TheHive # Cortex can be manipulated through TheHive with JSON over HTTP Job Analyzer","title":"Cortex manipulation through TheHive"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/#cortex-manipulation-through-thehive","text":"Cortex can be manipulated through TheHive with JSON over HTTP Job Analyzer","title":"Cortex manipulation through TheHive"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/analyzer/","text":"Author : R\u00e9mi ALLAIN (rallain@cyberprotect.fr) - Cyberprotect, SDN International Analyzer # Model definition # Attributes: - id (string) : Analyzer id - name (string) : Analyzer name - version (string) : Analyzer version - description (text) : Analyzer description - dataTypeList (multi-string) : List of data type this analyzer can manage - cortexIds (string) : List of Cortex server id Analyzer manipulation # Analyzer methods # HTTP Method URI Action GET /api/connector/cortex/analyzer List all analyzers GET /api/connector/cortex/analyzer/:analyzerId Get details of an analyzer GET /api/connector/cortex/analyzer/type/:dataType List analyzers matching the dataType","title":"Analyzer"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/analyzer/#analyzer","text":"","title":"Analyzer"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/analyzer/#model-definition","text":"Attributes: - id (string) : Analyzer id - name (string) : Analyzer name - version (string) : Analyzer version - description (text) : Analyzer description - dataTypeList (multi-string) : List of data type this analyzer can manage - cortexIds (string) : List of Cortex server id","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/analyzer/#analyzer-manipulation","text":"","title":"Analyzer manipulation"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/analyzer/#analyzer-methods","text":"HTTP Method URI Action GET /api/connector/cortex/analyzer List all analyzers GET /api/connector/cortex/analyzer/:analyzerId Get details of an analyzer GET /api/connector/cortex/analyzer/type/:dataType List analyzers matching the dataType","title":"Analyzer methods"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/","text":"Job # Model definition # Required attributes: - analyzerId (string): identifier of the analyzer used by the job - status (enumeration): status of the job ( InProgress , Success , Failure ) default= InProgress - artifactId (string): identifier of the artifact to analyze - startDate (date): job start date Optional attributes: - endDate (date): job end date - report (string): raw content of the report sent back by the analyzer - cortexId (string): identifier of the cortex server - cortexJobId (string): identifier of the job in the cortex server Job manipulation # Job methods # HTTP Method URI Action POST /api/connector/cortex/job Create a new Cortex job GET /api/connector/cortex/job/:jobId Get a cortex job POST /api/connector/cortex/job/_search Search for cortex jobs Create a new Cortex job # Creating a new job can be done by performing the following query POST /api/connector/cortex/job Parameters: - cortexId : identifier of the Cortex server - artifactId : identifier of the artifact as found with an artifact search - analyzerId : name of the analyzer used by the job","title":"Job"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/#job","text":"","title":"Job"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/#model-definition","text":"Required attributes: - analyzerId (string): identifier of the analyzer used by the job - status (enumeration): status of the job ( InProgress , Success , Failure ) default= InProgress - artifactId (string): identifier of the artifact to analyze - startDate (date): job start date Optional attributes: - endDate (date): job end date - report (string): raw content of the report sent back by the analyzer - cortexId (string): identifier of the cortex server - cortexJobId (string): identifier of the job in the cortex server","title":"Model definition"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/#job-manipulation","text":"","title":"Job manipulation"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/#job-methods","text":"HTTP Method URI Action POST /api/connector/cortex/job Create a new Cortex job GET /api/connector/cortex/job/:jobId Get a cortex job POST /api/connector/cortex/job/_search Search for cortex jobs","title":"Job methods"},{"location":"thehive/legacy/thehive3/api/connectors/cortex/job/#create-a-new-cortex-job","text":"Creating a new job can be done by performing the following query POST /api/connector/cortex/job Parameters: - cortexId : identifier of the Cortex server - artifactId : identifier of the artifact as found with an artifact search - analyzerId : name of the analyzer used by the job","title":"Create a new Cortex job"},{"location":"thehive/legacy/thehive3/api/connectors/misp/","text":"MISP connector # MISP and TheHive can interact between each other in both ways: * TheHive is able to import events from a MISP instance as alerts and create cases from them * TheHive is able to export a case into MISP as an event and update it with the artifacts flagged as IOC as MISP attributes It is possible to use the API to control those behaviours. MISP imports # API methods # HTTP Method URI Action GET /api/connector/misp/_syncAlerts Synchronize from all MISP instances all MISP events published since the last synchronization GET /api/connector/misp/_syncAllAlerts Synchronize from all MISP instances all MISP published events since the beginning GET /api/connector/misp/_syncArtifacts Synchronize all artifacts from already imported alerts from all MISP instances MISP exports # API methods # HTTP Method URI Action POST /api/connector/misp/export/:caseId/:mispName Export a case to MISP Exporting a case to MISP # Exporting a case to MISP can be done by performing the following query POST /api/connector/misp/export/:caseId/:mispName With: * caseId: the elasticsearch id of the case * mispName: the name given to the MISP instance in TheHive configuration No parameters need to be sent in the query body. The response of this query will be a JSON table containing all artifacts sent as attributes in the MISP event.","title":"MISP connector"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#misp-connector","text":"MISP and TheHive can interact between each other in both ways: * TheHive is able to import events from a MISP instance as alerts and create cases from them * TheHive is able to export a case into MISP as an event and update it with the artifacts flagged as IOC as MISP attributes It is possible to use the API to control those behaviours.","title":"MISP connector"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#misp-imports","text":"","title":"MISP imports"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#api-methods","text":"HTTP Method URI Action GET /api/connector/misp/_syncAlerts Synchronize from all MISP instances all MISP events published since the last synchronization GET /api/connector/misp/_syncAllAlerts Synchronize from all MISP instances all MISP published events since the beginning GET /api/connector/misp/_syncArtifacts Synchronize all artifacts from already imported alerts from all MISP instances","title":"API methods"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#misp-exports","text":"","title":"MISP exports"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#api-methods_1","text":"HTTP Method URI Action POST /api/connector/misp/export/:caseId/:mispName Export a case to MISP","title":"API methods"},{"location":"thehive/legacy/thehive3/api/connectors/misp/#exporting-a-case-to-misp","text":"Exporting a case to MISP can be done by performing the following query POST /api/connector/misp/export/:caseId/:mispName With: * caseId: the elasticsearch id of the case * mispName: the name given to the MISP instance in TheHive configuration No parameters need to be sent in the query body. The response of this query will be a JSON table containing all artifacts sent as attributes in the MISP event.","title":"Exporting a case to MISP"},{"location":"thehive/legacy/thehive3/installation/install-guide/","text":"Installation Guide # \u26a0\ufe0f Please read carrefully this documentation. Depending on you make a fresh installation or update an existing version, install version 3 or version 4, repository or packages names may vary. Current supported versions of TheHive are: - Version 3.5.0 and later that supports only Elasticsearch 7.x. - Version 4.0 and later. Instruction to install TheHive supporting Elasticsearch 6.x (EoL in Nov. 2020) are still detailled in this documentation. Before installing TheHive, you need to choose the installation option which suits your environment as described below. Once you have a chosen an option and installed the software, read the Configuration Guide . We also advise reading the Administration Guide . Table of Contents # Installation Options RPM DEB Docker Binary Build it Yourself Elasticsearch Installation System Package Start the Service Elasticsearch inside a Docker Installation Options # TheHive is available as: an RPM package a DEB package a Docker image a binary package In addition, TheHive can be also be built from the source code . RPM # RPM packages are published on a our RPM repository. All packages are signed using our GPG key 562CBC1C . Its fingerprint is: 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C Run the following command to import the GPG key : sudo rpm --import https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY Release versions # The release repository contains packages for TheHive 3.5.0+ and TheHive 4.0.0+ Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/release/noarch gpgcheck = 1 Then you will able to install TheHive 3.5.0+ the package using yum : yum install thehive or install TheHive 4.0.0+ : yum install thehive4 Stable versions (or legacy versions) # The Stable repository is a legacy repository and contains packages for TheHive 3.4.4 that does not support Elasticsearch version 7.x**, but version 6.x. Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/stable/noarch gpgcheck = 1 Then you will able to install TheHive 3.4.4 package using yum : yum install thehive Following beta versions # To follow beta versions of TheHive, use the following setup: And setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/beta/noarch gpgcheck = 1 Then you will able to install beta version of TheHive 3.x package using yum : yum install thehive or install beta version of TheHive 4.x : yum install thehive4 \u26a0\ufe0f We do not recommend that configuration for production servers Once the package is installed, proceed to the configuration using the Configuration Guide . For additional configuration options, please refer to the Administration Guide . DEB # Debian packages are published on a our DEB packages repository. All packages are signed using our GPG key 562CBC1C . Its fingerprint is: 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C Release versions # The release repository contains packages for TheHive 3.5.0+ and TheHive 4.0.0+ Setup apt configuration with the release repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then you will able to install TheHive 3.5.0+ the package using apt command: apt install thehive or install TheHive 4.0.0+ : apt install thehive4 Stable versions (or legacy versions) # The main repository is a legacy repository and contains packages for TheHive 3.4.4 that does not support Elasticsearch version 7.x , but version 6.x. Setup apt configuration with the main repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org stable main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then you will able to install TheHive 3.4.4 package using apt command: apt install thehive Beta versions # To follow beta versions of TheHive, use the following commands: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org beta main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive \u26a0\ufe0f We do not recommend that configuration for production servers Docker # To use the Docker image, you must use Docker (courtesy of Captain Obvious). TheHive requires Elasticsearch to run. You can use docker-compose to start them together in Docker or install and configure Elasticsearch manually. Use Docker-compose # Docker-compose can start multiple dockers and link them together. The following docker-compose.yml file starts Elasticsearch and TheHive: version: \"2\" services: elasticsearch: image: elasticsearch:7.9.1 environment: - http.host=0.0.0.0 - discovery.type=single-node ulimits: nofile: soft: 65536 hard: 65536 cortex: image: thehiveproject/cortex:3.1.0-1 depends_on: - elasticsearch ports: - \"0.0.0.0:9001:9001\" thehive: image: thehiveproject/thehive:3.5.0-1 depends_on: - elasticsearch - cortex ports: - \"0.0.0.0:9000:9000\" command: --cortex-port 9001 Put this file in an empty folder and run docker-compose up . TheHive is exposed on 9000/tcp port and Cortex on 9001/tcp. These ports can be changed by modifying the docker-compose file. You can specify a custom TheHive configuration file ( application.conf ) by adding the following lines in the thehive section of your docker-compose file: volumes: - /path/to/application.conf:/etc/thehive/application.conf To take effect, be sure that: - '/path/to/application.conf' is readable for the user who runs the docker daemon (typically 644) - you specified command: --no-config in your docker-compose.yml file You should define where the data (i.e. the Elasticsearch database) will be located on your operating system by adding the following lines in the elasticsearch section of your docker-compose file: volumes: - /path/to/data:/usr/share/elasticsearch/data Running ElasticSearch in production mode requires a minimum vm.max_map_count of 262144. ElasticSearch documentation provides instructions on how to query and change this value. If you want to make Cortex be available on TheHive, you must create an account on Cortex, define an API key for it and provide that key to TheHive container using parameter --cortex-key or environment TH_CORTEX_KEY . Manual Installation of Elasticsearch # Elasticsearch can be installed on the same server as TheHive or on a different one. You can then configure TheHive according to the documentation and run TheHive docker as follow: docker run --volume /path/to/thehive/application.conf:/etc/thehive/application.conf thehiveproject/thehive:latest --no-config You can add the --publish docker option to expose TheHive HTTP service. Customize the Docker Image # By default, the TheHive Docker image has minimal configuration: - choose a random secret ( play.http.secret.key ) - search for the Elasticsearch instance (host named elasticsearch ) and add it to configuration - search for a Cortex instance (host named cortex ) and add it to configuration This behavior can be disabled by adding --no-config to the Docker command line: docker run thehiveproject/thehive:latest --no-config Or by adding the line command: --no-config in the thehive section of docker-compose file. It is possible to start database migration at startup with the parameter --auto-migration . If the initial administrator doesn't exist yet, you can request its creation with --create-admin followed by the user login and its password. You can also create a normal user with --create-user followed by the user login and its roles and its password. The image accepts more options. All options are available using environment variables. For boolean variable, 1 means true and other value means false. For multivalued variables, values are separated by coma. This is possible only with --create-admin . Option Env variable Description --no-config TH_NO_CONFIG Do not try to configure TheHive (add the secret and Elasticsearch) --no-config-secret TH_NO_CONFIG_SECRET Do not add the random secret to the configuration --secret <secret> TH_SECRET Cryptographic secret needed to secure sessions --show-secret TH_SHOW_SECRET Show the generated secret --no-config-es TH_NO_CONFIG_ES Do not add the Elasticsearch hosts to configuration --es-uri <uri> TH_CONFIG_ES Use this string to configure elasticsearch hosts (format: http(s)://host:port,host:port(/prefix)?querystring) --es-hostname <host> TH_ES_HOSTNAME Resolve this hostname to find Elasticsearch instances --no-config-cortex TH_NO_CONFIG_CORTEX Do not add Cortex configuration --cortex-proto <proto> TH_CORTEX_PROTO Define the protocol to connect to Cortex (default: http ) --cortex-port <port> TH_CORTEX_PORT Define the port to connect to Cortex (default: 9001 ) --cortex-url <url> TH_CORTEX_URL Add the Cortex connection --cortex-hostname <host> TH_CORTEX_HOSTNAME Resolve this hostname to find the Cortex instance --cortex-key <key> TH_CORTEX_KEY Define Cortex key --auto-migration TH_AUTO_MIGRATION Migrate the database, if needed --create-admin <user> <password TH_CREATE_ADMIN_LOGIN TH_CREATE_ADMIN_PASSWORD Create the first admin user, if not exist yet --create-user <user> <role> <password> TH_CREATE_USER_LOGIN TH_CREATE_USER_ROLE TH_CREATE_USER_PASSWORD Create a user, only in conjunction with admin creation Note : please remember that you must install and configure Elasticsearch . What to Do Next? # Once the Docker image is up and running, proceed to the configuration using the Configuration Guide . For additional configuration options, please refer to the Administration Guide . Pre-release Versions # If you would like to use pre-release, beta versions of our Docker images and help us find bugs to the benefit of the whole community, please use thehiveproject/thehive:version-RCx . For example thehiveproject/thehive:3.1.0-RC1 . Binary # The following section contains the instructions to manually install TheHive using binaries on Ubuntu 20.04 LTS . 1. Minimal Ubuntu Installation # Install a minimal Ubuntu 20.04 system with the following software: Java runtime environment 1.8+ (JRE) Elasticsearch 7.x Make sure your system is up-to-date: sudo apt-get update sudo apt-get upgrade 2. Install a Java Virtual Machine # You can install either Oracle Java or OpenJDK. The latter is recommended. sudo apt-get install openjdk-11-jre-headless 3. Install Elasticsearch # To install Elasticsearch, please read the Elasticsearch Installation section below. 4. Install TheHive # Binary packages can be downloaded from Bintray . The latest version is called thehive-latest.zip . Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive-latest.zip unzip thehive-latest.zip ln -s thehive-x.x.x thehive Note : if you would like to use pre-release, beta versions of and help us find bugs to the benefit of the whole community, please download https://download.thehive-project.org/thehive-version-RCx.zip . For example https://download.thehive-project.org/thehive-3.5.0-RC1-1.zip . 5. First start # It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: sudo addgroup thehive sudo adduser --system thehive sudo cp /opt/thehive/package/thehive.service /usr/lib/systemd/system sudo chown -R thehive:thehive /opt/thehive sudo chgrp thehive /etc/thehive/application.conf sudo chmod 640 /etc/thehive/application.conf sudo systemctl enable thehive sudo service thehive start The only required parameter in order to start TheHive is the key of the server ( play.http.secret.key ). This key is used to authenticate cookies that contain data. If TheHive runs in cluster mode, all instances must share the same key. You can generate the minimal configuration with the following commands (they assume that you have created a dedicated user for TheHive, named thehive ): sudo mkdir /etc/thehive ( cat << _EOF_ # Secret key # ~~~~~ # The secret key is used to secure cryptographics functions. # If you deploy your application to several instances be sure to use the same key! play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\" _EOF_ ) | sudo tee -a /etc/thehive/application.conf Now you can start TheHive. To do so, change your current directory to the TheHive installation directory ( /opt/thehive in this guide), then execute: bin/thehive -Dconfig.file = /etc/thehive/application.conf Please note that the service may take some time to start. Once it is started, you may launch your browser and connect to http://YOUR_SERVER_ADDRESS:9000/ . Please note that the service may take some time to start. The first time you connect you will have to create the database schema. Click \"Migrate database\" to create the DB schema. Once done, you should be redirected to the page for creating the administrator's account. Once created, you should be redirected to the login page. Warning : at this stage, if you missed the creation of the admin account, you will not be able to do it unless you delete TheHive's index from Elasticsearch. In the case you made a mistake, first find out what is the current index of TheHive by running the following command on a host where the Elasticsearch DB used by TheHive is located: $ curl http://127.0.0.1:9200/_cat/indices?v The indexes that TheHive uses always start with the_hive_ following by a number. Let's assume that the output of the command is: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open cortex_1 PC_pLFGBS5G2TNQYr4ajgw 5 1 609 6 2 .1mb 2 .1mb yellow open the_hive_13 ft7GGTfhTr-4lSzZw5r1DQ 5 1 180131 3 51 .3mb 51 .3mb The index used by TheHive is the_hive_13 . To delete it, run the following command: $ curl -X DELETE http://127.0.0.1:9200/the_hive_13 Then reload the page or restart TheHive. 6. Update # To update TheHive from binaries, just stop the service, download the latest package, rebuild the link /opt/thehive and restart the service. service thehive stop cd /opt wget https://download.thehive-project.org/thehive-latest.zip unzip thehive-latest.zip rm /opt/thehive && ln -s thehive-x.x.x thehive chown -R thehive:thehive /opt/thehive /opt/thehive-x.x.x service thehive start 7. Configuration # To configure TheHive, read the Configuration Guide . For additional configuration options, please refer to the Administration Guide . Build it Yourself # The following section contains a step-by-step guide to build TheHive from its sources. 1. Pre-requisites # The following software are required to download and build TheHive: Java Development Kit 11 (JDK) git: use the system package or download it Node.js with its package manager (NPM) Grunt: after installing Node.js, run sudo npm install -g grunt-cli Bower: after installing Node.js, run sudo npm install -g bower Elasticsearch 5.6 2. Build # To install the requirements and build TheHive from sources, please follow the instructions below depending on your operating system. 2.1. CentOS/RHEL # Packages sudo yum -y install git bzip2 Installation of OpenJDK sudo yum -y install java-11-openjdk-devel Installation of Node.js Install the EPEL repository. You should have the extras repository enabled, then: sudo yum -y install epel-release Then, you can install Node.js, Grunt, and Bower: sudo yum -y install nodejs sudo npm install -g grunt-cli bower Installation of Elasticsearch To install Elasticsearch, please read the Elasticsearch Installation section below. 2.2. Ubuntu # Packages sudo apt-get install git wget Installation of Oracle JDK sudo apt install openjdk-11-jdk-headless Installation of Node.js, Grunt and Bower sudo apt-get install curl curl -sL https://deb.nodesource.com/setup_11.x | sudo -E bash - sudo apt-get install -y nodejs sudo npm install -g grunt-cli bower Installation of Elasticsearch To install Elasticsearch, please read the Elasticsearch Installation section below. 2.3. TheHive # Download The Source git clone https://github.com/TheHive-Project/TheHive.git Build the Project cd TheHive ./sbt clean stage This operation may take some time to complete as it will download all dependencies then build the back-end. This command cleans previous build files and creates an autonomous package in the target/universal/stage directory. This packages contains TheHive binaries with required libraries ( /lib ), configuration files ( /conf ) and startup scripts ( /bin ). Binaries are built and stored in TheHive/target/universal/stage/ . You can install them in /opt/thehive for example. sudo cp -r TheHive/target/universal/stage /opt/thehive Configure TheHive, read the Configuration Guide . For additional configuration options, please refer to the Administration Guide . 2.4 Configure and Start Elasticsearch # Edit /etc/elasticsearch/elasticsearch.yml and add the following lines: http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive thread_pool.search.queue_size: 100000 Start the service: service elasticsearch restart 3. First start # Follow the first start section of the binary installation method above to start using TheHive. 4. Build the Front-end Only # Building the back-end builds also the front-end, so you don't need to build it separately. This section is useful only for troubleshooting or for installing the front-end on a reverse proxy. Go to the front-end directory: cd TheHive/ui Install Node.js libraries, which are required by this step, bower libraries (JavaScript libraries downloaded by the browser). Then build the front-end : npm install bower install grunt build This step generates static files (HTML, JavaScript and related resources) in the dist directory. They can be readily imported on a HTTP server. Elasticsearch Installation # If, for some reason, you need to install Elasticsearch, it can be installed using a system package or a Docker image. Version 5.X must be used. From version 6, Elasticsearch drops mapping type . System Package # Install the Elasticsearch package provided by Elastic Debian, Ubuntu # # PGP key installation sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-key D88E42B4 # Alternative PGP key installation # wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - # Debian repository configuration echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list # Install https support for apt sudo apt install apt-transport-https # Elasticsearch installation sudo apt update && sudo apt install elasticsearch The Debian package does not start up the service by default, to prevent the instance from accidentally joining a cluster, without being configured appropriately. CentOS, RedHat, OpenSuSE # # PGP key installation sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch Create the file elasticsearch.repo in /etc/yum.repos.d/ for RedHat and CentOS, or in /etc/zypp/repos.d/ for OpenSuSE distributions, and add the following lines: [elasticsearch-5.x] name=Elasticsearch repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md Then, you can use the following command: # On CentOS and older Red Hat based distributions. sudo yum install elasticsearch # On Fedora and other newer Red Hat distributions. sudo dnf install elasticsearch # On OpenSUSE based distributions. sudo zypper install elasticsearch If you prefer using Elasticsearch inside a docker, see Elasticsearch inside a Docker . Configuration # It is highly recommended to avoid exposing this service to an untrusted zone. If Elasticsearch and TheHive run on the same host (and not in a docker), edit /etc/elasticsearch/elasticsearch.yml and set network.host parameter with 127.0.0.1 . TheHive use dynamic scripts to make partial updates. Hence, they must be activated using script.inline: true . The cluster name must also be set ( hive for example). Threadpool queue size must be set with a high value ( 100000 ). The default size will get the queue easily overloaded. Edit /etc/elasticsearch/elasticsearch.yml and add the following lines: http.host: 127.0.0.1 cluster.name: hive thread_pool.search.queue_size: 100000 Start the Service # Now that Elasticsearch is configured, start it as a service and check whether it's running: sudo systemctl enable elasticsearch.service sudo systemctl start elasticsearch.service sudo systemctl status elasticsearch.service The status should be active (running) . If it's not running, you can check for the reason in the logs: sudo journalctl -u elasticsearch.service Note that by default, the database is stored in /var/lib/elasticsearch and the logs in /var/log/elasticsearch Elasticsearch inside a Docker # You can also start Elasticsearch inside a docker. Use the following command and do not forget to specify the absolute path for persistent data on your host : docker run \\ --name elasticsearch \\ --hostname elasticsearch \\ --rm \\ --publish 127.0.0.1:9200:9200 \\ --volume ***DATA_DIR***:/usr/share/elasticsearch/data \\ -e \"http.host=0.0.0.0\" \\ -e \"discovery.type=single-node\" \\ -e \"xpack.security.enabled=false\" \\ -e \"cluster.name=hive\" \\ -e \"script.inline=true\" \\ -e \"thread_pool.index.queue_size=100000\" \\ -e \"thread_pool.search.queue_size=100000\" \\ -e \"thread_pool.bulk.queue_size=100000\" \\ docker.elastic.co/elasticsearch/elasticsearch:7.9.1","title":"Installation Guide"},{"location":"thehive/legacy/thehive3/installation/install-guide/#installation-guide","text":"\u26a0\ufe0f Please read carrefully this documentation. Depending on you make a fresh installation or update an existing version, install version 3 or version 4, repository or packages names may vary. Current supported versions of TheHive are: - Version 3.5.0 and later that supports only Elasticsearch 7.x. - Version 4.0 and later. Instruction to install TheHive supporting Elasticsearch 6.x (EoL in Nov. 2020) are still detailled in this documentation. Before installing TheHive, you need to choose the installation option which suits your environment as described below. Once you have a chosen an option and installed the software, read the Configuration Guide . We also advise reading the Administration Guide .","title":"Installation Guide"},{"location":"thehive/legacy/thehive3/installation/install-guide/#table-of-contents","text":"Installation Options RPM DEB Docker Binary Build it Yourself Elasticsearch Installation System Package Start the Service Elasticsearch inside a Docker","title":"Table of Contents"},{"location":"thehive/legacy/thehive3/installation/install-guide/#installation-options","text":"TheHive is available as: an RPM package a DEB package a Docker image a binary package In addition, TheHive can be also be built from the source code .","title":"Installation Options"},{"location":"thehive/legacy/thehive3/installation/install-guide/#rpm","text":"RPM packages are published on a our RPM repository. All packages are signed using our GPG key 562CBC1C . Its fingerprint is: 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C Run the following command to import the GPG key : sudo rpm --import https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY","title":"RPM"},{"location":"thehive/legacy/thehive3/installation/install-guide/#release-versions","text":"The release repository contains packages for TheHive 3.5.0+ and TheHive 4.0.0+ Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/release/noarch gpgcheck = 1 Then you will able to install TheHive 3.5.0+ the package using yum : yum install thehive or install TheHive 4.0.0+ : yum install thehive4","title":"Release versions"},{"location":"thehive/legacy/thehive3/installation/install-guide/#stable-versions-or-legacy-versions","text":"The Stable repository is a legacy repository and contains packages for TheHive 3.4.4 that does not support Elasticsearch version 7.x**, but version 6.x. Setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/stable/noarch gpgcheck = 1 Then you will able to install TheHive 3.4.4 package using yum : yum install thehive","title":"Stable versions (or legacy versions)"},{"location":"thehive/legacy/thehive3/installation/install-guide/#following-beta-versions","text":"To follow beta versions of TheHive, use the following setup: And setup your system to connect the RPM repository. Create and edit the file /etc/yum.repos.d/thehive-project.repo : [ thehive-project ] enabled = 1 priority = 1 name = TheHive-Project RPM repository baseurl = https://rpm.thehive-project.org/beta/noarch gpgcheck = 1 Then you will able to install beta version of TheHive 3.x package using yum : yum install thehive or install beta version of TheHive 4.x : yum install thehive4 \u26a0\ufe0f We do not recommend that configuration for production servers Once the package is installed, proceed to the configuration using the Configuration Guide . For additional configuration options, please refer to the Administration Guide .","title":"Following beta versions"},{"location":"thehive/legacy/thehive3/installation/install-guide/#deb","text":"Debian packages are published on a our DEB packages repository. All packages are signed using our GPG key 562CBC1C . Its fingerprint is: 0CD5 AC59 DE5C 5A8E 0EE1 3849 3D99 BB18 562C BC1C","title":"DEB"},{"location":"thehive/legacy/thehive3/installation/install-guide/#release-versions_1","text":"The release repository contains packages for TheHive 3.5.0+ and TheHive 4.0.0+ Setup apt configuration with the release repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org release main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then you will able to install TheHive 3.5.0+ the package using apt command: apt install thehive or install TheHive 4.0.0+ : apt install thehive4","title":"Release versions"},{"location":"thehive/legacy/thehive3/installation/install-guide/#stable-versions-or-legacy-versions_1","text":"The main repository is a legacy repository and contains packages for TheHive 3.4.4 that does not support Elasticsearch version 7.x , but version 6.x. Setup apt configuration with the main repository: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org stable main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update Then you will able to install TheHive 3.4.4 package using apt command: apt install thehive","title":"Stable versions (or legacy versions)"},{"location":"thehive/legacy/thehive3/installation/install-guide/#beta-versions","text":"To follow beta versions of TheHive, use the following commands: curl https://raw.githubusercontent.com/TheHive-Project/TheHive/master/PGP-PUBLIC-KEY | sudo apt-key add - echo 'deb https://deb.thehive-project.org beta main' | sudo tee -a /etc/apt/sources.list.d/thehive-project.list sudo apt-get update sudo apt-get install thehive \u26a0\ufe0f We do not recommend that configuration for production servers","title":"Beta versions"},{"location":"thehive/legacy/thehive3/installation/install-guide/#docker","text":"To use the Docker image, you must use Docker (courtesy of Captain Obvious). TheHive requires Elasticsearch to run. You can use docker-compose to start them together in Docker or install and configure Elasticsearch manually.","title":"Docker"},{"location":"thehive/legacy/thehive3/installation/install-guide/#use-docker-compose","text":"Docker-compose can start multiple dockers and link them together. The following docker-compose.yml file starts Elasticsearch and TheHive: version: \"2\" services: elasticsearch: image: elasticsearch:7.9.1 environment: - http.host=0.0.0.0 - discovery.type=single-node ulimits: nofile: soft: 65536 hard: 65536 cortex: image: thehiveproject/cortex:3.1.0-1 depends_on: - elasticsearch ports: - \"0.0.0.0:9001:9001\" thehive: image: thehiveproject/thehive:3.5.0-1 depends_on: - elasticsearch - cortex ports: - \"0.0.0.0:9000:9000\" command: --cortex-port 9001 Put this file in an empty folder and run docker-compose up . TheHive is exposed on 9000/tcp port and Cortex on 9001/tcp. These ports can be changed by modifying the docker-compose file. You can specify a custom TheHive configuration file ( application.conf ) by adding the following lines in the thehive section of your docker-compose file: volumes: - /path/to/application.conf:/etc/thehive/application.conf To take effect, be sure that: - '/path/to/application.conf' is readable for the user who runs the docker daemon (typically 644) - you specified command: --no-config in your docker-compose.yml file You should define where the data (i.e. the Elasticsearch database) will be located on your operating system by adding the following lines in the elasticsearch section of your docker-compose file: volumes: - /path/to/data:/usr/share/elasticsearch/data Running ElasticSearch in production mode requires a minimum vm.max_map_count of 262144. ElasticSearch documentation provides instructions on how to query and change this value. If you want to make Cortex be available on TheHive, you must create an account on Cortex, define an API key for it and provide that key to TheHive container using parameter --cortex-key or environment TH_CORTEX_KEY .","title":"Use Docker-compose"},{"location":"thehive/legacy/thehive3/installation/install-guide/#manual-installation-of-elasticsearch","text":"Elasticsearch can be installed on the same server as TheHive or on a different one. You can then configure TheHive according to the documentation and run TheHive docker as follow: docker run --volume /path/to/thehive/application.conf:/etc/thehive/application.conf thehiveproject/thehive:latest --no-config You can add the --publish docker option to expose TheHive HTTP service.","title":"Manual Installation of Elasticsearch"},{"location":"thehive/legacy/thehive3/installation/install-guide/#customize-the-docker-image","text":"By default, the TheHive Docker image has minimal configuration: - choose a random secret ( play.http.secret.key ) - search for the Elasticsearch instance (host named elasticsearch ) and add it to configuration - search for a Cortex instance (host named cortex ) and add it to configuration This behavior can be disabled by adding --no-config to the Docker command line: docker run thehiveproject/thehive:latest --no-config Or by adding the line command: --no-config in the thehive section of docker-compose file. It is possible to start database migration at startup with the parameter --auto-migration . If the initial administrator doesn't exist yet, you can request its creation with --create-admin followed by the user login and its password. You can also create a normal user with --create-user followed by the user login and its roles and its password. The image accepts more options. All options are available using environment variables. For boolean variable, 1 means true and other value means false. For multivalued variables, values are separated by coma. This is possible only with --create-admin . Option Env variable Description --no-config TH_NO_CONFIG Do not try to configure TheHive (add the secret and Elasticsearch) --no-config-secret TH_NO_CONFIG_SECRET Do not add the random secret to the configuration --secret <secret> TH_SECRET Cryptographic secret needed to secure sessions --show-secret TH_SHOW_SECRET Show the generated secret --no-config-es TH_NO_CONFIG_ES Do not add the Elasticsearch hosts to configuration --es-uri <uri> TH_CONFIG_ES Use this string to configure elasticsearch hosts (format: http(s)://host:port,host:port(/prefix)?querystring) --es-hostname <host> TH_ES_HOSTNAME Resolve this hostname to find Elasticsearch instances --no-config-cortex TH_NO_CONFIG_CORTEX Do not add Cortex configuration --cortex-proto <proto> TH_CORTEX_PROTO Define the protocol to connect to Cortex (default: http ) --cortex-port <port> TH_CORTEX_PORT Define the port to connect to Cortex (default: 9001 ) --cortex-url <url> TH_CORTEX_URL Add the Cortex connection --cortex-hostname <host> TH_CORTEX_HOSTNAME Resolve this hostname to find the Cortex instance --cortex-key <key> TH_CORTEX_KEY Define Cortex key --auto-migration TH_AUTO_MIGRATION Migrate the database, if needed --create-admin <user> <password TH_CREATE_ADMIN_LOGIN TH_CREATE_ADMIN_PASSWORD Create the first admin user, if not exist yet --create-user <user> <role> <password> TH_CREATE_USER_LOGIN TH_CREATE_USER_ROLE TH_CREATE_USER_PASSWORD Create a user, only in conjunction with admin creation Note : please remember that you must install and configure Elasticsearch .","title":"Customize the Docker Image"},{"location":"thehive/legacy/thehive3/installation/install-guide/#what-to-do-next","text":"Once the Docker image is up and running, proceed to the configuration using the Configuration Guide . For additional configuration options, please refer to the Administration Guide .","title":"What to Do Next?"},{"location":"thehive/legacy/thehive3/installation/install-guide/#pre-release-versions","text":"If you would like to use pre-release, beta versions of our Docker images and help us find bugs to the benefit of the whole community, please use thehiveproject/thehive:version-RCx . For example thehiveproject/thehive:3.1.0-RC1 .","title":"Pre-release Versions"},{"location":"thehive/legacy/thehive3/installation/install-guide/#binary","text":"The following section contains the instructions to manually install TheHive using binaries on Ubuntu 20.04 LTS .","title":"Binary"},{"location":"thehive/legacy/thehive3/installation/install-guide/#1-minimal-ubuntu-installation","text":"Install a minimal Ubuntu 20.04 system with the following software: Java runtime environment 1.8+ (JRE) Elasticsearch 7.x Make sure your system is up-to-date: sudo apt-get update sudo apt-get upgrade","title":"1. Minimal Ubuntu Installation"},{"location":"thehive/legacy/thehive3/installation/install-guide/#2-install-a-java-virtual-machine","text":"You can install either Oracle Java or OpenJDK. The latter is recommended. sudo apt-get install openjdk-11-jre-headless","title":"2. Install a Java Virtual Machine"},{"location":"thehive/legacy/thehive3/installation/install-guide/#3-install-elasticsearch","text":"To install Elasticsearch, please read the Elasticsearch Installation section below.","title":"3. Install Elasticsearch"},{"location":"thehive/legacy/thehive3/installation/install-guide/#4-install-thehive","text":"Binary packages can be downloaded from Bintray . The latest version is called thehive-latest.zip . Download and unzip the chosen binary package. TheHive files can be installed wherever you want on the filesystem. In this guide, we assume you have chosen to install them under /opt . cd /opt wget https://download.thehive-project.org/thehive-latest.zip unzip thehive-latest.zip ln -s thehive-x.x.x thehive Note : if you would like to use pre-release, beta versions of and help us find bugs to the benefit of the whole community, please download https://download.thehive-project.org/thehive-version-RCx.zip . For example https://download.thehive-project.org/thehive-3.5.0-RC1-1.zip .","title":"4. Install TheHive"},{"location":"thehive/legacy/thehive3/installation/install-guide/#5-first-start","text":"It is recommended to use a dedicated, non-privileged user account to start TheHive. If so, make sure that the chosen account can create log files in /opt/thehive/logs . If you'd rather start the application as a service, use the following commands: sudo addgroup thehive sudo adduser --system thehive sudo cp /opt/thehive/package/thehive.service /usr/lib/systemd/system sudo chown -R thehive:thehive /opt/thehive sudo chgrp thehive /etc/thehive/application.conf sudo chmod 640 /etc/thehive/application.conf sudo systemctl enable thehive sudo service thehive start The only required parameter in order to start TheHive is the key of the server ( play.http.secret.key ). This key is used to authenticate cookies that contain data. If TheHive runs in cluster mode, all instances must share the same key. You can generate the minimal configuration with the following commands (they assume that you have created a dedicated user for TheHive, named thehive ): sudo mkdir /etc/thehive ( cat << _EOF_ # Secret key # ~~~~~ # The secret key is used to secure cryptographics functions. # If you deploy your application to several instances be sure to use the same key! play.http.secret.key=\"$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 64 | head -n 1)\" _EOF_ ) | sudo tee -a /etc/thehive/application.conf Now you can start TheHive. To do so, change your current directory to the TheHive installation directory ( /opt/thehive in this guide), then execute: bin/thehive -Dconfig.file = /etc/thehive/application.conf Please note that the service may take some time to start. Once it is started, you may launch your browser and connect to http://YOUR_SERVER_ADDRESS:9000/ . Please note that the service may take some time to start. The first time you connect you will have to create the database schema. Click \"Migrate database\" to create the DB schema. Once done, you should be redirected to the page for creating the administrator's account. Once created, you should be redirected to the login page. Warning : at this stage, if you missed the creation of the admin account, you will not be able to do it unless you delete TheHive's index from Elasticsearch. In the case you made a mistake, first find out what is the current index of TheHive by running the following command on a host where the Elasticsearch DB used by TheHive is located: $ curl http://127.0.0.1:9200/_cat/indices?v The indexes that TheHive uses always start with the_hive_ following by a number. Let's assume that the output of the command is: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open cortex_1 PC_pLFGBS5G2TNQYr4ajgw 5 1 609 6 2 .1mb 2 .1mb yellow open the_hive_13 ft7GGTfhTr-4lSzZw5r1DQ 5 1 180131 3 51 .3mb 51 .3mb The index used by TheHive is the_hive_13 . To delete it, run the following command: $ curl -X DELETE http://127.0.0.1:9200/the_hive_13 Then reload the page or restart TheHive.","title":"5. First start"},{"location":"thehive/legacy/thehive3/installation/install-guide/#6-update","text":"To update TheHive from binaries, just stop the service, download the latest package, rebuild the link /opt/thehive and restart the service. service thehive stop cd /opt wget https://download.thehive-project.org/thehive-latest.zip unzip thehive-latest.zip rm /opt/thehive && ln -s thehive-x.x.x thehive chown -R thehive:thehive /opt/thehive /opt/thehive-x.x.x service thehive start","title":"6. Update"},{"location":"thehive/legacy/thehive3/installation/install-guide/#7-configuration","text":"To configure TheHive, read the Configuration Guide . For additional configuration options, please refer to the Administration Guide .","title":"7. Configuration"},{"location":"thehive/legacy/thehive3/installation/install-guide/#build-it-yourself","text":"The following section contains a step-by-step guide to build TheHive from its sources.","title":"Build it Yourself"},{"location":"thehive/legacy/thehive3/installation/install-guide/#1-pre-requisites","text":"The following software are required to download and build TheHive: Java Development Kit 11 (JDK) git: use the system package or download it Node.js with its package manager (NPM) Grunt: after installing Node.js, run sudo npm install -g grunt-cli Bower: after installing Node.js, run sudo npm install -g bower Elasticsearch 5.6","title":"1. Pre-requisites"},{"location":"thehive/legacy/thehive3/installation/install-guide/#2-build","text":"To install the requirements and build TheHive from sources, please follow the instructions below depending on your operating system.","title":"2. Build"},{"location":"thehive/legacy/thehive3/installation/install-guide/#21-centosrhel","text":"Packages sudo yum -y install git bzip2 Installation of OpenJDK sudo yum -y install java-11-openjdk-devel Installation of Node.js Install the EPEL repository. You should have the extras repository enabled, then: sudo yum -y install epel-release Then, you can install Node.js, Grunt, and Bower: sudo yum -y install nodejs sudo npm install -g grunt-cli bower Installation of Elasticsearch To install Elasticsearch, please read the Elasticsearch Installation section below.","title":"2.1. CentOS/RHEL"},{"location":"thehive/legacy/thehive3/installation/install-guide/#22-ubuntu","text":"Packages sudo apt-get install git wget Installation of Oracle JDK sudo apt install openjdk-11-jdk-headless Installation of Node.js, Grunt and Bower sudo apt-get install curl curl -sL https://deb.nodesource.com/setup_11.x | sudo -E bash - sudo apt-get install -y nodejs sudo npm install -g grunt-cli bower Installation of Elasticsearch To install Elasticsearch, please read the Elasticsearch Installation section below.","title":"2.2. Ubuntu"},{"location":"thehive/legacy/thehive3/installation/install-guide/#23-thehive","text":"Download The Source git clone https://github.com/TheHive-Project/TheHive.git Build the Project cd TheHive ./sbt clean stage This operation may take some time to complete as it will download all dependencies then build the back-end. This command cleans previous build files and creates an autonomous package in the target/universal/stage directory. This packages contains TheHive binaries with required libraries ( /lib ), configuration files ( /conf ) and startup scripts ( /bin ). Binaries are built and stored in TheHive/target/universal/stage/ . You can install them in /opt/thehive for example. sudo cp -r TheHive/target/universal/stage /opt/thehive Configure TheHive, read the Configuration Guide . For additional configuration options, please refer to the Administration Guide .","title":"2.3. TheHive"},{"location":"thehive/legacy/thehive3/installation/install-guide/#24-configure-and-start-elasticsearch","text":"Edit /etc/elasticsearch/elasticsearch.yml and add the following lines: http.host: 127.0.0.1 discovery.type: single-node cluster.name: hive thread_pool.search.queue_size: 100000 Start the service: service elasticsearch restart","title":"2.4 Configure and Start Elasticsearch"},{"location":"thehive/legacy/thehive3/installation/install-guide/#3-first-start","text":"Follow the first start section of the binary installation method above to start using TheHive.","title":"3. First start"},{"location":"thehive/legacy/thehive3/installation/install-guide/#4-build-the-front-end-only","text":"Building the back-end builds also the front-end, so you don't need to build it separately. This section is useful only for troubleshooting or for installing the front-end on a reverse proxy. Go to the front-end directory: cd TheHive/ui Install Node.js libraries, which are required by this step, bower libraries (JavaScript libraries downloaded by the browser). Then build the front-end : npm install bower install grunt build This step generates static files (HTML, JavaScript and related resources) in the dist directory. They can be readily imported on a HTTP server.","title":"4. Build the Front-end Only"},{"location":"thehive/legacy/thehive3/installation/install-guide/#elasticsearch-installation","text":"If, for some reason, you need to install Elasticsearch, it can be installed using a system package or a Docker image. Version 5.X must be used. From version 6, Elasticsearch drops mapping type .","title":"Elasticsearch Installation"},{"location":"thehive/legacy/thehive3/installation/install-guide/#system-package","text":"Install the Elasticsearch package provided by Elastic","title":"System Package"},{"location":"thehive/legacy/thehive3/installation/install-guide/#debian-ubuntu","text":"# PGP key installation sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-key D88E42B4 # Alternative PGP key installation # wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add - # Debian repository configuration echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list # Install https support for apt sudo apt install apt-transport-https # Elasticsearch installation sudo apt update && sudo apt install elasticsearch The Debian package does not start up the service by default, to prevent the instance from accidentally joining a cluster, without being configured appropriately.","title":"Debian, Ubuntu"},{"location":"thehive/legacy/thehive3/installation/install-guide/#centos-redhat-opensuse","text":"# PGP key installation sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch Create the file elasticsearch.repo in /etc/yum.repos.d/ for RedHat and CentOS, or in /etc/zypp/repos.d/ for OpenSuSE distributions, and add the following lines: [elasticsearch-5.x] name=Elasticsearch repository for 7.x packages baseurl=https://artifacts.elastic.co/packages/7.x/yum gpgcheck=1 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch enabled=1 autorefresh=1 type=rpm-md Then, you can use the following command: # On CentOS and older Red Hat based distributions. sudo yum install elasticsearch # On Fedora and other newer Red Hat distributions. sudo dnf install elasticsearch # On OpenSUSE based distributions. sudo zypper install elasticsearch If you prefer using Elasticsearch inside a docker, see Elasticsearch inside a Docker .","title":"CentOS, RedHat, OpenSuSE"},{"location":"thehive/legacy/thehive3/installation/install-guide/#configuration","text":"It is highly recommended to avoid exposing this service to an untrusted zone. If Elasticsearch and TheHive run on the same host (and not in a docker), edit /etc/elasticsearch/elasticsearch.yml and set network.host parameter with 127.0.0.1 . TheHive use dynamic scripts to make partial updates. Hence, they must be activated using script.inline: true . The cluster name must also be set ( hive for example). Threadpool queue size must be set with a high value ( 100000 ). The default size will get the queue easily overloaded. Edit /etc/elasticsearch/elasticsearch.yml and add the following lines: http.host: 127.0.0.1 cluster.name: hive thread_pool.search.queue_size: 100000","title":"Configuration"},{"location":"thehive/legacy/thehive3/installation/install-guide/#start-the-service","text":"Now that Elasticsearch is configured, start it as a service and check whether it's running: sudo systemctl enable elasticsearch.service sudo systemctl start elasticsearch.service sudo systemctl status elasticsearch.service The status should be active (running) . If it's not running, you can check for the reason in the logs: sudo journalctl -u elasticsearch.service Note that by default, the database is stored in /var/lib/elasticsearch and the logs in /var/log/elasticsearch","title":"Start the Service"},{"location":"thehive/legacy/thehive3/installation/install-guide/#elasticsearch-inside-a-docker","text":"You can also start Elasticsearch inside a docker. Use the following command and do not forget to specify the absolute path for persistent data on your host : docker run \\ --name elasticsearch \\ --hostname elasticsearch \\ --rm \\ --publish 127.0.0.1:9200:9200 \\ --volume ***DATA_DIR***:/usr/share/elasticsearch/data \\ -e \"http.host=0.0.0.0\" \\ -e \"discovery.type=single-node\" \\ -e \"xpack.security.enabled=false\" \\ -e \"cluster.name=hive\" \\ -e \"script.inline=true\" \\ -e \"thread_pool.index.queue_size=100000\" \\ -e \"thread_pool.search.queue_size=100000\" \\ -e \"thread_pool.bulk.queue_size=100000\" \\ docker.elastic.co/elasticsearch/elasticsearch:7.9.1","title":"Elasticsearch inside a Docker"},{"location":"thehive/operations/backup-restore/","text":"Backup and restore # This guide has only been tested on single node Cassandra server Note https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsBackupRestore.html Overview # To be restored successfully, TheHive requires following data beeing saved: The database Files optionnally, the index. Cassandra # Pre requisites # To backup or export database from Cassandra, following information are required: Cassandra admin password keyspace used by thehive (default = thehive ). This can be checked in the application.conf configuration file, in the database configuration in storage , cql and keyspace attribute. Tip This information can be found in TheHive configuration: [..] db.janusgraph { storage { backend: cql hostname: [\"127.0.0.1\"] cql { cluster-name: thp keyspace: thehive } } [..] Backup # Following actions should be performed to backup the data successfully: Save the database schema Create a snapshot Save the data and the schema Save the database schema # This can be done with the following command: cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <SERVER_IP> -e \"DESCRIBE KEYSPACE <KEYSPACE>\" > schema.cql Create a snapshot and an archive # Considering that your keyspace is thehive and backup_name is the name of the snapshot, run the following commands: Before taking snapshots nodetool cleanup thehive Take a snapshot nodetool snapshot thehive -t backup_name Create and archive with the snapshot data: tar cjf backup.tbz /var/lib/cassandra/data/thehive/*/snapshots/backup_name/ Remove old snapshots (if necessary) nodetool -h localhost -p 7199 clearsnapshot -t <snapshotname> Example # Example of script to generate backups of TheHive keyspace #!/bin/bash ## Create a CQL file with the schema of the KEYSPACE ## and an tbz archive containing the snapshot ## Complete variables before running: ## KEYSPACE: Identify the right keyspace to save in cassandra ## SNAPSHOT: choose a name for the backup IP = 10 .1.1.1 SOURCE_KEYSPACE = thehive SNAPSHOT = thehive_20211124 SNAPSHOT_INDEX = 1 # Backup Cassandra nodetool cleanup ${ SOURCE_KEYSPACE } nodetool snapshot ${ SOURCE_KEYSPACE } -t ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } echo -n \"Cassandra admin password\" : read -s CASSANDRA_PASSWORD ## Save schema cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"DESCRIBE KEYSPACE ${ SOURCE_KEYSPACE } \" > schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Create archive if [[ $? == 0 ]] then tar cjf ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .tbz /var/lib/cassandra/data/ ${ SOURCE_KEYSPACE } /*/snapshots/ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } / fi Restore data # Pre requisites # Following data is required to restore TheHive database successfully: The database schema (example: schema.cql ) A backup of the database (example: backup.tbz ) Keyspace to restore does not exist in the database (or it will be overwritten) Restore # Restore keyspace cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <IP> -e \"source 'schema.cql';\" Unarchive backup files: tar jxf /PATH/TO/backup.tbz -C /tmp/cassandra_backup And restore snapshots files: cd /var/lib/cassandra/data/thehive for I in ` ls /tmp/cassandra/var/lib/cassandra/data/<KEYSPACE> ` ; do cp /tmp/cassandra/var/lib/cassandra/data/<KEYSPACE>/ $I /snapshots/<BACKUP_NAME>/* /var/lib/cassandra/data/<KEYSPACE>/ $I / ; done Ensure Cassandra user keep ownership on the files: chown -R cassandra:cassandra /var/lib/cassandra/data/<KEYSPACE> Refresh tables for TABLE in ` ls /var/lib/cassandra/data/<KEYSPACE> ` do sstableloader -d <IP> /var/lib/cassandra/data/<KEYSPACE>/<TABLE> done Ensure no Commitlog file exist before restarting Cassandra service. ( /var/lib/cassandra/commitlog ) Example of script to restore TheHive keyspace in Cassandra #!/bin/bash ## Restore a KEYSPACE and its data from a CQL file with the schema of the ## KEYSPACE and an tbz archive containing the snapshot ## Complete variables before running: ## IP: IP of cassandra server ## TMP: choose a TMP folder !!! this folder will be removed if exists. ## SOURCE_KEYSPACE: KEYSPACE used in the backup ## TARGET_KEYSPACE: new KEYSPACE name ; use same name of SOURCE_KEYSPACE if no changes ## SNAPSHOT: choose a name for the backup ## SNAPSHOT_INDEX: index of the snapshot (1, 20210401 ...) IP = 10 .1.1.1 TMP = /tmp/cassandra_backup SOURCE_KEYSPACE = \"thehive\" TARGET_KEYSPACE = \"\" SNAPSHOT = \"thehive_20211124\" SNAPSHOT_INDEX = \"1\" ## Uncompress data in TMP folder rm -rf ${ TMP } && mkdir ${ TMP } tar jxf ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .tbz -C ${ TMP } ## Read Cassandra password echo -n \"Cassandra admin password: \" read -s CASSANDRA_PASSWORD ## Define new KEYSPACE NAME sed -i \"s/ ${ SOURCE_KEYSPACE } / ${ TARGET_KEYSPACE } /g\" schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Restore keyspace cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } cassandra --file schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Restore data for TABLE in ` cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"use ${ TARGET_KEYSPACE } ; DESC tables ;\" ` do cp -r ${ TMP } /var/lib/cassandra/data/ ${ SOURCE_KEYSPACE } / ${ TABLE } -*/snapshots/ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } /* /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } / $TABLE -*/ done ## Change ownership chown -R cassandra:cassandra /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } ## sstableloader for TABLE in ` ls /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } ` do sstableloader -d ${ IP } /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } / ${ TABLE } done Files # Backup # Wether you use local or distributed files system storage, copy the content of the folder/bucket. Restore # Restore the saved files into the destination folder/bucket that will be used by TheHive.","title":"Backup & restore"},{"location":"thehive/operations/backup-restore/#backup-and-restore","text":"This guide has only been tested on single node Cassandra server Note https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsBackupRestore.html","title":"Backup and restore"},{"location":"thehive/operations/backup-restore/#overview","text":"To be restored successfully, TheHive requires following data beeing saved: The database Files optionnally, the index.","title":"Overview"},{"location":"thehive/operations/backup-restore/#cassandra","text":"","title":"Cassandra"},{"location":"thehive/operations/backup-restore/#pre-requisites","text":"To backup or export database from Cassandra, following information are required: Cassandra admin password keyspace used by thehive (default = thehive ). This can be checked in the application.conf configuration file, in the database configuration in storage , cql and keyspace attribute. Tip This information can be found in TheHive configuration: [..] db.janusgraph { storage { backend: cql hostname: [\"127.0.0.1\"] cql { cluster-name: thp keyspace: thehive } } [..]","title":"Pre requisites"},{"location":"thehive/operations/backup-restore/#backup","text":"Following actions should be performed to backup the data successfully: Save the database schema Create a snapshot Save the data and the schema","title":"Backup"},{"location":"thehive/operations/backup-restore/#save-the-database-schema","text":"This can be done with the following command: cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <SERVER_IP> -e \"DESCRIBE KEYSPACE <KEYSPACE>\" > schema.cql","title":"Save the database schema"},{"location":"thehive/operations/backup-restore/#create-a-snapshot-and-an-archive","text":"Considering that your keyspace is thehive and backup_name is the name of the snapshot, run the following commands: Before taking snapshots nodetool cleanup thehive Take a snapshot nodetool snapshot thehive -t backup_name Create and archive with the snapshot data: tar cjf backup.tbz /var/lib/cassandra/data/thehive/*/snapshots/backup_name/ Remove old snapshots (if necessary) nodetool -h localhost -p 7199 clearsnapshot -t <snapshotname>","title":"Create a snapshot and an archive"},{"location":"thehive/operations/backup-restore/#example","text":"Example of script to generate backups of TheHive keyspace #!/bin/bash ## Create a CQL file with the schema of the KEYSPACE ## and an tbz archive containing the snapshot ## Complete variables before running: ## KEYSPACE: Identify the right keyspace to save in cassandra ## SNAPSHOT: choose a name for the backup IP = 10 .1.1.1 SOURCE_KEYSPACE = thehive SNAPSHOT = thehive_20211124 SNAPSHOT_INDEX = 1 # Backup Cassandra nodetool cleanup ${ SOURCE_KEYSPACE } nodetool snapshot ${ SOURCE_KEYSPACE } -t ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } echo -n \"Cassandra admin password\" : read -s CASSANDRA_PASSWORD ## Save schema cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"DESCRIBE KEYSPACE ${ SOURCE_KEYSPACE } \" > schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Create archive if [[ $? == 0 ]] then tar cjf ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .tbz /var/lib/cassandra/data/ ${ SOURCE_KEYSPACE } /*/snapshots/ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } / fi","title":"Example"},{"location":"thehive/operations/backup-restore/#restore-data","text":"","title":"Restore data"},{"location":"thehive/operations/backup-restore/#pre-requisites_1","text":"Following data is required to restore TheHive database successfully: The database schema (example: schema.cql ) A backup of the database (example: backup.tbz ) Keyspace to restore does not exist in the database (or it will be overwritten)","title":"Pre requisites"},{"location":"thehive/operations/backup-restore/#restore","text":"Restore keyspace cqlsh -u cassandra -p <CASSANDRA_PASSWORD> <IP> -e \"source 'schema.cql';\" Unarchive backup files: tar jxf /PATH/TO/backup.tbz -C /tmp/cassandra_backup And restore snapshots files: cd /var/lib/cassandra/data/thehive for I in ` ls /tmp/cassandra/var/lib/cassandra/data/<KEYSPACE> ` ; do cp /tmp/cassandra/var/lib/cassandra/data/<KEYSPACE>/ $I /snapshots/<BACKUP_NAME>/* /var/lib/cassandra/data/<KEYSPACE>/ $I / ; done Ensure Cassandra user keep ownership on the files: chown -R cassandra:cassandra /var/lib/cassandra/data/<KEYSPACE> Refresh tables for TABLE in ` ls /var/lib/cassandra/data/<KEYSPACE> ` do sstableloader -d <IP> /var/lib/cassandra/data/<KEYSPACE>/<TABLE> done Ensure no Commitlog file exist before restarting Cassandra service. ( /var/lib/cassandra/commitlog ) Example of script to restore TheHive keyspace in Cassandra #!/bin/bash ## Restore a KEYSPACE and its data from a CQL file with the schema of the ## KEYSPACE and an tbz archive containing the snapshot ## Complete variables before running: ## IP: IP of cassandra server ## TMP: choose a TMP folder !!! this folder will be removed if exists. ## SOURCE_KEYSPACE: KEYSPACE used in the backup ## TARGET_KEYSPACE: new KEYSPACE name ; use same name of SOURCE_KEYSPACE if no changes ## SNAPSHOT: choose a name for the backup ## SNAPSHOT_INDEX: index of the snapshot (1, 20210401 ...) IP = 10 .1.1.1 TMP = /tmp/cassandra_backup SOURCE_KEYSPACE = \"thehive\" TARGET_KEYSPACE = \"\" SNAPSHOT = \"thehive_20211124\" SNAPSHOT_INDEX = \"1\" ## Uncompress data in TMP folder rm -rf ${ TMP } && mkdir ${ TMP } tar jxf ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .tbz -C ${ TMP } ## Read Cassandra password echo -n \"Cassandra admin password: \" read -s CASSANDRA_PASSWORD ## Define new KEYSPACE NAME sed -i \"s/ ${ SOURCE_KEYSPACE } / ${ TARGET_KEYSPACE } /g\" schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Restore keyspace cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } cassandra --file schema_ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } .cql ## Restore data for TABLE in ` cqlsh -u cassandra -p ${ CASSANDRA_PASSWORD } ${ IP } -e \"use ${ TARGET_KEYSPACE } ; DESC tables ;\" ` do cp -r ${ TMP } /var/lib/cassandra/data/ ${ SOURCE_KEYSPACE } / ${ TABLE } -*/snapshots/ ${ SNAPSHOT } _ ${ SNAPSHOT_INDEX } /* /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } / $TABLE -*/ done ## Change ownership chown -R cassandra:cassandra /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } ## sstableloader for TABLE in ` ls /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } ` do sstableloader -d ${ IP } /var/lib/cassandra/data/ ${ TARGET_KEYSPACE } / ${ TABLE } done","title":"Restore"},{"location":"thehive/operations/backup-restore/#files","text":"","title":"Files"},{"location":"thehive/operations/backup-restore/#backup_1","text":"Wether you use local or distributed files system storage, copy the content of the folder/bucket.","title":"Backup"},{"location":"thehive/operations/backup-restore/#restore_1","text":"Restore the saved files into the destination folder/bucket that will be used by TheHive.","title":"Restore"},{"location":"thehive/operations/cassandra-security/","text":"Security in Apache Cassandra # References Internal authentication https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureInternalAuthenticationTOC.html Node to node encryption https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLNodeToNode.html Client to node encryption https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLClientToNode.html https://docs.janusgraph.org/basics/configuration-reference/#storagecqlssl Authentication with Cassandra # Create an account and grant permissions on keyspace CREATE ROLE thehive WITH PASSWORD = 'thehive1234' AND LOGIN = true ; GRANT ALL PERMISSIONS ON KEYSPACE thehive TO thehive ; Configure TheHive with the account Update /etc/thehive/application.conf accordingly: db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"xxx.xxx.xxx.xxx\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"thehive1234\" cql { cluster-name: thp keyspace: thehive } } Cassandra node to node encryption # This document addresses communication between Cassandra servers, when a Cassandra cluster contains several nodes. server_encryption_options : internode_encryption : all keystore : /path/to/keystore.jks keystore_password : keystorepassword truststore : /path/to/truststore.jks truststore_password : truststorepassword # More advanced defaults below: protocol : TLS algorithm : SunX509 store_type : JKS cipher_suites : [ TLS_RSA_WITH_AES_128_CBC_SHA , TLS_RSA_WITH_AES_256_CBC_SHA , TLS_DHE_RSA_WITH_ AES_128_CBC_SHA , TLS_DHE_RSA_WITH_AES_256_CBC_SHA , TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA , TLS_ECDHE_R SA_WITH_AES_256_CBC_SHA ] require_client_auth : false Cassandra dedicated port for SSL (optional) # Optionally, you can setup a dedicated port for SSL communication. Update /etc/cassandra/cassandra.yml configuration file on each node: native_transport_port_ssl : 9142 Note By doing so, all SSL communications will be done using this port. Without this parameter, SSL is setup on native_transport_port . (everything is explained in the cassandra.yaml configuration file). Client to node encryption # This guide explains how to secure connection between Cassandra server and Cassandra clients (TheHive). This document doesn\u2019t address communication between Cassandra servers, when a Cassandra cluster contains several nodes. Requirements # The setup requires a valid X509 certificate for the Cassandra service. It must have standard properties of server certificate: key usage: Digital Signature, Non Repudiation, Key Encipherment, Key Agreement Extended Key Usage: TLS Web Server Authentication Cert Type: SSL Server It also must have a \"Subject Alternative Name\" with the identifier (DNS name or/and IP address) of the Cassandra server seen by the client. The format of the certificate file is PKCS12 (file with extention p12). Then create a truststore containing the certificate authority used to generate the certificate for Cassandra. The truststore must be in Java format (JKS). If you CA file is ca.crt, you can generate the truststore file with the following command: keytool -import -file /path/to/ca.crt -alias CA -keystore ca.jks This command ask a password for file integrity checking. The command keytool is available in any JDK distribution. Configuring Cassandra # Locate the section client_encryption_options and set the following options: client_encryption_options : enabled : true # If enabled and optional is set to true encrypted and unencrypted connections are handled. optional : false keystore : /pat/to/keystore.jks keystore_password : keystorepassword require_client_auth : false # Set trustore and truststore_password if require_client_auth is true # truststore: conf/.truststore # truststore_password: cassandra # More advanced defaults below: protocol : TLS algorithm : SunX509 store_type : JKS cipher_suites : [ TLS_RSA_WITH_AES_128_CBC_SHA , TLS_RSA_WITH_AES_256_CBC_SHA , TLS_DHE_RSA_WITH_ AES_128_CBC_SHA , TLS_DHE_RSA_WITH_AES_256_CBC_SHA , TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA , TLS_ECDHE_R SA_WITH_AES_256_CBC_SHA ] Then the service cassandra must be restarted. Configuring TheHive # db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"ip_node_1\", \"ip_node_2\", \"ip_node_3\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"thehive1234\" port: 9142 # if alternative port has been set in Cassandra configuration cql { cluster-name: thp keyspace: thehive ssl { enabled: true truststore { location: \"/path/to/truststore.jks\" password: \"truststorepassword\" } } } } Then the service thehive must be restarted.","title":"Cassandra & security"},{"location":"thehive/operations/cassandra-security/#security-in-apache-cassandra","text":"References Internal authentication https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureInternalAuthenticationTOC.html Node to node encryption https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLNodeToNode.html Client to node encryption https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/configuration/secureSSLClientToNode.html https://docs.janusgraph.org/basics/configuration-reference/#storagecqlssl","title":"Security in Apache Cassandra"},{"location":"thehive/operations/cassandra-security/#authentication-with-cassandra","text":"Create an account and grant permissions on keyspace CREATE ROLE thehive WITH PASSWORD = 'thehive1234' AND LOGIN = true ; GRANT ALL PERMISSIONS ON KEYSPACE thehive TO thehive ; Configure TheHive with the account Update /etc/thehive/application.conf accordingly: db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"xxx.xxx.xxx.xxx\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"thehive1234\" cql { cluster-name: thp keyspace: thehive } }","title":"Authentication with Cassandra"},{"location":"thehive/operations/cassandra-security/#cassandra-node-to-node-encryption","text":"This document addresses communication between Cassandra servers, when a Cassandra cluster contains several nodes. server_encryption_options : internode_encryption : all keystore : /path/to/keystore.jks keystore_password : keystorepassword truststore : /path/to/truststore.jks truststore_password : truststorepassword # More advanced defaults below: protocol : TLS algorithm : SunX509 store_type : JKS cipher_suites : [ TLS_RSA_WITH_AES_128_CBC_SHA , TLS_RSA_WITH_AES_256_CBC_SHA , TLS_DHE_RSA_WITH_ AES_128_CBC_SHA , TLS_DHE_RSA_WITH_AES_256_CBC_SHA , TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA , TLS_ECDHE_R SA_WITH_AES_256_CBC_SHA ] require_client_auth : false","title":"Cassandra node to node encryption"},{"location":"thehive/operations/cassandra-security/#cassandra-dedicated-port-for-ssl-optional","text":"Optionally, you can setup a dedicated port for SSL communication. Update /etc/cassandra/cassandra.yml configuration file on each node: native_transport_port_ssl : 9142 Note By doing so, all SSL communications will be done using this port. Without this parameter, SSL is setup on native_transport_port . (everything is explained in the cassandra.yaml configuration file).","title":"Cassandra dedicated port for SSL (optional)"},{"location":"thehive/operations/cassandra-security/#client-to-node-encryption","text":"This guide explains how to secure connection between Cassandra server and Cassandra clients (TheHive). This document doesn\u2019t address communication between Cassandra servers, when a Cassandra cluster contains several nodes.","title":"Client to node encryption"},{"location":"thehive/operations/cassandra-security/#requirements","text":"The setup requires a valid X509 certificate for the Cassandra service. It must have standard properties of server certificate: key usage: Digital Signature, Non Repudiation, Key Encipherment, Key Agreement Extended Key Usage: TLS Web Server Authentication Cert Type: SSL Server It also must have a \"Subject Alternative Name\" with the identifier (DNS name or/and IP address) of the Cassandra server seen by the client. The format of the certificate file is PKCS12 (file with extention p12). Then create a truststore containing the certificate authority used to generate the certificate for Cassandra. The truststore must be in Java format (JKS). If you CA file is ca.crt, you can generate the truststore file with the following command: keytool -import -file /path/to/ca.crt -alias CA -keystore ca.jks This command ask a password for file integrity checking. The command keytool is available in any JDK distribution.","title":"Requirements"},{"location":"thehive/operations/cassandra-security/#configuring-cassandra","text":"Locate the section client_encryption_options and set the following options: client_encryption_options : enabled : true # If enabled and optional is set to true encrypted and unencrypted connections are handled. optional : false keystore : /pat/to/keystore.jks keystore_password : keystorepassword require_client_auth : false # Set trustore and truststore_password if require_client_auth is true # truststore: conf/.truststore # truststore_password: cassandra # More advanced defaults below: protocol : TLS algorithm : SunX509 store_type : JKS cipher_suites : [ TLS_RSA_WITH_AES_128_CBC_SHA , TLS_RSA_WITH_AES_256_CBC_SHA , TLS_DHE_RSA_WITH_ AES_128_CBC_SHA , TLS_DHE_RSA_WITH_AES_256_CBC_SHA , TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA , TLS_ECDHE_R SA_WITH_AES_256_CBC_SHA ] Then the service cassandra must be restarted.","title":"Configuring Cassandra"},{"location":"thehive/operations/cassandra-security/#configuring-thehive","text":"db.janusgraph { storage { ## Cassandra configuration # More information at https://docs.janusgraph.org/basics/configuration-reference/#storagecql backend: cql hostname: [\"ip_node_1\", \"ip_node_2\", \"ip_node_3\"] # Cassandra authentication (if configured) username: \"thehive\" password: \"thehive1234\" port: 9142 # if alternative port has been set in Cassandra configuration cql { cluster-name: thp keyspace: thehive ssl { enabled: true truststore { location: \"/path/to/truststore.jks\" password: \"truststorepassword\" } } } } Then the service thehive must be restarted.","title":"Configuring TheHive"},{"location":"thehive/operations/fail2ban/","text":"Fail2ban # Adding TheHive into Fail2Ban # Considering TheHive logs sit in /var/log/thehive/application.log and fail2ban configuration is in /etc/fail2ban : Add a filter file in /etc/fail2ban/filter.d named thehive.conf with the following content: [INCLUDES] before = common.conf [Definition] failregex = ^.*- <HOST> (?:POST \\/api\\/login|GET .*) .*returned 401.*$ ignoreregex = Add a jail file in /etc/fail2ban/jail.d/ named thehive.local with the following content: [thehive] enabled = true port = 80,443 filter = thehive action = iptables-multiport[name=thehive, port=\"80,443\"] logpath = /var/log/thehive/application.log maxretry = 5 bantime = 14400 findtime = 1200 This will ban any IP address for 4 hours after 5 failed authentication are identified during a period of 20 min. Reload the configuration with the command fail2ban-client reload Manage banned IP addresses # Review banned IP addresses: fail2ban-client status thehive Unban an IP address: fail2ban-client set thehive unbanip <IP ADDRESS>","title":"Use fail2ban"},{"location":"thehive/operations/fail2ban/#fail2ban","text":"","title":"Fail2ban"},{"location":"thehive/operations/fail2ban/#adding-thehive-into-fail2ban","text":"Considering TheHive logs sit in /var/log/thehive/application.log and fail2ban configuration is in /etc/fail2ban : Add a filter file in /etc/fail2ban/filter.d named thehive.conf with the following content: [INCLUDES] before = common.conf [Definition] failregex = ^.*- <HOST> (?:POST \\/api\\/login|GET .*) .*returned 401.*$ ignoreregex = Add a jail file in /etc/fail2ban/jail.d/ named thehive.local with the following content: [thehive] enabled = true port = 80,443 filter = thehive action = iptables-multiport[name=thehive, port=\"80,443\"] logpath = /var/log/thehive/application.log maxretry = 5 bantime = 14400 findtime = 1200 This will ban any IP address for 4 hours after 5 failed authentication are identified during a period of 20 min. Reload the configuration with the command fail2ban-client reload","title":"Adding TheHive into Fail2Ban"},{"location":"thehive/operations/fail2ban/#manage-banned-ip-addresses","text":"Review banned IP addresses: fail2ban-client status thehive Unban an IP address: fail2ban-client set thehive unbanip <IP ADDRESS>","title":"Manage banned IP addresses"},{"location":"thehive/operations/https/","text":"","title":"Configure HTTPS"},{"location":"thehive/operations/migration/","text":"Migration to TheHive 4 # TheHive 4.x is delivered with a tool to migrate your data from TheHive 3.x. stored in Elasticsearch. Supported versions # Starting with TheHive 4.1.17, the migration tool supports migrating data from both TheHive 3.4.x and 3.5.x. Migrating from Possible target version TheHive 3.4.x + Elasticsearch 6.x TheHive 4.1.17+ TheHive 3.5.x + Elasticsearch 7.x TheHive 4.1.17+ How it works # All packages of TheHive4 distributed come with the migration program which can be used to import data from TheHive 3.4.0+. By default, it is installed in /opt/thehive/bin/migrate . Pre-requisite # In order to migrate the data: TheHive 4 must be installed on the system running the migration tool; TheHive4 must be configured ; in particular database , index , and file storage ; The service thehive must be stopped ( service thehive stop ) on the target server. This tools must also have access to Elasticsearch database (http://ES:9200) used by TheHive 3, and the configuration file of TheHive 3.x instance. Configuration of TheHive 4 # Warning In TheHive4, users are identified by their email addresses. Thus, a domain will be appended to usernames in order to migrate users from TheHive 3. TheHive 4.x comes with a default domain named thehive.local . Starting the migration without explicitely specifying a domain name will result in migrating all users with a username formatted like user@thehive.local . Change the default domain name used to import existing users in the configuration file of TheHive4 ( /etc/thehive/application.conf ) ; add or update the setting named auth.defaultUserDomain : auth.defaultUserDomain : \"mydomain.com\" This way, the domain mydomain.com will be appended to user accounts imported from TheHive 3.4+ ( user@mydomain.com ). Run the migration # Prepare, install and configure your new instance of TheHive 4.x by following the associated guides . Once TheHive4 configuration file ( /etc/thehive/application.conf ) is correctly filled the migrate command ca be executed. Info This recommended to run this program as the user in charge of running TheHive service ( thehive if you are installing the application with DEB or RPM package) The program comes with a large set of options: # /opt/thehive/bin/migrate --help TheHive migration tool 4.1.17-1 Usage: migrate [options] -v, --version -h, --help -l, --logger-config <file> logback configuration file -c, --config <file> global configuration file -i, --input <file> TheHive3 configuration file -o, --output <file> TheHive4 configuration file -d, --drop-database Drop TheHive4 database before migration -r, --resume Resume migration (or migrate on existing database) -m, --main-organisation <organisation> -u, --es-uri http://ip1:port,ip2:port TheHive3 ElasticSearch URI -e, --es-index <index> TheHive3 ElasticSearch index name -x, --es-index-version <index> TheHive3 ElasticSearch index name version number (default: autodetect) -a, --es-keepalive <duration> TheHive3 ElasticSearch keepalive -p, --es-pagesize <value> TheHive3 ElasticSearch page size -s, --es-single-type <bool> Elasticsearch single type -y, --transaction-pagesize <value> page size for each transaction -t, --thread-count <value> number of threads --max-case-age <duration> migrate only cases whose age is less than <duration> --min-case-age <duration> migrate only cases whose age is greater than <duration> --case-from-date <date> migrate only cases created from <date> --case-until-date <date> migrate only cases created until <date> --case-from-number <number> migrate only cases from this case number --case-until-number <number> migrate only cases until this case number --max-alert-age <duration> migrate only alerts whose age is less than <duration> --min-alert-age <duration> migrate only alerts whose age is greater than <duration> --alert-from-date <date> migrate only alerts created from <date> --alert-until-date <date> migrate only alerts created until <date> --include-alert-types <type>,<type>... migrate only alerts with this types --exclude-alert-types <type>,<type>... don't migrate alerts with this types --include-alert-sources <source>,<source>... migrate only alerts with this sources --exclude-alert-sources <source>,<source>... don't migrate alerts with this sources --max-audit-age <duration> migrate only audits whose age is less than <duration> --min-audit-age <duration> migrate only audits whose age is greater than <duration> --audit-from-date <date> migrate only audits created from <date> --audit-until-date <date> migrate only audits created until <date> --include-audit-actions <value> migration only audits with this action (Update, Creation, Delete) --exclude-audit-actions <value> don't migration audits with this action (Update, Creation, Delete) --include-audit-objectTypes <value> migration only audits with this objectType (case, case_artifact, case_task, ...) --exclude-audit-objectTypes <value> don't migration audits with this objectType (case, case_artifact, case_task, ...) --case-number-shift <value> transpose case number by adding this value Accepted date formats are \"yyyyMMdd[HH[mm[ss]]]\" and \"MMdd\" The Format for duration is: <length> <unit>. Accepted units are: DAY: d, day HOUR: h, hr, hour MINUTE: m, min, minute SECOND: s, sec, second MILLISECOND: ms, milli, millisecond Most of these options are filters you can apply to the program. For example, you could decide to import only some of the Cases/Alerts from your old instance: Import Cases/Alerts not older than X days/hours, Import Cases/Alerts with the ID number, Import part of Audit trail ... Taking the assumption that you are migrating a database hosted in a remote server, with TheHive 3, a basic command line to migrate data to a new instance will be like: /opt/thehive/bin/migrate \\ --output /etc/thehive/application.conf \\ --main-organisation myOrganisation \\ --es-uri http://ELASTICSEARCH_IP_ADDRESS:9200 \\ --es-index the_hive with: Option Description --output specifies the configuration file of TheHive 4.0 (the one that has previously been configured with at least, the database and file storage ) --main-organisation specifies the Organisation named myOrganisation to create during the migration. The tool will then create Users, Cases and Alerts from TheHive3 under that organisation; --es-uri specifies the URL of the Elasticsearch server. If using authentication on Elasticsearch, --input option with a configuration file for TheHive3 is required --es-index specifies the index used in Elasticsearch. Example TheHive 3.4.x + Elasticsearch 6.x When migrating, start a new database, create an organisation named StrangeBee , add all users in this organisation, and do not keep audit trails older than 90d. Requirements The option --es-single-type true is mandatory to migrate data from Elasticsearch 6.x /opt/thehive/bin/migrate \\ --drop-database \\ --input /etc/thehive/thehive3.conf \\ --output /etc/thehive/application.conf \\ --main-organisation StrangeBee \\ --max-audit-age 90d \\ --es-single-type true TheHive 3.5.x + Elasticsearch 7.x When migrating, start a new database, create an organisation named StrangeBee , add all users in this organisation, and do not keep alert trails created before the March, 25th of 2019. /opt/thehive/bin/migrate \\ --drop-database \\ --input /etc/thehive/thehive3.conf \\ --output /etc/thehive/application.conf \\ --main-organisation StrangeBee \\ --alert-from-date 20190325 Info The migration process can be very long, from several hours to several days, depending on the volume of data to migrate. We highly recommand to not start the application during the migration. Using authentication on Cassandra # if you are using a dedicated account on Cassandra to access TheHive 4 data, this user must have permissions to create keyspaces on the database: GRANT CREATE on ALL KEYSPACES to username ; Migration logs # The migration tool generates some logs during the process. By default, every 10 sec. a log is generated with information regarding the situation of the migration: [info] o.t.t.m.Migrate - [Migrate cases and alerts] CaseTemplate/Task:32 Organisation:1/1 Case/Task:160/201 Case:31/52 Job:103/138 ObservableType:3/17 Alert:25/235 Audit:3207/2986 CaseTemplate:6/6 Alert/Observable:700(52ms) Case/Observable:1325/1665 User:9/9 CustomField:13/13 Case/Task/Log:20/27 Numbers of Observables, Cases and others are estimations and not a definite value as computing these number can be very tedious. Files from MISP imported with TheHive 2.13 and earlier It is important to notice that migrating Cases/Alerts containing MISP event that were imported with TheHive 2.13 ( Sept 2017 ) or older, will cause observable files not being imported in TheHive 4. Indeed, until this version, TheHive referenced the file to the AttributeId in MISP and was not automatically downloaded. It then could generate a log like this: [warn] o.t.t.m.t.Input - Pre 2.13 file observables are ignored in MISP alert ffa3a8503ab0cd4f99fc6937a8e9b827 Starting TheHive 4 # Once the migration process is sucessfully completed, TheHive4 can be started. Warning During the first start data are indexed and service is not available ; this can take some time. Do not stop or restart the service at this time.","title":"Migration from TheHive 3.x"},{"location":"thehive/operations/migration/#migration-to-thehive-4","text":"TheHive 4.x is delivered with a tool to migrate your data from TheHive 3.x. stored in Elasticsearch.","title":"Migration to TheHive 4"},{"location":"thehive/operations/migration/#supported-versions","text":"Starting with TheHive 4.1.17, the migration tool supports migrating data from both TheHive 3.4.x and 3.5.x. Migrating from Possible target version TheHive 3.4.x + Elasticsearch 6.x TheHive 4.1.17+ TheHive 3.5.x + Elasticsearch 7.x TheHive 4.1.17+","title":"Supported versions"},{"location":"thehive/operations/migration/#how-it-works","text":"All packages of TheHive4 distributed come with the migration program which can be used to import data from TheHive 3.4.0+. By default, it is installed in /opt/thehive/bin/migrate .","title":"How it works"},{"location":"thehive/operations/migration/#pre-requisite","text":"In order to migrate the data: TheHive 4 must be installed on the system running the migration tool; TheHive4 must be configured ; in particular database , index , and file storage ; The service thehive must be stopped ( service thehive stop ) on the target server. This tools must also have access to Elasticsearch database (http://ES:9200) used by TheHive 3, and the configuration file of TheHive 3.x instance.","title":"Pre-requisite"},{"location":"thehive/operations/migration/#configuration-of-thehive-4","text":"Warning In TheHive4, users are identified by their email addresses. Thus, a domain will be appended to usernames in order to migrate users from TheHive 3. TheHive 4.x comes with a default domain named thehive.local . Starting the migration without explicitely specifying a domain name will result in migrating all users with a username formatted like user@thehive.local . Change the default domain name used to import existing users in the configuration file of TheHive4 ( /etc/thehive/application.conf ) ; add or update the setting named auth.defaultUserDomain : auth.defaultUserDomain : \"mydomain.com\" This way, the domain mydomain.com will be appended to user accounts imported from TheHive 3.4+ ( user@mydomain.com ).","title":"Configuration of TheHive 4"},{"location":"thehive/operations/migration/#run-the-migration","text":"Prepare, install and configure your new instance of TheHive 4.x by following the associated guides . Once TheHive4 configuration file ( /etc/thehive/application.conf ) is correctly filled the migrate command ca be executed. Info This recommended to run this program as the user in charge of running TheHive service ( thehive if you are installing the application with DEB or RPM package) The program comes with a large set of options: # /opt/thehive/bin/migrate --help TheHive migration tool 4.1.17-1 Usage: migrate [options] -v, --version -h, --help -l, --logger-config <file> logback configuration file -c, --config <file> global configuration file -i, --input <file> TheHive3 configuration file -o, --output <file> TheHive4 configuration file -d, --drop-database Drop TheHive4 database before migration -r, --resume Resume migration (or migrate on existing database) -m, --main-organisation <organisation> -u, --es-uri http://ip1:port,ip2:port TheHive3 ElasticSearch URI -e, --es-index <index> TheHive3 ElasticSearch index name -x, --es-index-version <index> TheHive3 ElasticSearch index name version number (default: autodetect) -a, --es-keepalive <duration> TheHive3 ElasticSearch keepalive -p, --es-pagesize <value> TheHive3 ElasticSearch page size -s, --es-single-type <bool> Elasticsearch single type -y, --transaction-pagesize <value> page size for each transaction -t, --thread-count <value> number of threads --max-case-age <duration> migrate only cases whose age is less than <duration> --min-case-age <duration> migrate only cases whose age is greater than <duration> --case-from-date <date> migrate only cases created from <date> --case-until-date <date> migrate only cases created until <date> --case-from-number <number> migrate only cases from this case number --case-until-number <number> migrate only cases until this case number --max-alert-age <duration> migrate only alerts whose age is less than <duration> --min-alert-age <duration> migrate only alerts whose age is greater than <duration> --alert-from-date <date> migrate only alerts created from <date> --alert-until-date <date> migrate only alerts created until <date> --include-alert-types <type>,<type>... migrate only alerts with this types --exclude-alert-types <type>,<type>... don't migrate alerts with this types --include-alert-sources <source>,<source>... migrate only alerts with this sources --exclude-alert-sources <source>,<source>... don't migrate alerts with this sources --max-audit-age <duration> migrate only audits whose age is less than <duration> --min-audit-age <duration> migrate only audits whose age is greater than <duration> --audit-from-date <date> migrate only audits created from <date> --audit-until-date <date> migrate only audits created until <date> --include-audit-actions <value> migration only audits with this action (Update, Creation, Delete) --exclude-audit-actions <value> don't migration audits with this action (Update, Creation, Delete) --include-audit-objectTypes <value> migration only audits with this objectType (case, case_artifact, case_task, ...) --exclude-audit-objectTypes <value> don't migration audits with this objectType (case, case_artifact, case_task, ...) --case-number-shift <value> transpose case number by adding this value Accepted date formats are \"yyyyMMdd[HH[mm[ss]]]\" and \"MMdd\" The Format for duration is: <length> <unit>. Accepted units are: DAY: d, day HOUR: h, hr, hour MINUTE: m, min, minute SECOND: s, sec, second MILLISECOND: ms, milli, millisecond Most of these options are filters you can apply to the program. For example, you could decide to import only some of the Cases/Alerts from your old instance: Import Cases/Alerts not older than X days/hours, Import Cases/Alerts with the ID number, Import part of Audit trail ... Taking the assumption that you are migrating a database hosted in a remote server, with TheHive 3, a basic command line to migrate data to a new instance will be like: /opt/thehive/bin/migrate \\ --output /etc/thehive/application.conf \\ --main-organisation myOrganisation \\ --es-uri http://ELASTICSEARCH_IP_ADDRESS:9200 \\ --es-index the_hive with: Option Description --output specifies the configuration file of TheHive 4.0 (the one that has previously been configured with at least, the database and file storage ) --main-organisation specifies the Organisation named myOrganisation to create during the migration. The tool will then create Users, Cases and Alerts from TheHive3 under that organisation; --es-uri specifies the URL of the Elasticsearch server. If using authentication on Elasticsearch, --input option with a configuration file for TheHive3 is required --es-index specifies the index used in Elasticsearch. Example TheHive 3.4.x + Elasticsearch 6.x When migrating, start a new database, create an organisation named StrangeBee , add all users in this organisation, and do not keep audit trails older than 90d. Requirements The option --es-single-type true is mandatory to migrate data from Elasticsearch 6.x /opt/thehive/bin/migrate \\ --drop-database \\ --input /etc/thehive/thehive3.conf \\ --output /etc/thehive/application.conf \\ --main-organisation StrangeBee \\ --max-audit-age 90d \\ --es-single-type true TheHive 3.5.x + Elasticsearch 7.x When migrating, start a new database, create an organisation named StrangeBee , add all users in this organisation, and do not keep alert trails created before the March, 25th of 2019. /opt/thehive/bin/migrate \\ --drop-database \\ --input /etc/thehive/thehive3.conf \\ --output /etc/thehive/application.conf \\ --main-organisation StrangeBee \\ --alert-from-date 20190325 Info The migration process can be very long, from several hours to several days, depending on the volume of data to migrate. We highly recommand to not start the application during the migration.","title":"Run the migration"},{"location":"thehive/operations/migration/#using-authentication-on-cassandra","text":"if you are using a dedicated account on Cassandra to access TheHive 4 data, this user must have permissions to create keyspaces on the database: GRANT CREATE on ALL KEYSPACES to username ;","title":"Using authentication on Cassandra"},{"location":"thehive/operations/migration/#migration-logs","text":"The migration tool generates some logs during the process. By default, every 10 sec. a log is generated with information regarding the situation of the migration: [info] o.t.t.m.Migrate - [Migrate cases and alerts] CaseTemplate/Task:32 Organisation:1/1 Case/Task:160/201 Case:31/52 Job:103/138 ObservableType:3/17 Alert:25/235 Audit:3207/2986 CaseTemplate:6/6 Alert/Observable:700(52ms) Case/Observable:1325/1665 User:9/9 CustomField:13/13 Case/Task/Log:20/27 Numbers of Observables, Cases and others are estimations and not a definite value as computing these number can be very tedious. Files from MISP imported with TheHive 2.13 and earlier It is important to notice that migrating Cases/Alerts containing MISP event that were imported with TheHive 2.13 ( Sept 2017 ) or older, will cause observable files not being imported in TheHive 4. Indeed, until this version, TheHive referenced the file to the AttributeId in MISP and was not automatically downloaded. It then could generate a log like this: [warn] o.t.t.m.t.Input - Pre 2.13 file observables are ignored in MISP alert ffa3a8503ab0cd4f99fc6937a8e9b827","title":"Migration logs"},{"location":"thehive/operations/migration/#starting-thehive-4","text":"Once the migration process is sucessfully completed, TheHive4 can be started. Warning During the first start data are indexed and service is not available ; this can take some time. Do not stop or restart the service at this time.","title":"Starting TheHive 4"},{"location":"thehive/operations/troubleshooting/","text":"Troubleshooting # For some issues, we need extra information in logs to troubleshoot and understand to root causes. To gather and share this, please read carefully and follow these steps. Warning ENABLING TRACE LOGS HAS SIGNIFICANT IMPACT ON PERFORMANCES. DO NOT ENABLE IT ON PRODUCTION SERVERS. Stop TheHive service and ensure it is stopped # service thehive stop Ensure the service is stopped with the following command: service thehive status Renew application.log file # in /var/log/thehive move the file application.log to application.log.bak mv /var/log/thehive/application.log /var/log/thehive/application.log.bak Update log configuration # Edit the file /etc/thehive/logback.xml . Look for the line containing <logger name=\"org.thp\" level=\"INFO\"/> and update it to have following lines: [..] <logger name= \"org.thp\" level= \"TRACE\" /> [..] Save the file. Restart the service # service thehive start A new log file /var/log/thehive/application.log should be created and filed with a huge amount of logs. Wait for the issue to appear and/or the application stop. Save the logs # Copy the log file in a safe place. cp /var/log/thehive/application.log /root Share it with us # Create an issue on Github and please share context and symptoms with the log file. Please add information regarding: Context: instance (single node/cluster, backend type, index engine) System: Operating System, amount of RAM, #CPU for each server/node Symptoms: what you did, how you you come to this situation, what happened The log file with traces Revert # To get back a to normal log configuration, stop thehive, update logback.xml file with the previous configuration, and restart the application.","title":"Troubleshooting"},{"location":"thehive/operations/troubleshooting/#troubleshooting","text":"For some issues, we need extra information in logs to troubleshoot and understand to root causes. To gather and share this, please read carefully and follow these steps. Warning ENABLING TRACE LOGS HAS SIGNIFICANT IMPACT ON PERFORMANCES. DO NOT ENABLE IT ON PRODUCTION SERVERS.","title":"Troubleshooting"},{"location":"thehive/operations/troubleshooting/#stop-thehive-service-and-ensure-it-is-stopped","text":"service thehive stop Ensure the service is stopped with the following command: service thehive status","title":"Stop TheHive service and ensure it is stopped"},{"location":"thehive/operations/troubleshooting/#renew-applicationlog-file","text":"in /var/log/thehive move the file application.log to application.log.bak mv /var/log/thehive/application.log /var/log/thehive/application.log.bak","title":"Renew application.log file"},{"location":"thehive/operations/troubleshooting/#update-log-configuration","text":"Edit the file /etc/thehive/logback.xml . Look for the line containing <logger name=\"org.thp\" level=\"INFO\"/> and update it to have following lines: [..] <logger name= \"org.thp\" level= \"TRACE\" /> [..] Save the file.","title":"Update log configuration"},{"location":"thehive/operations/troubleshooting/#restart-the-service","text":"service thehive start A new log file /var/log/thehive/application.log should be created and filed with a huge amount of logs. Wait for the issue to appear and/or the application stop.","title":"Restart the service"},{"location":"thehive/operations/troubleshooting/#save-the-logs","text":"Copy the log file in a safe place. cp /var/log/thehive/application.log /root","title":"Save the logs"},{"location":"thehive/operations/troubleshooting/#share-it-with-us","text":"Create an issue on Github and please share context and symptoms with the log file. Please add information regarding: Context: instance (single node/cluster, backend type, index engine) System: Operating System, amount of RAM, #CPU for each server/node Symptoms: what you did, how you you come to this situation, what happened The log file with traces","title":"Share it with us"},{"location":"thehive/operations/troubleshooting/#revert","text":"To get back a to normal log configuration, stop thehive, update logback.xml file with the previous configuration, and restart the application.","title":"Revert"},{"location":"thehive/operations/update/","text":"Update guides # Update from TheHive 4.0.x to TheHive 4.1.0 # TheHive 4.1.0 comes with an updated application stack, with new components dedicated to performance improvement. TheHive 4.1.0 requires the usage of a dedicated index engine to manages indexed data. As a result, the minimum configuration required has been updated: If you are a new user of TheHive, follow the installation and configuration guide . If you are an existing user of TheHive 4.0.x, an index engine should be configured alongside the database. And wether you are using a standalone server or a cluster, the solution to implement and the configuration to update are different. According to the setup, the instance can use: A local engine, Lucene driven by TheHive A centralised engine, Elasticsearch . Updating a standalone server # In this case, a Lucene can be used. TheHive 4.1.0 comes with its Lucene engine. The configuration of TheHive, can be updated like this: Create a dedicated folder for indexes (for example /opt/thp/thehive/index ). This folder should belong to the user thehive:thehive mkdir /opt/thp/thehive/index chown -R thehive:thehive /opt/thp/thehive/index Add the index configuration in the db.janusgraph part: ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"127.0.0.1\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Restart TheHive service thehive restart Once TheHive configuration is updated and restarted, new indexes are created during the start-up phase. Warning The start-up phase of TheHive and the indexes creation can take a certain amount if time. This phase will be quicker once indexes exist. Updating a cluster # In this case, a Elasticsearch should be used, as all nodes should have access to the same index. Once your Elasticsearch instance is up and running, The configuration of TheHive can be updated like this: Here is an example of configuration, use your IP address/hostnames. ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"10.1.2.1\", \"10.1.2.2\", \"10.1.2.3\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : elasticsearch hostname : [\"10.1.2.5\"] index-name : thehive } } } } In this configuration, all TheHive nodes should have the same configuration. Restart all nodes of the cluster. Info the cluster makes it work out ; one of the nodes manage the indexing process while others are waiting for it to be ready. More information # More information about the configuration of database and indexes can be found in the dedicated configuration guide","title":"Howto update"},{"location":"thehive/operations/update/#update-guides","text":"","title":"Update guides"},{"location":"thehive/operations/update/#update-from-thehive-40x-to-thehive-410","text":"TheHive 4.1.0 comes with an updated application stack, with new components dedicated to performance improvement. TheHive 4.1.0 requires the usage of a dedicated index engine to manages indexed data. As a result, the minimum configuration required has been updated: If you are a new user of TheHive, follow the installation and configuration guide . If you are an existing user of TheHive 4.0.x, an index engine should be configured alongside the database. And wether you are using a standalone server or a cluster, the solution to implement and the configuration to update are different. According to the setup, the instance can use: A local engine, Lucene driven by TheHive A centralised engine, Elasticsearch .","title":"Update from TheHive 4.0.x to TheHive 4.1.0"},{"location":"thehive/operations/update/#updating-a-standalone-server","text":"In this case, a Lucene can be used. TheHive 4.1.0 comes with its Lucene engine. The configuration of TheHive, can be updated like this: Create a dedicated folder for indexes (for example /opt/thp/thehive/index ). This folder should belong to the user thehive:thehive mkdir /opt/thp/thehive/index chown -R thehive:thehive /opt/thp/thehive/index Add the index configuration in the db.janusgraph part: ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"127.0.0.1\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend: lucene directory: /opt/thp/thehive/index } } } } Restart TheHive service thehive restart Once TheHive configuration is updated and restarted, new indexes are created during the start-up phase. Warning The start-up phase of TheHive and the indexes creation can take a certain amount if time. This phase will be quicker once indexes exist.","title":"Updating a standalone server"},{"location":"thehive/operations/update/#updating-a-cluster","text":"In this case, a Elasticsearch should be used, as all nodes should have access to the same index. Once your Elasticsearch instance is up and running, The configuration of TheHive can be updated like this: Here is an example of configuration, use your IP address/hostnames. ## Database Configuration db { provider: janusgraph janusgraph { ## Storage configuration storage { backend: cql hostname: [\"10.1.2.1\", \"10.1.2.2\", \"10.1.2.3\"] ## Cassandra authentication (if configured) username: \"thehive_account\" password: \"cassandra_password\" cql { cluster-name: thp keyspace: thehive } } ## Index configuration index { search { backend : elasticsearch hostname : [\"10.1.2.5\"] index-name : thehive } } } } In this configuration, all TheHive nodes should have the same configuration. Restart all nodes of the cluster. Info the cluster makes it work out ; one of the nodes manage the indexing process while others are waiting for it to be ready.","title":"Updating a cluster"},{"location":"thehive/operations/update/#more-information","text":"More information about the configuration of database and indexes can be found in the dedicated configuration guide","title":"More information"},{"location":"thehive/user-guides/","text":"User guides # Get a Quick start with TheHive or follow the guides bellow for more details: For administrators # Administrators as users defined in the admin organisation, created by default in TheHive. Administators have the responsibility of managing the platform by defining organisations and all the platform data available for to all the organisations. Manage organisations Manage profiles, roles and permissions Manage Custom fields Manage Observable types Manage Analyzers templates Manage Tags & taxonomies Manage Tactics, Techniques & Procedures Platform Status For organisation managers # Organisation managers are users belonging to any organisation other than admin and having one of the following permissions to manage users, case template, custom tags and UI configuration. TheHive comes with a default role for organisation managers, called org-admin . Organisations, users and sharing Manage users Manage Case templates Manage custom tags Manage UI configurations For Analysts # Analysts are user belonging to any organisation other than admin without organisation management permissions. Create Alerts Create Cases Create Tasks Create Observables Create TTPs Run Responders Run Analyzers Sharing Cases, Tasks and Observables Close Cases Export Cases to MISP User settings","title":"User guides"},{"location":"thehive/user-guides/#user-guides","text":"Get a Quick start with TheHive or follow the guides bellow for more details:","title":"User guides"},{"location":"thehive/user-guides/#for-administrators","text":"Administrators as users defined in the admin organisation, created by default in TheHive. Administators have the responsibility of managing the platform by defining organisations and all the platform data available for to all the organisations. Manage organisations Manage profiles, roles and permissions Manage Custom fields Manage Observable types Manage Analyzers templates Manage Tags & taxonomies Manage Tactics, Techniques & Procedures Platform Status","title":"For administrators"},{"location":"thehive/user-guides/#for-organisation-managers","text":"Organisation managers are users belonging to any organisation other than admin and having one of the following permissions to manage users, case template, custom tags and UI configuration. TheHive comes with a default role for organisation managers, called org-admin . Organisations, users and sharing Manage users Manage Case templates Manage custom tags Manage UI configurations","title":"For organisation managers"},{"location":"thehive/user-guides/#for-analysts","text":"Analysts are user belonging to any organisation other than admin without organisation management permissions. Create Alerts Create Cases Create Tasks Create Observables Create TTPs Run Responders Run Analyzers Sharing Cases, Tasks and Observables Close Cases Export Cases to MISP User settings","title":"For Analysts"},{"location":"thehive/user-guides/quick-start/","text":"Quick start with TheHive # TL;DR # Default administrator account: admin@thehive.local / secret Login with default account Create an organisaton Create a user account Before starting # Starting from TheHive 4.0-RC1, an email address is requested, and is mandatory to register a new user, and to log in the application. Intialize TheHive 4 # This version of TheHive comes with a big improvement: Multi-tenancy support Fine grained permissions Customized user profiles (a set of permissions) After TheHive installation, a default organisation called \"admin\" is created and contains the initial default super administrator user, having a profile called \"admin\" . We will discuss the user profiles later, but note that the \"admin\" user has all the administration permissions like: create organisation define profiles define observable types define custom fields Members of \" admin \" organisation are dedicated to user accounts in charge of administrating the solution. The initial user has the following credentials: login: admin@thehive.local password: secret This default group cannot create and own Cases or any other related objects like Tasks or Observables . First login # When TheHive starts the first time, you need to login using the credential of the \"admin\" user indicated above ( admin@thehive.local / secret ), and you will be redirected to the administration home page: List of organisations . Note that this organisation cannot be deleted . Possible operations for the \"admin\" users (members of the \"admin\" organisation) are accessible from the \"Admin\" menu located on the header bar: admin Organisation cannot manage Cases. so let's create an organisation and its users. Create an organisation # The initial action that a super admin have to make is to create the organisations (tenants) that will use TheHive to deal with incident response. From the \"List of organisations\" page, hit the \"New Organisation\" button to open the organisation dialog. The organisation name is required and must be unique. Hit \"Save\" to confirm. Create a user # Once you have created an organisation you can open its details page by clicking \"Configure\". This organisation details page, for users with \"admin\" profile allows managing organisation users only. You can see on this page: The details of the organisation: name, description, the user that created it A tab to manage users: Create them Edit their password, api key Edit their profile Reset their 2FA settings Lock and delete them To create a user, just click the \"Create new user\" button, that opens the user creation dialog. Note: The \"Profile\" field will be populated by the profiles that can be assigned to organisation users only. (Administration profiles will not be listed). The first user you must create for each organisation, should be a user with \"org-admin\" profile. That profile allows all the operations within an organisation. A user with \"org-admin\" profile will be able to connect and configure its organisation by at least: Creating other users Creating case templates Once you have created the users, you can set their passwords (they will be able to change them from their own account page). To do that, click on the \"New password\" button on the corresponding user's row and then hit ENTER or click the green check button: Login as org-admin user # Once the user is created, (s)he can connect to TheHive and start using it based on the profile. Note that users with \"org-admin\" profile have an \"Organisation\" menu in the right side corner of the header bar given access to the organisation configuration page with and additionnal tab for case template management. Now that the organisation and users are created, let's define custom fields and then use them to define case templates.","title":"Quick start with TheHive"},{"location":"thehive/user-guides/quick-start/#quick-start-with-thehive","text":"","title":"Quick start with TheHive"},{"location":"thehive/user-guides/quick-start/#tldr","text":"Default administrator account: admin@thehive.local / secret Login with default account Create an organisaton Create a user account","title":"TL;DR"},{"location":"thehive/user-guides/quick-start/#before-starting","text":"Starting from TheHive 4.0-RC1, an email address is requested, and is mandatory to register a new user, and to log in the application.","title":"Before starting"},{"location":"thehive/user-guides/quick-start/#intialize-thehive-4","text":"This version of TheHive comes with a big improvement: Multi-tenancy support Fine grained permissions Customized user profiles (a set of permissions) After TheHive installation, a default organisation called \"admin\" is created and contains the initial default super administrator user, having a profile called \"admin\" . We will discuss the user profiles later, but note that the \"admin\" user has all the administration permissions like: create organisation define profiles define observable types define custom fields Members of \" admin \" organisation are dedicated to user accounts in charge of administrating the solution. The initial user has the following credentials: login: admin@thehive.local password: secret This default group cannot create and own Cases or any other related objects like Tasks or Observables .","title":"Intialize TheHive 4"},{"location":"thehive/user-guides/quick-start/#first-login","text":"When TheHive starts the first time, you need to login using the credential of the \"admin\" user indicated above ( admin@thehive.local / secret ), and you will be redirected to the administration home page: List of organisations . Note that this organisation cannot be deleted . Possible operations for the \"admin\" users (members of the \"admin\" organisation) are accessible from the \"Admin\" menu located on the header bar: admin Organisation cannot manage Cases. so let's create an organisation and its users.","title":"First login"},{"location":"thehive/user-guides/quick-start/#create-an-organisation","text":"The initial action that a super admin have to make is to create the organisations (tenants) that will use TheHive to deal with incident response. From the \"List of organisations\" page, hit the \"New Organisation\" button to open the organisation dialog. The organisation name is required and must be unique. Hit \"Save\" to confirm.","title":"Create an organisation"},{"location":"thehive/user-guides/quick-start/#create-a-user","text":"Once you have created an organisation you can open its details page by clicking \"Configure\". This organisation details page, for users with \"admin\" profile allows managing organisation users only. You can see on this page: The details of the organisation: name, description, the user that created it A tab to manage users: Create them Edit their password, api key Edit their profile Reset their 2FA settings Lock and delete them To create a user, just click the \"Create new user\" button, that opens the user creation dialog. Note: The \"Profile\" field will be populated by the profiles that can be assigned to organisation users only. (Administration profiles will not be listed). The first user you must create for each organisation, should be a user with \"org-admin\" profile. That profile allows all the operations within an organisation. A user with \"org-admin\" profile will be able to connect and configure its organisation by at least: Creating other users Creating case templates Once you have created the users, you can set their passwords (they will be able to change them from their own account page). To do that, click on the \"New password\" button on the corresponding user's row and then hit ENTER or click the green check button:","title":"Create a user"},{"location":"thehive/user-guides/quick-start/#login-as-org-admin-user","text":"Once the user is created, (s)he can connect to TheHive and start using it based on the profile. Note that users with \"org-admin\" profile have an \"Organisation\" menu in the right side corner of the header bar given access to the organisation configuration page with and additionnal tab for case template management. Now that the organisation and users are created, let's define custom fields and then use them to define case templates.","title":"Login as org-admin user"},{"location":"thehive/user-guides/administrators/analyzer-templates/","text":"Manage analyzer template # Before TheHive4, we used to call them Report templates and we allowed two types of templates: Short reports: used to customise the display of analysis report summary Long reports: used to customise the rendering of the raw report of a given analyzer report Starting from TheHive4, short reports have been removed, and TheHive will display the analysis summary the same way for all analyzers: display a tag using taxonomies and level color. List analyzer templates # The management page is accessible from the header menu through the Admin > Analyzer templates menu and required a use with the manageAnalyzerTemplate permission (refer to Profiles and permissions ). Note that analyzer templates are global and common to all the organisations. Analyzer templates are still customisable via the UI and can also be imported. Import analyzer templates # TheHive Project provides a set of analyzer templates (we use the same report-templates.zip archive for backward compatibility reasons). The template archive is available at https://download.thehive-project.org/report-templates.zip . To import the zip file, click on the Import templates , this opens the import dialog. Drop the zip files or click to select it from your storage and finally click Yes, import template archive .","title":"Manage analyzer template"},{"location":"thehive/user-guides/administrators/analyzer-templates/#manage-analyzer-template","text":"Before TheHive4, we used to call them Report templates and we allowed two types of templates: Short reports: used to customise the display of analysis report summary Long reports: used to customise the rendering of the raw report of a given analyzer report Starting from TheHive4, short reports have been removed, and TheHive will display the analysis summary the same way for all analyzers: display a tag using taxonomies and level color.","title":"Manage analyzer template"},{"location":"thehive/user-guides/administrators/analyzer-templates/#list-analyzer-templates","text":"The management page is accessible from the header menu through the Admin > Analyzer templates menu and required a use with the manageAnalyzerTemplate permission (refer to Profiles and permissions ). Note that analyzer templates are global and common to all the organisations. Analyzer templates are still customisable via the UI and can also be imported.","title":"List analyzer templates"},{"location":"thehive/user-guides/administrators/analyzer-templates/#import-analyzer-templates","text":"TheHive Project provides a set of analyzer templates (we use the same report-templates.zip archive for backward compatibility reasons). The template archive is available at https://download.thehive-project.org/report-templates.zip . To import the zip file, click on the Import templates , this opens the import dialog. Drop the zip files or click to select it from your storage and finally click Yes, import template archive .","title":"Import analyzer templates"},{"location":"thehive/user-guides/administrators/custom-fields/","text":"Manage custom fields # In TheHive 4, Metrics have been removed. Why? Because metrics are simply, numeric custom fields. To manage Custom fields you need to login as an \"admin\" user (Member of the \"admin\" organisation) that has a profile including the manageCustomField permission (refer to Profiles and permissions for detailed information). The default \"admin\" user has that permission. \u26a0\ufe0f Note Custom fields are global to all the organisation. When installing TheHive, the list of custom fields is initially empty, administrators have to populate it. To create a custom field, click on the \"Add custom field\" button that opens a dialog: You need to set: a display name a name (automatically pre-filled by the UI based on the display name) a description a type: on of string , intger , booleen , date and float (new type added by TheHive 4) possible values (not available for date and boolean fields) wether the field is mandatory or not (will be prompted when you close a Case without setting its value) Once the custom field is created, you can edit its details or delete it: Only unused custom fields can be removed:","title":"Manage custom fields"},{"location":"thehive/user-guides/administrators/custom-fields/#manage-custom-fields","text":"In TheHive 4, Metrics have been removed. Why? Because metrics are simply, numeric custom fields. To manage Custom fields you need to login as an \"admin\" user (Member of the \"admin\" organisation) that has a profile including the manageCustomField permission (refer to Profiles and permissions for detailed information). The default \"admin\" user has that permission. \u26a0\ufe0f Note Custom fields are global to all the organisation. When installing TheHive, the list of custom fields is initially empty, administrators have to populate it. To create a custom field, click on the \"Add custom field\" button that opens a dialog: You need to set: a display name a name (automatically pre-filled by the UI based on the display name) a description a type: on of string , intger , booleen , date and float (new type added by TheHive 4) possible values (not available for date and boolean fields) wether the field is mandatory or not (will be prompted when you close a Case without setting its value) Once the custom field is created, you can edit its details or delete it: Only unused custom fields can be removed:","title":"Manage custom fields"},{"location":"thehive/user-guides/administrators/observable-types/","text":"Manage observable types # In TheHive4, we have big plans for observable types, since we plan to support observable templates insteand of a simple string value. But this feature is planned for the future. In TheHive 4.0 observable datatype are common to all the organisation, and manageable by administrators (members of the \"admin\" organisation). The management page is accessible from the header menu through the Admin > Observable types menu and required a use with the manageObservableTemplate permission (refer to Profiles and permissions ).","title":"Manage observable types"},{"location":"thehive/user-guides/administrators/observable-types/#manage-observable-types","text":"In TheHive4, we have big plans for observable types, since we plan to support observable templates insteand of a simple string value. But this feature is planned for the future. In TheHive 4.0 observable datatype are common to all the organisation, and manageable by administrators (members of the \"admin\" organisation). The management page is accessible from the header menu through the Admin > Observable types menu and required a use with the manageObservableTemplate permission (refer to Profiles and permissions ).","title":"Manage observable types"},{"location":"thehive/user-guides/administrators/organisations/","text":"Organisations # An organisation can't be deleted To create an organisation , clic on the New Organisation button in Admin > Organisations : Provide an organisation name and a description then clic Save :","title":"Organisations"},{"location":"thehive/user-guides/administrators/organisations/#organisations","text":"An organisation can't be deleted To create an organisation , clic on the New Organisation button in Admin > Organisations : Provide an organisation name and a description then clic Save :","title":"Organisations"},{"location":"thehive/user-guides/administrators/plateform-status/","text":"Plateform status # With the database update and the new indexing engine, a health status page has been introduced. This page not only displays health status for indexes, but also for global data in the database. Indeed, when an element is created, no duplicate should exist, a control is then processed. This is sumarised in the Data health status part. Accessing to this page requires to be admin of the plateform, or at least, have managePlateform permission. Health status page # This plage can be accessed in the Admin organisation view. Open the Admin menu and click on Plateform status Note When opening the page, the indexes status can take a while. with: List of indexes and their health status. Database objects number and Index objects number should be equal for a good health status When the status is Error , proceed to reindex List of data types health status in the database and their duplicate state Process for a duplicate check on a specific data type if status is XXX warning XXX Quand un \u00e9l\u00e9ment est cr\u00e9\u00e9 et qu'il ne doit pas y avoir de doublon (caseNumber, alert type+source+sourceRef, customField, ...), un contr\u00f4le est r\u00e9alis\u00e9 (duplicationCheck). Le r\u00e9sultat de ces contr\u00f4les sont dans la cl\u00e9 duplicateStats avec les champs : - last pour le r\u00e9sultat du dernier check (avec le nombre de doublon et la dur\u00e9e du check en milliseconds), - lastDate pour la date du dernier check - global est l'aggr\u00e9gation de tous les checks depuis le lancement de TH (avec le nombre d'iterations) Les checks sont d\u00e9clench\u00e9s de fa\u00e7on \u00e0 limiter le nombre d'iterations quand on fait plusieurs ajouts dans un courte p\u00e9riode. needCheck indique qu'un check est en attente et duplicateTimer que je check est programm\u00e9. En plus des contr\u00f4les de doublon, il y a une multitude d'autres contr\u00f4les (un share doit \u00eatre attach\u00e9 \u00e0 une seule orga et un seule case, une alerte ne peut \u00eatre import\u00e9 qu'une fois, ...). Ces checks sont r\u00e9alis\u00e9s toutes les 6 heures par d\u00e9faut. On retrouve le r\u00e9sultat de ces checks dans globalStats avec la m\u00eame logique last, lastDate et global. Par contre, les cl\u00e9s ne sont pas duplicate mais un ensemble de valeurs propres \u00e0 chaque type de contr\u00f4le (orphan, extraOrganisation, nonExistentOrganisation, missingOrganisation, ...). Chaque valeur me permettent d'identifier la situation recontr\u00e9e.","title":"Plateform status"},{"location":"thehive/user-guides/administrators/plateform-status/#plateform-status","text":"With the database update and the new indexing engine, a health status page has been introduced. This page not only displays health status for indexes, but also for global data in the database. Indeed, when an element is created, no duplicate should exist, a control is then processed. This is sumarised in the Data health status part. Accessing to this page requires to be admin of the plateform, or at least, have managePlateform permission.","title":"Plateform status"},{"location":"thehive/user-guides/administrators/plateform-status/#health-status-page","text":"This plage can be accessed in the Admin organisation view. Open the Admin menu and click on Plateform status Note When opening the page, the indexes status can take a while. with: List of indexes and their health status. Database objects number and Index objects number should be equal for a good health status When the status is Error , proceed to reindex List of data types health status in the database and their duplicate state Process for a duplicate check on a specific data type if status is XXX warning XXX Quand un \u00e9l\u00e9ment est cr\u00e9\u00e9 et qu'il ne doit pas y avoir de doublon (caseNumber, alert type+source+sourceRef, customField, ...), un contr\u00f4le est r\u00e9alis\u00e9 (duplicationCheck). Le r\u00e9sultat de ces contr\u00f4les sont dans la cl\u00e9 duplicateStats avec les champs : - last pour le r\u00e9sultat du dernier check (avec le nombre de doublon et la dur\u00e9e du check en milliseconds), - lastDate pour la date du dernier check - global est l'aggr\u00e9gation de tous les checks depuis le lancement de TH (avec le nombre d'iterations) Les checks sont d\u00e9clench\u00e9s de fa\u00e7on \u00e0 limiter le nombre d'iterations quand on fait plusieurs ajouts dans un courte p\u00e9riode. needCheck indique qu'un check est en attente et duplicateTimer que je check est programm\u00e9. En plus des contr\u00f4les de doublon, il y a une multitude d'autres contr\u00f4les (un share doit \u00eatre attach\u00e9 \u00e0 une seule orga et un seule case, une alerte ne peut \u00eatre import\u00e9 qu'une fois, ...). Ces checks sont r\u00e9alis\u00e9s toutes les 6 heures par d\u00e9faut. On retrouve le r\u00e9sultat de ces checks dans globalStats avec la m\u00eame logique last, lastDate et global. Par contre, les cl\u00e9s ne sont pas duplicate mais un ensemble de valeurs propres \u00e0 chaque type de contr\u00f4le (orphan, extraOrganisation, nonExistentOrganisation, missingOrganisation, ...). Chaque valeur me permettent d'identifier la situation recontr\u00e9e.","title":"Health status page"},{"location":"thehive/user-guides/administrators/profiles/","text":"User Profiles management # User profiles is a new concept introduced by TheHive4 and coming from the support of role based access control aka RBAC. In TheHive4, users will be assigned a Profile , within an Organisation . A Profile is composed by a set of predefined permissions. Permissions # A Profile is a set of permissions attached to a User and an Organisation . It defines what the user can do on an object hold by the organisation. TheHive has a finite list of permissions: manageOrganisation (1) : the user can create , update an organisation manageConfig (1): the user can update configuration manageProfile (1): the user can create , update and delete profiles manageTag (1): the user can create , update and delete tags manageCustomField (1): the user can create , update and delete custom fields manageCase : the user can create , update and delete cases manageObservable : the user can create , update and delete observables manageAlert : the user can create , update and import alerts manageUser : the user can create , update and delete users manageCaseTemplate : the user can create , update and delete case template manageTask : the user can create , update and delete tasks manageShare : the user can share case, task and observable with other organisation manageAnalyse (2): the user can execute analyse manageAction (2): the user can execute actions manageAnalyzerTemplate (2): the user can create , update and delete analyzer template (previously named report template) (1) Organisations, configuration, profiles and tags are global objects. The related permissions are effective only on \u201cadmin\u201d organisation. (2) Actions, analysis and template is available only if Cortex connector is enabled Note Read information doesn\u2019t require specific permission. By default, users in an organisation can see all data shared with that organisation (cf. shares, discussed in Organisations,Users and sharing ). Profiles # We distinguish two types of profiles: Administration Profiles Organisation Profiles The management page is accessible from the header menu through the Admin > Profiles menu and required a use with the manageProfile permission (refer to the section above). TheHive comes with default profiles but they can be updated and removed (if not used). New profiles can be created. Once the New Profile button is clicked, a dialog is opened asking for the profile type, a name for the profile and a selection of permissions. Multiple selection can be made using CTRL+click. If it is used, a profile can\u2019t be remove but can be updated. Default profiles are: admin : can manage all global objects and users. Can\u2019t create case. analyst : can manage cases and other related objects (observables, tasks, \u2026), including shring them org-admin : all permissions except those related to global objects read-only : no permission","title":"User Profiles management"},{"location":"thehive/user-guides/administrators/profiles/#user-profiles-management","text":"User profiles is a new concept introduced by TheHive4 and coming from the support of role based access control aka RBAC. In TheHive4, users will be assigned a Profile , within an Organisation . A Profile is composed by a set of predefined permissions.","title":"User Profiles management"},{"location":"thehive/user-guides/administrators/profiles/#permissions","text":"A Profile is a set of permissions attached to a User and an Organisation . It defines what the user can do on an object hold by the organisation. TheHive has a finite list of permissions: manageOrganisation (1) : the user can create , update an organisation manageConfig (1): the user can update configuration manageProfile (1): the user can create , update and delete profiles manageTag (1): the user can create , update and delete tags manageCustomField (1): the user can create , update and delete custom fields manageCase : the user can create , update and delete cases manageObservable : the user can create , update and delete observables manageAlert : the user can create , update and import alerts manageUser : the user can create , update and delete users manageCaseTemplate : the user can create , update and delete case template manageTask : the user can create , update and delete tasks manageShare : the user can share case, task and observable with other organisation manageAnalyse (2): the user can execute analyse manageAction (2): the user can execute actions manageAnalyzerTemplate (2): the user can create , update and delete analyzer template (previously named report template) (1) Organisations, configuration, profiles and tags are global objects. The related permissions are effective only on \u201cadmin\u201d organisation. (2) Actions, analysis and template is available only if Cortex connector is enabled Note Read information doesn\u2019t require specific permission. By default, users in an organisation can see all data shared with that organisation (cf. shares, discussed in Organisations,Users and sharing ).","title":"Permissions"},{"location":"thehive/user-guides/administrators/profiles/#profiles","text":"We distinguish two types of profiles: Administration Profiles Organisation Profiles The management page is accessible from the header menu through the Admin > Profiles menu and required a use with the manageProfile permission (refer to the section above). TheHive comes with default profiles but they can be updated and removed (if not used). New profiles can be created. Once the New Profile button is clicked, a dialog is opened asking for the profile type, a name for the profile and a selection of permissions. Multiple selection can be made using CTRL+click. If it is used, a profile can\u2019t be remove but can be updated. Default profiles are: admin : can manage all global objects and users. Can\u2019t create case. analyst : can manage cases and other related objects (observables, tasks, \u2026), including shring them org-admin : all permissions except those related to global objects read-only : no permission","title":"Profiles"},{"location":"thehive/user-guides/administrators/tactics-techniques-procedures/","text":"Tactics, Techniques & Procedures # TheHive 4.1.0+ is required to use TTPs Starting with version 4.1.0, TheHive allows to bind Cases to TTPs (Tactics, Techniques & Procedures) . The MITRE ATT&CK framework has been chosen to define these TTPs. Import MITRE ATT&CK patterns # To access and import MITRE ATT&CK patterns definition, beeing admin or at least have the role managePattern is required. In the admin organisation, open the ATT&CK Patterns menu Click on Import MITRE ATT&CK Patterns and select the appropriate file Ensure patterns are imported Tip A direct link to the current zip archive of MITRE ATT&CK patterns let you download it quickly from the official github page. Use MITRE ATT&CK # Refer to this page to learn how to add TTPs ( Tactics, Techniques and Procedures ) to a Case.","title":"Tactics, Techniques & Procedures"},{"location":"thehive/user-guides/administrators/tactics-techniques-procedures/#tactics-techniques-procedures","text":"TheHive 4.1.0+ is required to use TTPs Starting with version 4.1.0, TheHive allows to bind Cases to TTPs (Tactics, Techniques & Procedures) . The MITRE ATT&CK framework has been chosen to define these TTPs.","title":"Tactics, Techniques &amp; Procedures"},{"location":"thehive/user-guides/administrators/tactics-techniques-procedures/#import-mitre-attck-patterns","text":"To access and import MITRE ATT&CK patterns definition, beeing admin or at least have the role managePattern is required. In the admin organisation, open the ATT&CK Patterns menu Click on Import MITRE ATT&CK Patterns and select the appropriate file Ensure patterns are imported Tip A direct link to the current zip archive of MITRE ATT&CK patterns let you download it quickly from the official github page.","title":"Import MITRE ATT&amp;CK patterns"},{"location":"thehive/user-guides/administrators/tactics-techniques-procedures/#use-mitre-attck","text":"Refer to this page to learn how to add TTPs ( Tactics, Techniques and Procedures ) to a Case.","title":"Use MITRE ATT&amp;CK"},{"location":"thehive/user-guides/administrators/tags-and-taxonomies/","text":"Taxonomies and Tags # TheHive 4.1.0+ is required to use Taxonomies TheHive 4.1.0 introduces the support of Taxonomies as it is defined and published by MISP . These set of classification libraries can be used in THeHive to tag Cases , Observables and Alerts . Tip Not only MISP-Taxonomies are supported by TheHive, but you can also build your own by: Following the IETF draft https://tools.ietf.org/id/draft-dulaunoy-misp-taxonomy-format-07.html Draw inspiration from an existing definition file :-) By default, TheHive does not contain any taxonomy. Import taxonomies # To access and import taxonomies, beeing admin or at least have the role manageTaxonomy is required. In the admin organisation, open the Taxonomies menu Click on Import taxonomies and select the file containing the libraries Tip A direct link to the current zip archive of MISP-Taxonomies let you download it quickly. Enable interesting taxonomies # Select the libraries you would like your user be able to use in Case or Observables , and enable it . Your browser does not support the video tag. Warning Enabling a taxonomy means all users of all Organisations can use one or more included tags in a Case or Observable . Tags from taxonomies versus free text tags # In the UI, users can add free text tags, and also choose to add a tag from a library in a dedicated view. Free text tags are managed at the Organisation level by users with orgadmin profile, or at least manageTag permission. Refer to appropriate pages to learn about how to manage custom tags , and how to use tags in TheHive. Info If a tag is imported with an Alert or created with the API, TheHive tries to dissect it as a machinetag . It tries to identify a namespace, a predicate and an optional value. If successful, and if an associated taxonomy exists and is enabled , the tag is linked to the library ; if not, it is considered as a free text tag.","title":"Taxonomies and Tags"},{"location":"thehive/user-guides/administrators/tags-and-taxonomies/#taxonomies-and-tags","text":"TheHive 4.1.0+ is required to use Taxonomies TheHive 4.1.0 introduces the support of Taxonomies as it is defined and published by MISP . These set of classification libraries can be used in THeHive to tag Cases , Observables and Alerts . Tip Not only MISP-Taxonomies are supported by TheHive, but you can also build your own by: Following the IETF draft https://tools.ietf.org/id/draft-dulaunoy-misp-taxonomy-format-07.html Draw inspiration from an existing definition file :-) By default, TheHive does not contain any taxonomy.","title":"Taxonomies and Tags"},{"location":"thehive/user-guides/administrators/tags-and-taxonomies/#import-taxonomies","text":"To access and import taxonomies, beeing admin or at least have the role manageTaxonomy is required. In the admin organisation, open the Taxonomies menu Click on Import taxonomies and select the file containing the libraries Tip A direct link to the current zip archive of MISP-Taxonomies let you download it quickly.","title":"Import taxonomies"},{"location":"thehive/user-guides/administrators/tags-and-taxonomies/#enable-interesting-taxonomies","text":"Select the libraries you would like your user be able to use in Case or Observables , and enable it . Your browser does not support the video tag. Warning Enabling a taxonomy means all users of all Organisations can use one or more included tags in a Case or Observable .","title":"Enable interesting taxonomies"},{"location":"thehive/user-guides/administrators/tags-and-taxonomies/#tags-from-taxonomies-versus-free-text-tags","text":"In the UI, users can add free text tags, and also choose to add a tag from a library in a dedicated view. Free text tags are managed at the Organisation level by users with orgadmin profile, or at least manageTag permission. Refer to appropriate pages to learn about how to manage custom tags , and how to use tags in TheHive. Info If a tag is imported with an Alert or created with the API, TheHive tries to dissect it as a machinetag . It tries to identify a namespace, a predicate and an optional value. If successful, and if an associated taxonomy exists and is enabled , the tag is linked to the library ; if not, it is considered as a free text tag.","title":"Tags from taxonomies versus free text tags"},{"location":"thehive/user-guides/analysts/close-case/","text":"Close Cases # Closing a case is one of the basic TheHive functionnalities. It indicates the investigations and responses on this incident are over. To close a case , you must have the manageCase permission (refer to Profiles and permissions ) You can find the Close button on the case banner: Closing a case requires that all tasks contained in the case are closed. If you didn't closed the tasks before, a pop-up will suggest you to close them all. Finally, provide the necessary details to close the case: Status: If the case was a True Positive , a False Positive , if this is still Indeterminate or Other (not an incident) If True positive: Was there an impact (yes/no) Summary: a summary of the incident Once the details provided, clic on Close case.","title":"Close Cases"},{"location":"thehive/user-guides/analysts/close-case/#close-cases","text":"Closing a case is one of the basic TheHive functionnalities. It indicates the investigations and responses on this incident are over. To close a case , you must have the manageCase permission (refer to Profiles and permissions ) You can find the Close button on the case banner: Closing a case requires that all tasks contained in the case are closed. If you didn't closed the tasks before, a pop-up will suggest you to close them all. Finally, provide the necessary details to close the case: Status: If the case was a True Positive , a False Positive , if this is still Indeterminate or Other (not an incident) If True positive: Was there an impact (yes/no) Summary: a summary of the incident Once the details provided, clic on Close case.","title":"Close Cases"},{"location":"thehive/user-guides/analysts/create-alerts/","text":"Create Alerts # In TheHive4, creating an alert is possible only through the API. (refer to Create Alerts ) To create an alert, the account must have manageAlert permission. (refer to Profiles and permissions )","title":"Create Alerts"},{"location":"thehive/user-guides/analysts/create-alerts/#create-alerts","text":"In TheHive4, creating an alert is possible only through the API. (refer to Create Alerts ) To create an alert, the account must have manageAlert permission. (refer to Profiles and permissions )","title":"Create Alerts"},{"location":"thehive/user-guides/analysts/create-case/","text":"Create Cases # Creating a case is one of the basic TheHive functionnalities. To create a case , you must have the manageCase permission (refer to Profiles and permissions ) In TheHive banner, clic the button New case : Then you can either chose to use a Case template , or start it from scratch using Empty case (this option may be unavailable following your organisation configuration): Once you chose your template, fill the case details: Title * Date ( startDate ) * Severity * TLP/PAP * Tags Description * Case tasks Information annoted with a '*' are mandatory information. Once case details filled, finally clic on Create case button.","title":"Create Cases"},{"location":"thehive/user-guides/analysts/create-case/#create-cases","text":"Creating a case is one of the basic TheHive functionnalities. To create a case , you must have the manageCase permission (refer to Profiles and permissions ) In TheHive banner, clic the button New case : Then you can either chose to use a Case template , or start it from scratch using Empty case (this option may be unavailable following your organisation configuration): Once you chose your template, fill the case details: Title * Date ( startDate ) * Severity * TLP/PAP * Tags Description * Case tasks Information annoted with a '*' are mandatory information. Once case details filled, finally clic on Create case button.","title":"Create Cases"},{"location":"thehive/user-guides/analysts/create-observables/","text":"Create Case Observables # In a TheHive case , you can declare observables . To create an observable , open the Observables list ( Case > Observables ). you must have the manageCase permission (refer to Profiles and permissions ) You will find the Add observable button under the Observables tab: In the pop-up, you are invited to fill the observable (s) details: Type *: The observable dataType (eg: ip, hash, domain, ...) Value *: Your observable value (eg: 8.8.8.8) One observable per line: Create one observable per line inserted in value field. One single multiline observable: Create one observable , no matter the number of lines (useful for long URLs for example). TLP *: Define here the way the information should be shared. Is IOC: Check it if this observable is considered as Indicator of Compromission. Has been sighted: Has this observable been sighted on your information system. Ignore for similarity: Do not correlate this observable with other similar observables . Tags **: Tag your observable with insightful information. Description **: Description of the observable . Details annoted with a ' ' are mandatory. Detail annoted with ' *' mean at least. Finally clic on Create Observable(s)","title":"Create Case Observables"},{"location":"thehive/user-guides/analysts/create-observables/#create-case-observables","text":"In a TheHive case , you can declare observables . To create an observable , open the Observables list ( Case > Observables ). you must have the manageCase permission (refer to Profiles and permissions ) You will find the Add observable button under the Observables tab: In the pop-up, you are invited to fill the observable (s) details: Type *: The observable dataType (eg: ip, hash, domain, ...) Value *: Your observable value (eg: 8.8.8.8) One observable per line: Create one observable per line inserted in value field. One single multiline observable: Create one observable , no matter the number of lines (useful for long URLs for example). TLP *: Define here the way the information should be shared. Is IOC: Check it if this observable is considered as Indicator of Compromission. Has been sighted: Has this observable been sighted on your information system. Ignore for similarity: Do not correlate this observable with other similar observables . Tags **: Tag your observable with insightful information. Description **: Description of the observable . Details annoted with a ' ' are mandatory. Detail annoted with ' *' mean at least. Finally clic on Create Observable(s)","title":"Create Case Observables"},{"location":"thehive/user-guides/analysts/create-tasks/","text":"Manage Case Tasks # In a TheHive case , you can find the tab tasks . Tasks List # You can consult cases Task list ( Case > Tasks ). you must have the manageCase permission (refer to Profiles and permissions ) The list contains the following information: Group: The task group membership Task: The task title Date: the startDate of the task Assignee: The user assigned to the task Actions: Delete, start/close or trigger a responder on the task Create a task # You can create a task in a case . you must have the manageCase permission (refer to Profiles and permissions ) Open your Task list and clic Create task button. On the top of the Task list a ribbon will appear, inviting you to fill the Task group and the Task title Task information # Open your task to retrieve it's information ( Case > Tasks > Task ) Task Actions # Open your task to find the possible actions ( Case > Tasks > Task ) Task actions buttons are on the top-right of a Task page : You can trigger the following actions on a task : Sharing: Ability to share with Linked organisations the task Require Action: Declare that an action is required on this task Flag: Put a flag on the task Close: Close the task Responders: Trigger a responder on the task Basic information # Open your task to retrieve these information ( Case > Tasks > Task ) A task Basic information contains the following elements: Title of the task * Group of tasks * Assignee of the task * startDate of the task * Duration of the task Status of the task All information annoted with a '*' can be modified by clicking the pen when hovering the information. Task description # Open your task to retrieve this information ( Case > Tasks > Task ) Under the Basic information , you can find the description field. It's a free text field, markdown formatted. Task logs # Open your task to retrieve the task logs ( Case > Tasks > Task ) The Add new task log allows you to create a Task log . Task logs are markdown formatted text. You can also attach a file to the log. Task logs possible actions are: Create a task log Modify a task log Delete a task log Trigger a responder on the task log","title":"Manage Case Tasks"},{"location":"thehive/user-guides/analysts/create-tasks/#manage-case-tasks","text":"In a TheHive case , you can find the tab tasks .","title":"Manage Case Tasks"},{"location":"thehive/user-guides/analysts/create-tasks/#tasks-list","text":"You can consult cases Task list ( Case > Tasks ). you must have the manageCase permission (refer to Profiles and permissions ) The list contains the following information: Group: The task group membership Task: The task title Date: the startDate of the task Assignee: The user assigned to the task Actions: Delete, start/close or trigger a responder on the task","title":"Tasks List"},{"location":"thehive/user-guides/analysts/create-tasks/#create-a-task","text":"You can create a task in a case . you must have the manageCase permission (refer to Profiles and permissions ) Open your Task list and clic Create task button. On the top of the Task list a ribbon will appear, inviting you to fill the Task group and the Task title","title":"Create a task"},{"location":"thehive/user-guides/analysts/create-tasks/#task-information","text":"Open your task to retrieve it's information ( Case > Tasks > Task )","title":"Task information"},{"location":"thehive/user-guides/analysts/create-tasks/#task-actions","text":"Open your task to find the possible actions ( Case > Tasks > Task ) Task actions buttons are on the top-right of a Task page : You can trigger the following actions on a task : Sharing: Ability to share with Linked organisations the task Require Action: Declare that an action is required on this task Flag: Put a flag on the task Close: Close the task Responders: Trigger a responder on the task","title":"Task Actions"},{"location":"thehive/user-guides/analysts/create-tasks/#basic-information","text":"Open your task to retrieve these information ( Case > Tasks > Task ) A task Basic information contains the following elements: Title of the task * Group of tasks * Assignee of the task * startDate of the task * Duration of the task Status of the task All information annoted with a '*' can be modified by clicking the pen when hovering the information.","title":"Basic information"},{"location":"thehive/user-guides/analysts/create-tasks/#task-description","text":"Open your task to retrieve this information ( Case > Tasks > Task ) Under the Basic information , you can find the description field. It's a free text field, markdown formatted.","title":"Task description"},{"location":"thehive/user-guides/analysts/create-tasks/#task-logs","text":"Open your task to retrieve the task logs ( Case > Tasks > Task ) The Add new task log allows you to create a Task log . Task logs are markdown formatted text. You can also attach a file to the log. Task logs possible actions are: Create a task log Modify a task log Delete a task log Trigger a responder on the task log","title":"Task logs"},{"location":"thehive/user-guides/analysts/export-case/","text":"Export Cases to MISP # TheHive4 has the capability to export a case to a MISP instance. This functionnality allows you to easily share your incident and findings with communities. To export a case , you must have the manageCase permission (refer to Profiles and permissions ) You also must have a MISP instance connected to your TheHive (refer to MISP Connector ) Trigger the Export button on a case action ribbon ( Case > Export ): In the MISP export pop-up, you can chose the MISP instance(s) where you want to export your case . Clic the Export button to send your case to the MISP instance.","title":"Export Cases to MISP"},{"location":"thehive/user-guides/analysts/export-case/#export-cases-to-misp","text":"TheHive4 has the capability to export a case to a MISP instance. This functionnality allows you to easily share your incident and findings with communities. To export a case , you must have the manageCase permission (refer to Profiles and permissions ) You also must have a MISP instance connected to your TheHive (refer to MISP Connector ) Trigger the Export button on a case action ribbon ( Case > Export ): In the MISP export pop-up, you can chose the MISP instance(s) where you want to export your case . Clic the Export button to send your case to the MISP instance.","title":"Export Cases to MISP"},{"location":"thehive/user-guides/analysts/run-analyzers/","text":"Run Analyzers # In TheHive4 you can run analyzers on observables . To run an analyzer , you must have the manageAnalyse permission (refer to Profiles and permissions ) From an observable page # You can trigger an analyzer on a single observable from it's page ( Case > Observables > Observable ). In the Analysis section, you'll find every analyzers available for your organisation and compatible with the observable dataType : On the right side of the Analysis section, you can trigger the analyzers of your choice by clicking on the fire button, or run them all via the button Run all : From the observables list # You can also trigger one or more analyzers on one or more observables from the Observables list ( Case > Observables ) On the left side of the Observables list , you have checkboxes to select which observables to act on. You can even select all of them using the checkbox that is at the very top of the Observables list : Once selected, clic on the Selected observables menu, and chose Run analyzers : Finally select the desired analyzers to trigger and clic Run selected analyzers : Consult analyzers report # Once the analyzer has been triggered and the job terminated, you can consult the Job report directly within TheHive. Short report # In the Observables list ( Case > Observables ), you have access to a short report : Long report # On the Observable page ( Case > Observables > Observable ), in the Analysis table, you can consult a HTML formatted long report by clicking on the analysis link:","title":"Run Analyzers"},{"location":"thehive/user-guides/analysts/run-analyzers/#run-analyzers","text":"In TheHive4 you can run analyzers on observables . To run an analyzer , you must have the manageAnalyse permission (refer to Profiles and permissions )","title":"Run Analyzers"},{"location":"thehive/user-guides/analysts/run-analyzers/#from-an-observable-page","text":"You can trigger an analyzer on a single observable from it's page ( Case > Observables > Observable ). In the Analysis section, you'll find every analyzers available for your organisation and compatible with the observable dataType : On the right side of the Analysis section, you can trigger the analyzers of your choice by clicking on the fire button, or run them all via the button Run all :","title":"From an observable page"},{"location":"thehive/user-guides/analysts/run-analyzers/#from-the-observables-list","text":"You can also trigger one or more analyzers on one or more observables from the Observables list ( Case > Observables ) On the left side of the Observables list , you have checkboxes to select which observables to act on. You can even select all of them using the checkbox that is at the very top of the Observables list : Once selected, clic on the Selected observables menu, and chose Run analyzers : Finally select the desired analyzers to trigger and clic Run selected analyzers :","title":"From the observables list"},{"location":"thehive/user-guides/analysts/run-analyzers/#consult-analyzers-report","text":"Once the analyzer has been triggered and the job terminated, you can consult the Job report directly within TheHive.","title":"Consult analyzers report"},{"location":"thehive/user-guides/analysts/run-analyzers/#short-report","text":"In the Observables list ( Case > Observables ), you have access to a short report :","title":"Short report"},{"location":"thehive/user-guides/analysts/run-analyzers/#long-report","text":"On the Observable page ( Case > Observables > Observable ), in the Analysis table, you can consult a HTML formatted long report by clicking on the analysis link:","title":"Long report"},{"location":"thehive/user-guides/analysts/run-responders/","text":"Run Responders # In TheHive4, you can run responders on 4 type of objects: A case A task A task log An observable To run a responder , you must have the manageAction permission (refer to Profiles and permissions ) A report will be generated and provided to you. From a case # You can trigger a responder from a case . On the case Action ribbon , trigger the Responders button From a task # You can trigger a responder from a task ( Case > Tasks > Task ) On the task Action ribbon , trigger the Responders button. From a task log # You can trigger a responder from a task log ( Case > Tasks > Task > Task log ) On the task log Action ribbon , trigger the Responders button. From an observable # You can trigger a responder from an observable ( Case > Observables > Observable ) On the observable Action ribbon , trigger the Responders button. View responder report # responders provides you a report that can have two status: Success Failure The report is visible in the object where you triggered it ( case , observable , task or task log ) In addition of the status, a text report is provided allowing you to know what happens:","title":"Run Responders"},{"location":"thehive/user-guides/analysts/run-responders/#run-responders","text":"In TheHive4, you can run responders on 4 type of objects: A case A task A task log An observable To run a responder , you must have the manageAction permission (refer to Profiles and permissions ) A report will be generated and provided to you.","title":"Run Responders"},{"location":"thehive/user-guides/analysts/run-responders/#from-a-case","text":"You can trigger a responder from a case . On the case Action ribbon , trigger the Responders button","title":"From a case"},{"location":"thehive/user-guides/analysts/run-responders/#from-a-task","text":"You can trigger a responder from a task ( Case > Tasks > Task ) On the task Action ribbon , trigger the Responders button.","title":"From a task"},{"location":"thehive/user-guides/analysts/run-responders/#from-a-task-log","text":"You can trigger a responder from a task log ( Case > Tasks > Task > Task log ) On the task log Action ribbon , trigger the Responders button.","title":"From a task log"},{"location":"thehive/user-guides/analysts/run-responders/#from-an-observable","text":"You can trigger a responder from an observable ( Case > Observables > Observable ) On the observable Action ribbon , trigger the Responders button.","title":"From an observable"},{"location":"thehive/user-guides/analysts/run-responders/#view-responder-report","text":"responders provides you a report that can have two status: Success Failure The report is visible in the object where you triggered it ( case , observable , task or task log ) In addition of the status, a text report is provided allowing you to know what happens:","title":"View responder report"},{"location":"thehive/user-guides/analysts/sharing/","text":"Sharing Cases, Tasks and Observables # In TheHive4, you can share 3 type of objects: A case A task An observable To share an object, you must have the manageShare permission (refer to Profiles and permissions ) You can share only with organisations that are linked to your organisation (refer to Organisations, Users and sharing ) Share a case # You can share your case by clicking the Sharing button in the case Action ribbon When you share a case , you have to chose: To which organisation(s) To which profile To share tasks or not To share observables or not Share a task # You can share a task (the case have to be shared too for this functionnality to be available) At the very bottom of a Task page ( Case > Observables > Observables ), in the section Task sharing , clic on Add share Then you can select to which organisation you will share the task : Share an observable # You can share an observable (the case have to be shared too for this functionnality to be available) At the very bottom of a Observable page ( Case > Observables > Observable ), in the section Sharing , clic on Add share Then you can select to which organisation you will share the observable : Delete a share # You can cancel the share of an object. For each object type, go to the Share list and trigger the Delete button in the Actions column:","title":"Sharing Cases, Tasks and Observables"},{"location":"thehive/user-guides/analysts/sharing/#sharing-cases-tasks-and-observables","text":"In TheHive4, you can share 3 type of objects: A case A task An observable To share an object, you must have the manageShare permission (refer to Profiles and permissions ) You can share only with organisations that are linked to your organisation (refer to Organisations, Users and sharing )","title":"Sharing Cases, Tasks and Observables"},{"location":"thehive/user-guides/analysts/sharing/#share-a-case","text":"You can share your case by clicking the Sharing button in the case Action ribbon When you share a case , you have to chose: To which organisation(s) To which profile To share tasks or not To share observables or not","title":"Share a case"},{"location":"thehive/user-guides/analysts/sharing/#share-a-task","text":"You can share a task (the case have to be shared too for this functionnality to be available) At the very bottom of a Task page ( Case > Observables > Observables ), in the section Task sharing , clic on Add share Then you can select to which organisation you will share the task :","title":"Share a task"},{"location":"thehive/user-guides/analysts/sharing/#share-an-observable","text":"You can share an observable (the case have to be shared too for this functionnality to be available) At the very bottom of a Observable page ( Case > Observables > Observable ), in the section Sharing , clic on Add share Then you can select to which organisation you will share the observable :","title":"Share an observable"},{"location":"thehive/user-guides/analysts/sharing/#delete-a-share","text":"You can cancel the share of an object. For each object type, go to the Share list and trigger the Delete button in the Actions column:","title":"Delete a share"},{"location":"thehive/user-guides/analysts/ttps/","text":"Tactics, Techniques and Procedures # In TheHive4 you can enrich your cases with TTPs. To manage a case TTPs , you must have the manageCase permission (refer to Profiles and permissions ) Add a TTP to a case # To add a TTP to a case , go to the TTPs list ( Case > TTPs ) then clic the Add TTP button: In the Add Tactic, Technique and Procedure pop-up, you can select: The occur date The Tactic The Technique (you can use filters on techniques) The Procedure (clic to Add procedure to open this free text field) Finally, clic on Add TTP in the bottom of the pop-up: Delete a TTP from a case # You can delete a TTP from a case . Go to the TTPs list ( Case > TTPs ), then clic on the Delete button in the Actions column:","title":"Tactics, Techniques and Procedures"},{"location":"thehive/user-guides/analysts/ttps/#tactics-techniques-and-procedures","text":"In TheHive4 you can enrich your cases with TTPs. To manage a case TTPs , you must have the manageCase permission (refer to Profiles and permissions )","title":"Tactics, Techniques and Procedures"},{"location":"thehive/user-guides/analysts/ttps/#add-a-ttp-to-a-case","text":"To add a TTP to a case , go to the TTPs list ( Case > TTPs ) then clic the Add TTP button: In the Add Tactic, Technique and Procedure pop-up, you can select: The occur date The Tactic The Technique (you can use filters on techniques) The Procedure (clic to Add procedure to open this free text field) Finally, clic on Add TTP in the bottom of the pop-up:","title":"Add a TTP to a case"},{"location":"thehive/user-guides/analysts/ttps/#delete-a-ttp-from-a-case","text":"You can delete a TTP from a case . Go to the TTPs list ( Case > TTPs ), then clic on the Delete button in the Actions column:","title":"Delete a TTP from a case"},{"location":"thehive/user-guides/analysts/user-settings/","text":"User settings configuration # Every TheHive user, has a set of settings that can be updated through the Settings menu located on the right hand side of the navigation bar This page allows the following operations: User settings configuration Update basic Info Update password Configure MFA Update basic Info # This section gives the user the ability to update the his/her name and upload an avatar image Update password # This section is hidden by default, the user needs to enable it, set the current password and the new one twice. Clicking Save button to submit the form Configure MFA # This section allows a user to enable 2FA authentication using a TOTP application (Google Authenticator, Authy, Microsoft Authenticator, 1password etc.) to scan the QR code or the code underneath it. The 2FA will generate A TOTP that the user should supply in the MFA Code area. If it is valid, 2FA will be activated. A Disable button allows the user to deactivate the 2FA settings. A user with 2FA activated, will be prompted to provide a TOTP during login process.","title":"User settings configuration"},{"location":"thehive/user-guides/analysts/user-settings/#user-settings-configuration","text":"Every TheHive user, has a set of settings that can be updated through the Settings menu located on the right hand side of the navigation bar This page allows the following operations: User settings configuration Update basic Info Update password Configure MFA","title":"User settings configuration"},{"location":"thehive/user-guides/analysts/user-settings/#update-basic-info","text":"This section gives the user the ability to update the his/her name and upload an avatar image","title":"Update basic Info"},{"location":"thehive/user-guides/analysts/user-settings/#update-password","text":"This section is hidden by default, the user needs to enable it, set the current password and the new one twice. Clicking Save button to submit the form","title":"Update password"},{"location":"thehive/user-guides/analysts/user-settings/#configure-mfa","text":"This section allows a user to enable 2FA authentication using a TOTP application (Google Authenticator, Authy, Microsoft Authenticator, 1password etc.) to scan the QR code or the code underneath it. The 2FA will generate A TOTP that the user should supply in the MFA Code area. If it is valid, 2FA will be activated. A Disable button allows the user to deactivate the 2FA settings. A user with 2FA activated, will be prompted to provide a TOTP during login process.","title":"Configure MFA"},{"location":"thehive/user-guides/organisation-managers/case-templates/","text":"Case Templates # Some cases may share the same structure ( customfields , tags , tasks , description , ...). Templates are here to automatically add tasks, description, metrics and custom fields while creating a new case. A user can choose to create an empty case or based on a registered template. List case templates # The management of the case templates is accessible through the menu Organisation > Case Templates . To manage them your profile must have the permission 'manageCaseTemplate' (refer to Profiles and permissions ). Create or upload template # Create a case template # In the case templates management page, clic the New template button ( Organisation > Case Templates > New Template ). In the case template you can set: Title prefix Severity TLP/PAP Tags Description Tasks Customfields Two fields are mandatory: Template name (should be unique) Description Import a case template # You can also import your case template using a file in JSON format by clicking on the Import template button ( Organisation > Case templates > Import template ) Edit a case template # To edit a case template, open the case template list and clic the edit button on the actions column ( Organisation > Case Templates > Edit ). Export a case template # To export a case template, open the case template list and clic the export button on the actions column ( Organisation > Case Templates > Export ). Delete a case template # To delete a case template, open the case template list and clic the export button on the actions column ( Organisation > Case Templates > Export ).","title":"Case Templates"},{"location":"thehive/user-guides/organisation-managers/case-templates/#case-templates","text":"Some cases may share the same structure ( customfields , tags , tasks , description , ...). Templates are here to automatically add tasks, description, metrics and custom fields while creating a new case. A user can choose to create an empty case or based on a registered template.","title":"Case Templates"},{"location":"thehive/user-guides/organisation-managers/case-templates/#list-case-templates","text":"The management of the case templates is accessible through the menu Organisation > Case Templates . To manage them your profile must have the permission 'manageCaseTemplate' (refer to Profiles and permissions ).","title":"List case templates"},{"location":"thehive/user-guides/organisation-managers/case-templates/#create-or-upload-template","text":"","title":"Create or upload template"},{"location":"thehive/user-guides/organisation-managers/case-templates/#create-a-case-template","text":"In the case templates management page, clic the New template button ( Organisation > Case Templates > New Template ). In the case template you can set: Title prefix Severity TLP/PAP Tags Description Tasks Customfields Two fields are mandatory: Template name (should be unique) Description","title":"Create a case template"},{"location":"thehive/user-guides/organisation-managers/case-templates/#import-a-case-template","text":"You can also import your case template using a file in JSON format by clicking on the Import template button ( Organisation > Case templates > Import template )","title":"Import a case template"},{"location":"thehive/user-guides/organisation-managers/case-templates/#edit-a-case-template","text":"To edit a case template, open the case template list and clic the edit button on the actions column ( Organisation > Case Templates > Edit ).","title":"Edit a case template"},{"location":"thehive/user-guides/organisation-managers/case-templates/#export-a-case-template","text":"To export a case template, open the case template list and clic the export button on the actions column ( Organisation > Case Templates > Export ).","title":"Export a case template"},{"location":"thehive/user-guides/organisation-managers/case-templates/#delete-a-case-template","text":"To delete a case template, open the case template list and clic the export button on the actions column ( Organisation > Case Templates > Export ).","title":"Delete a case template"},{"location":"thehive/user-guides/organisation-managers/custom-tags/","text":"Custom Tags # custom tags are tags manually created (out of libraries). You must have the permission manageTag on your profile to manage custom tags. (refer to Profiles and permissions ) List custom tags # You can find the list of your custom tags in Organization > Custom tags . The list contains the following information, for each tag : Number of cases tagged Number of alerts tagged Number of observables tagged Number of case templates containing the tag Modify a custom-tag border colour # You can modify your custom tags border colours. In the custom tags list ( Organization > Custom tags ), in the Colour column, clic on the square or colour code value to modify it. This will apply to all cases , alerts and observables that contains the tag . Delete a custom tag # You can also delete a custom tag. In the custom tags list ( Organization > Custom tags ), in the Actions column, clic on the delete button \u26a0\ufe0f Note Deleting a custom tag will delete the tag on each object containing it.","title":"Custom tags"},{"location":"thehive/user-guides/organisation-managers/custom-tags/#custom-tags","text":"custom tags are tags manually created (out of libraries). You must have the permission manageTag on your profile to manage custom tags. (refer to Profiles and permissions )","title":"Custom Tags"},{"location":"thehive/user-guides/organisation-managers/custom-tags/#list-custom-tags","text":"You can find the list of your custom tags in Organization > Custom tags . The list contains the following information, for each tag : Number of cases tagged Number of alerts tagged Number of observables tagged Number of case templates containing the tag","title":"List custom tags"},{"location":"thehive/user-guides/organisation-managers/custom-tags/#modify-a-custom-tag-border-colour","text":"You can modify your custom tags border colours. In the custom tags list ( Organization > Custom tags ), in the Colour column, clic on the square or colour code value to modify it. This will apply to all cases , alerts and observables that contains the tag .","title":"Modify a custom-tag border colour"},{"location":"thehive/user-guides/organisation-managers/custom-tags/#delete-a-custom-tag","text":"You can also delete a custom tag. In the custom tags list ( Organization > Custom tags ), in the Actions column, clic on the delete button \u26a0\ufe0f Note Deleting a custom tag will delete the tag on each object containing it.","title":"Delete a custom tag"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/","text":"Organisations, Users and sharing # User role, profile and permission # User # In TheHive, a user is a member of one or more organisations. One user has a profile for each organisation and can have different profiles for different organisations. For example: \u201c analyst \u201d in \u201c organisationA \u201d; and \u201c admin \u201d in \u201c organisationB \u201d; and \u201c read-only \u201d in \u201c organisationC \u201d. Organisations and sharing # TheHive comes with a default organisation named \"admin\" and is dedicated to users with administrator permissions of TheHive instance. This organisation is very specific so that it can manage global objects and cannot contain cases or any other related elements. By default, organisations can\u2019t see each other, and can't share with any. To do so, an organisation must be \"linked\" with another one. Only super administrators or users with manageOrganisation permissions can give the ability of a organisation to see an other one. This ability named \u201c link \u201d is unidirectional. Link with other organisations # To share a case with another organisation, a user must be able to see it: its organisation must be \"linked\" with the targeted organisation. Share and effective permissions # When a user creates a case, the case is linked to the user\u2019s organisation with the profile \u201corg-admin\u201d. It means that there is no restriction, the effective permissions are the permissions the user has in his organisation. If he decides to share that case with another organisation, he must choose the profile applied on that share. To exerce a action on a case, the related permission must be present in the user profile and in the case share. When you share a case, you can share its tasks or observables but it is not mandatory. Tasks (and observables) can be unitary shared. They can be shared only with organisations for which case is already shared. A case can be shared only once for a given organisation. Thus a case an its tasks/observables are shared with the same permissions for the same organisation.","title":"Organisations, Users and sharing"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#organisations-users-and-sharing","text":"","title":"Organisations, Users and sharing"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#user-role-profile-and-permission","text":"","title":"User role, profile and permission"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#user","text":"In TheHive, a user is a member of one or more organisations. One user has a profile for each organisation and can have different profiles for different organisations. For example: \u201c analyst \u201d in \u201c organisationA \u201d; and \u201c admin \u201d in \u201c organisationB \u201d; and \u201c read-only \u201d in \u201c organisationC \u201d.","title":"User"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#organisations-and-sharing","text":"TheHive comes with a default organisation named \"admin\" and is dedicated to users with administrator permissions of TheHive instance. This organisation is very specific so that it can manage global objects and cannot contain cases or any other related elements. By default, organisations can\u2019t see each other, and can't share with any. To do so, an organisation must be \"linked\" with another one. Only super administrators or users with manageOrganisation permissions can give the ability of a organisation to see an other one. This ability named \u201c link \u201d is unidirectional.","title":"Organisations and sharing"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#link-with-other-organisations","text":"To share a case with another organisation, a user must be able to see it: its organisation must be \"linked\" with the targeted organisation.","title":"Link with other organisations"},{"location":"thehive/user-guides/organisation-managers/organisations-users-sharing/#share-and-effective-permissions","text":"When a user creates a case, the case is linked to the user\u2019s organisation with the profile \u201corg-admin\u201d. It means that there is no restriction, the effective permissions are the permissions the user has in his organisation. If he decides to share that case with another organisation, he must choose the profile applied on that share. To exerce a action on a case, the related permission must be present in the user profile and in the case share. When you share a case, you can share its tasks or observables but it is not mandatory. Tasks (and observables) can be unitary shared. They can be shared only with organisations for which case is already shared. A case can be shared only once for a given organisation. Thus a case an its tasks/observables are shared with the same permissions for the same organisation.","title":"Share and effective permissions"},{"location":"thehive/user-guides/organisation-managers/ui-configuration/","text":"UI configuration # You can change some user interface settings in the page UI Configuration ( Organisation > UI Configuration ) You must have the permission manageConfig on your profile to manage UI Configuration. (refer to Profiles and permissions ) Hide Empty Case button # Check this checkbox to prevent your analyst to create a case without using a case template . Merge alerts into closed cases # Check this checkbox to disallow merging alerts into closed cases Select the default filter of alert case similarity panel # In this dropdown list, you can chose from various filter the default one used in alerts or cases similarity panel Define the default date format used to display dates # Define the time format used in your organisation .","title":"UI configuration"},{"location":"thehive/user-guides/organisation-managers/ui-configuration/#ui-configuration","text":"You can change some user interface settings in the page UI Configuration ( Organisation > UI Configuration ) You must have the permission manageConfig on your profile to manage UI Configuration. (refer to Profiles and permissions )","title":"UI configuration"},{"location":"thehive/user-guides/organisation-managers/ui-configuration/#hide-empty-case-button","text":"Check this checkbox to prevent your analyst to create a case without using a case template .","title":"Hide Empty Case button"},{"location":"thehive/user-guides/organisation-managers/ui-configuration/#merge-alerts-into-closed-cases","text":"Check this checkbox to disallow merging alerts into closed cases","title":"Merge alerts into closed cases"},{"location":"thehive/user-guides/organisation-managers/ui-configuration/#select-the-default-filter-of-alert-case-similarity-panel","text":"In this dropdown list, you can chose from various filter the default one used in alerts or cases similarity panel","title":"Select the default filter of alert case similarity panel"},{"location":"thehive/user-guides/organisation-managers/ui-configuration/#define-the-default-date-format-used-to-display-dates","text":"Define the time format used in your organisation .","title":"Define the default date format used to display dates"},{"location":"thehive/user-guides/organisation-managers/users-management/","text":"Users management # In TheHive4 you can manage users that belongs to your organisation in the Users page ( Organisation > Users ) You must have the permission manageUser on your profile to manage users of your organisation . (refer to Profiles and permissions ) Create new user # You can create a new user in your organisation . clic the Create new user button in the Users page ( Organisation > Users ) You must provide the following information (they are all mandatory): Organisation (autmatically filled, non modifiable) Login (This will be used by the user to authenticate) Full Name (This will be used as display name) Profile (drop-down list to set a profile, that will define user permissions ) List users # You can list the users that belongs to your organisation in the Users page ( Organisation > Users ) In this list you can find the following information: Status Login Full Name Profile API Key MFA activation Creation and last update dates Set or modify a user password # To set or modify a user password, clic the button New password (if the user never had a password) or Edit password in the column Password of the User list ( Organisation > Users ) Create, renew, revoke or reveal an user API Key # In the column API Key of the user list ( Organisation > Users ), you can: Create an API Key if the user never had one before Renew the user API Key Revoke the user API Key Reveal the user API Key in your user interface. Edit user information # You can edit the following user information from the user list ( Organisation > Users ): Full name Profile To edit an user information, clic the Edit user button on the Actions column: Lock an user # Locking an user make the account unusable for any action. You can lock an user from the User list ( Organisation > Users ). To lock an user , clic on the lock button: Delete an user # You can delete an user from your organisation via the User list ( Organisation > Users ) \u26a0\ufe0f Note Deleting an user is irrevocable. Recreating an user with the same information will not reattribute the cases the previous account was assigned to. To delete an user, clic on the trash icon:","title":"Users management"},{"location":"thehive/user-guides/organisation-managers/users-management/#users-management","text":"In TheHive4 you can manage users that belongs to your organisation in the Users page ( Organisation > Users ) You must have the permission manageUser on your profile to manage users of your organisation . (refer to Profiles and permissions )","title":"Users management"},{"location":"thehive/user-guides/organisation-managers/users-management/#create-new-user","text":"You can create a new user in your organisation . clic the Create new user button in the Users page ( Organisation > Users ) You must provide the following information (they are all mandatory): Organisation (autmatically filled, non modifiable) Login (This will be used by the user to authenticate) Full Name (This will be used as display name) Profile (drop-down list to set a profile, that will define user permissions )","title":"Create new user"},{"location":"thehive/user-guides/organisation-managers/users-management/#list-users","text":"You can list the users that belongs to your organisation in the Users page ( Organisation > Users ) In this list you can find the following information: Status Login Full Name Profile API Key MFA activation Creation and last update dates","title":"List users"},{"location":"thehive/user-guides/organisation-managers/users-management/#set-or-modify-a-user-password","text":"To set or modify a user password, clic the button New password (if the user never had a password) or Edit password in the column Password of the User list ( Organisation > Users )","title":"Set or modify a user password"},{"location":"thehive/user-guides/organisation-managers/users-management/#create-renew-revoke-or-reveal-an-user-api-key","text":"In the column API Key of the user list ( Organisation > Users ), you can: Create an API Key if the user never had one before Renew the user API Key Revoke the user API Key Reveal the user API Key in your user interface.","title":"Create, renew, revoke or reveal an user API Key"},{"location":"thehive/user-guides/organisation-managers/users-management/#edit-user-information","text":"You can edit the following user information from the user list ( Organisation > Users ): Full name Profile To edit an user information, clic the Edit user button on the Actions column:","title":"Edit user information"},{"location":"thehive/user-guides/organisation-managers/users-management/#lock-an-user","text":"Locking an user make the account unusable for any action. You can lock an user from the User list ( Organisation > Users ). To lock an user , clic on the lock button:","title":"Lock an user"},{"location":"thehive/user-guides/organisation-managers/users-management/#delete-an-user","text":"You can delete an user from your organisation via the User list ( Organisation > Users ) \u26a0\ufe0f Note Deleting an user is irrevocable. Recreating an user with the same information will not reattribute the cases the previous account was assigned to. To delete an user, clic on the trash icon:","title":"Delete an user"}]}